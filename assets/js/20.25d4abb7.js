(window.webpackJsonp=window.webpackJsonp||[]).push([[20],{192:function(s,t,a){"use strict";a.r(t);var n=a(0),o=Object(n.a)({},function(){this.$createElement;this._self._c;return this._m(0)},[function(){var s=this,t=s.$createElement,a=s._self._c||t;return a("div",{staticClass:"content"},[a("h1",{attrs:{id:"多层感知机（mlp）"}},[s._v("多层感知机（MLP）")]),s._v(" "),a("ul",[a("li",[s._v("使用 tf.keras.datasets 获得数据集并预处理")]),s._v(" "),a("li",[s._v("使用 tf.keras.Model 和 tf.keras.layers 构建模型")]),s._v(" "),a("li",[s._v("构建模型训练流程，使用 tf.keras.losses 计算损失函数，并使用 tf.keras.optimizer 优化模型")]),s._v(" "),a("li",[s._v("构建模型评估流程，使用 tf.keras.metrics 计算评估指标")])]),s._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" tensorflow "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" tf\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("import")]),s._v(" numpy "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" np\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("__version__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("pre",[a("code",[s._v("2.0.0\n")])]),s._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("!pip uninstall tensorflow\n")])])]),a("pre",[a("code",[s._v("Uninstalling tensorflow-1.15.0rc3:\n  Would remove:\n    /usr/local/bin/estimator_ckpt_converter\n    /usr/local/bin/freeze_graph\n    /usr/local/bin/saved_model_cli\n    /usr/local/bin/tensorboard\n    /usr/local/bin/tf_upgrade_v2\n    /usr/local/bin/tflite_convert\n    /usr/local/bin/toco\n    /usr/local/bin/toco_from_protos\n    /usr/local/lib/python3.6/dist-packages/tensorflow-1.15.0rc3.dist-info/*\n    /usr/local/lib/python3.6/dist-packages/tensorflow/*\n    /usr/local/lib/python3.6/dist-packages/tensorflow_core/*\nProceed (y/n)? y\n  Successfully uninstalled tensorflow-1.15.0rc3\n")])]),s._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("!pip install tensorflow"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("==")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("2.0")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v(".0")]),s._v("\n")])])]),a("pre",[a("code",[s._v("Collecting tensorflow==2.0.0\n[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)\n[K     |████████████████████████████████| 86.3MB 386kB/s \n[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.0)\nRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)\nRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.33.6)\nRequirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\nRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)\nCollecting tensorflow-estimator<2.1.0,>=2.0.0 (from tensorflow==2.0.0)\n[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)\n[K     |████████████████████████████████| 450kB 34.9MB/s \n[?25hCollecting tensorboard<2.1.0,>=2.0.0 (from tensorflow==2.0.0)\n[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)\n[K     |████████████████████████████████| 3.8MB 29.5MB/s \n[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.16.5)\nRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.7)\nRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)\nRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.7.1)\nRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)\nRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.15.0)\nRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)\nRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.0)\nRequirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)\nRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==2.0.0) (2.8.0)\nRequirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (3.1.1)\nRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (41.2.0)\nRequirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow==2.0.0) (0.16.0)\nInstalling collected packages: tensorflow-estimator, tensorboard, tensorflow\n  Found existing installation: tensorflow-estimator 1.15.1\n    Uninstalling tensorflow-estimator-1.15.1:\n      Successfully uninstalled tensorflow-estimator-1.15.1\n  Found existing installation: tensorboard 1.15.0\n    Uninstalling tensorboard-1.15.0:\n      Successfully uninstalled tensorboard-1.15.0\nSuccessfully installed tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.0\n")])]),s._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("__version__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("pre",[a("code",[s._v("2.0.0\n")])]),s._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("executing_eagerly"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" \n"),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("#tf.enable_eager_execution()")]),s._v("\n")])])]),a("pre",[a("code",[s._v("True\n")])]),s._v(" "),a("h1",{attrs:{id:"数据获取及预处理：-tf-keras-datasets"}},[s._v("数据获取及预处理： tf.keras.datasets")]),s._v(" "),a("p",[s._v("先进行预备工作，实现一个简单的 MNISTLoader 类来读取 MNIST 数据集数据。这里使用了 tf.keras.datasets 快速载入 MNIST 数据集。")]),s._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("MNISTLoader")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    mnist "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("datasets"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("mnist\n    "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" mnist"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("load_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("expand_dims"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("astype"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("float32"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("/")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("255.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("expand_dims"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("astype"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("float32"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("/")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("255.0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" axis"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("-")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_label "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("astype"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("int32"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("    "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# [60000]")]),s._v("\n    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test_label "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("astype"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("int32"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("      "),a("span",{pre:!0,attrs:{class:"token comment"}},[s._v("# [10000]")]),s._v("\n    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("num_train_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("num_test_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n  \n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("get_batch")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" batch_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    index "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("random"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("randint"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" np"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("shape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" batch_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("train_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),s._v("\n    \n")])])]),a("h1",{attrs:{id:"模型的构建：-tf-keras-model-和-tf-keras-layers"}},[s._v("模型的构建： tf.keras.Model 和 tf.keras.layers")]),s._v(" "),a("p",[s._v("多层感知机的模型类实现与上面的线性模型类似，使用 tf.keras.Model 和 tf.keras.layers 构建，所不同的地方在于层数增加了（顾名思义，“多层” 感知机），以及引入了非线性激活函数（这里使用了 ReLU 函数 ， 即下方的 activation=tf.nn.relu ）。该模型输入一个向量（比如这里是拉直的 1×784 手写体数字图片），输出 10 维的向量，分别代表这张图片属于 0 到 9 的概率。")]),s._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("class")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token class-name"}},[s._v("MLP")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("__init__")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("super")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("__init__"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("flatten "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("dense1 "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("units"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("100")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" activation"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("relu"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("dense2 "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("layers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Dense"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("units "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("10")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    \n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("def")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token function"}},[s._v("call")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    x "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("flatten"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("inputs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    x "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("dense1"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    x "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" self"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("dense2"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    output "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("nn"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("softmax"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("x"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("return")]),s._v(" output\n")])])]),a("h1",{attrs:{id:"模型的训练：-tf-keras-losses-和-tf-keras-optimizer"}},[s._v("模型的训练： tf.keras.losses 和 tf.keras.optimizer")]),s._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("num_epochs "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("5")]),s._v("\nbatch_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("50")]),s._v("\nlearning_rate "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("0.001")]),s._v("\n")])])]),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("model "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" MLP"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\ndata_loader "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" MNISTLoader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\noptimizer "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("optimizers"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("Adam"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("learning_rate"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("learning_rate"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("p",[s._v("然后迭代进行以下步骤：")]),s._v(" "),a("p",[s._v("从 DataLoader 中随机取一批训练数据；")]),s._v(" "),a("p",[s._v("将这批数据送入模型，计算出模型的预测值；")]),s._v(" "),a("p",[s._v("将模型预测值与真实值进行比较，计算损失函数（loss）。这里使用 tf.keras.losses 中的交叉熵函数作为损失函数；")]),s._v(" "),a("p",[s._v("计算损失函数关于模型变量的导数；")]),s._v(" "),a("p",[s._v("将求出的导数值传入优化器，使用优化器的 apply_gradients 方法更新模型参数以最小化损失函数")]),s._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("num_batches "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("data_loader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("num_train_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("//")]),s._v(" batch_size "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" num_epochs"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" batch_index "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("num_batches"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n  X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" data_loader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("get_batch"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("batch_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n  "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("with")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("GradientTape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("as")]),s._v(" tape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n    y_pred "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("X"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    loss "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("losses"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("sparse_categorical_crossentropy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y_true"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("y"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_pred"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("y_pred"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    loss "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("reduce_mean"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n    "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"batch %d: loss %f"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("%")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("batch_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("numpy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n  grads "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tape"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("gradient"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("loss"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("variables"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n  optimizer"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("apply_gradients"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("grads_and_vars"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("zip")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("grads"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("variables"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("pre",[a("code",[s._v("batch 0: loss 2.285256\nbatch 1: loss 2.205372\nbatch 2: loss 2.334929\nbatch 3: loss 2.056121\nbatch 4: loss 2.081305\nbatch 5: loss 1.941437\nbatch 6: loss 2.061041\nbatch 7: loss 1.972376\nbatch 8: loss 1.754983\nbatch 9: loss 1.749182\nbatch 10: loss 1.632017\nbatch 11: loss 1.592685\nbatch 12: loss 1.543856\nbatch 13: loss 1.629244\nbatch 14: loss 1.514880\nbatch 15: loss 1.428778\nbatch 16: loss 1.384761\nbatch 17: loss 1.380737\nbatch 18: loss 1.174533\nbatch 19: loss 1.367718\nbatch 20: loss 1.302076\nbatch 21: loss 1.189340\nbatch 22: loss 1.042583\nbatch 23: loss 1.025285\nbatch 24: loss 1.220943\nbatch 25: loss 1.032526\nbatch 26: loss 0.888587\nbatch 27: loss 1.059231\nbatch 28: loss 1.008494\nbatch 29: loss 0.836554\nbatch 30: loss 0.864243\nbatch 31: loss 0.854915\nbatch 32: loss 0.907002\nbatch 33: loss 1.022292\nbatch 34: loss 0.848349\nbatch 35: loss 0.761802\nbatch 36: loss 0.874089\nbatch 37: loss 0.739989\nbatch 38: loss 0.718120\nbatch 39: loss 0.734513\nbatch 40: loss 0.776238\nbatch 41: loss 0.595228\nbatch 42: loss 0.819337\nbatch 43: loss 0.679247\nbatch 44: loss 0.594760\nbatch 45: loss 0.811826\nbatch 46: loss 0.661954\nbatch 47: loss 0.793975\nbatch 48: loss 0.486048\nbatch 49: loss 0.591936\nbatch 50: loss 0.585743\nbatch 51: loss 0.614658\nbatch 52: loss 0.406398\nbatch 53: loss 0.634467\nbatch 54: loss 0.614022\nbatch 55: loss 0.672396\nbatch 56: loss 0.476113\nbatch 57: loss 0.537915\nbatch 58: loss 0.513415\nbatch 59: loss 0.550884\nbatch 60: loss 0.583696\nbatch 61: loss 0.651275\nbatch 62: loss 0.506319\nbatch 63: loss 0.502315\nbatch 64: loss 0.688968\nbatch 65: loss 0.448250\nbatch 66: loss 0.605824\nbatch 67: loss 0.489055\nbatch 68: loss 0.405522\nbatch 69: loss 0.525403\nbatch 70: loss 0.635453\nbatch 71: loss 0.301678\nbatch 72: loss 0.635552\nbatch 73: loss 0.568243\nbatch 74: loss 0.697631\nbatch 75: loss 0.460927\nbatch 76: loss 0.476688\nbatch 77: loss 0.357887\nbatch 78: loss 0.431935\nbatch 79: loss 0.565880\nbatch 80: loss 0.566409\nbatch 81: loss 0.468039\nbatch 82: loss 0.622586\nbatch 83: loss 0.487366\nbatch 84: loss 0.498076\nbatch 85: loss 0.385445\nbatch 86: loss 0.478501\nbatch 87: loss 0.558496\nbatch 88: loss 0.526953\nbatch 89: loss 0.547417\nbatch 90: loss 0.547640\nbatch 91: loss 0.557868\nbatch 92: loss 0.358667\nbatch 93: loss 0.409599\nbatch 94: loss 0.561230\nbatch 95: loss 0.668404\nbatch 96: loss 0.559714\nbatch 97: loss 0.524012\nbatch 98: loss 0.474580\nbatch 99: loss 0.399339\nbatch 100: loss 0.475849\nbatch 101: loss 0.324837\nbatch 102: loss 0.389984\nbatch 103: loss 0.473782\nbatch 104: loss 0.419292\nbatch 105: loss 0.416162\nbatch 106: loss 0.509604\nbatch 107: loss 0.346391\nbatch 108: loss 0.352209\nbatch 109: loss 0.488571\nbatch 110: loss 0.419189\nbatch 111: loss 0.437918\nbatch 112: loss 0.350439\nbatch 113: loss 0.315364\nbatch 114: loss 0.351902\nbatch 115: loss 0.549310\nbatch 116: loss 0.420044\nbatch 117: loss 0.357893\nbatch 118: loss 0.327098\nbatch 119: loss 0.293401\nbatch 120: loss 0.291696\nbatch 121: loss 0.294370\nbatch 122: loss 0.379132\nbatch 123: loss 0.437771\nbatch 124: loss 0.364326\nbatch 125: loss 0.363065\nbatch 126: loss 0.515139\nbatch 127: loss 0.221047\nbatch 128: loss 0.207894\nbatch 129: loss 0.304511\nbatch 130: loss 0.569853\nbatch 131: loss 0.434927\nbatch 132: loss 0.402055\nbatch 133: loss 0.551857\nbatch 134: loss 0.451924\nbatch 135: loss 0.390041\nbatch 136: loss 0.205925\nbatch 137: loss 0.300728\nbatch 138: loss 0.279311\nbatch 139: loss 0.254808\nbatch 140: loss 0.406166\nbatch 141: loss 0.510225\nbatch 142: loss 0.336390\nbatch 143: loss 0.286932\nbatch 144: loss 0.334305\nbatch 145: loss 0.505116\nbatch 146: loss 0.296393\nbatch 147: loss 0.498919\nbatch 148: loss 0.507318\nbatch 149: loss 0.261467\nbatch 150: loss 0.298310\nbatch 151: loss 0.411035\nbatch 152: loss 0.361716\nbatch 153: loss 0.277789\nbatch 154: loss 0.288142\nbatch 155: loss 0.430213\nbatch 156: loss 0.242033\nbatch 157: loss 0.438750\nbatch 158: loss 0.418472\nbatch 159: loss 0.703795\nbatch 160: loss 0.331291\nbatch 161: loss 0.392003\nbatch 162: loss 0.503135\nbatch 163: loss 0.416751\nbatch 164: loss 0.649559\nbatch 165: loss 0.284626\nbatch 166: loss 0.253399\nbatch 167: loss 0.320030\nbatch 168: loss 0.521847\nbatch 169: loss 0.307325\nbatch 170: loss 0.274674\nbatch 171: loss 0.358530\nbatch 172: loss 0.518101\nbatch 173: loss 0.390567\nbatch 174: loss 0.174029\nbatch 175: loss 0.539849\nbatch 176: loss 0.428587\nbatch 177: loss 0.329828\nbatch 178: loss 0.812834\nbatch 179: loss 0.396530\nbatch 180: loss 0.224222\nbatch 181: loss 0.258882\nbatch 182: loss 0.207767\nbatch 183: loss 0.155181\nbatch 184: loss 0.377799\nbatch 185: loss 0.284999\nbatch 186: loss 0.323863\nbatch 187: loss 0.338120\nbatch 188: loss 0.188672\nbatch 189: loss 0.327143\nbatch 190: loss 0.504763\nbatch 191: loss 0.337281\nbatch 192: loss 0.262181\nbatch 193: loss 0.223661\nbatch 194: loss 0.283222\nbatch 195: loss 0.190049\nbatch 196: loss 0.274772\nbatch 197: loss 0.282986\nbatch 198: loss 0.372376\nbatch 199: loss 0.440532\nbatch 200: loss 0.427018\nbatch 201: loss 0.316213\nbatch 202: loss 0.460297\nbatch 203: loss 0.486977\nbatch 204: loss 0.261368\nbatch 205: loss 0.321267\nbatch 206: loss 0.350332\nbatch 207: loss 0.225021\nbatch 208: loss 0.342973\nbatch 209: loss 0.244674\nbatch 210: loss 0.441389\nbatch 211: loss 0.378784\nbatch 212: loss 0.413090\nbatch 213: loss 0.252654\nbatch 214: loss 0.414331\nbatch 215: loss 0.536494\nbatch 216: loss 0.192939\nbatch 217: loss 0.213020\nbatch 218: loss 0.297483\nbatch 219: loss 0.343482\nbatch 220: loss 0.412594\nbatch 221: loss 0.324474\nbatch 222: loss 0.318865\nbatch 223: loss 0.234760\nbatch 224: loss 0.272884\nbatch 225: loss 0.242461\nbatch 226: loss 0.545350\nbatch 227: loss 0.312405\nbatch 228: loss 0.391701\nbatch 229: loss 0.372519\nbatch 230: loss 0.472101\nbatch 231: loss 0.377553\nbatch 232: loss 0.216674\nbatch 233: loss 0.271941\nbatch 234: loss 0.558961\nbatch 235: loss 0.170775\nbatch 236: loss 0.262034\nbatch 237: loss 0.225955\nbatch 238: loss 0.212476\nbatch 239: loss 0.260501\nbatch 240: loss 0.214531\nbatch 241: loss 0.323311\nbatch 242: loss 0.478120\nbatch 243: loss 0.236381\nbatch 244: loss 0.410681\nbatch 245: loss 0.263916\nbatch 246: loss 0.323504\nbatch 247: loss 0.399251\nbatch 248: loss 0.294628\nbatch 249: loss 0.326098\nbatch 250: loss 0.372162\nbatch 251: loss 0.495363\nbatch 252: loss 0.221127\nbatch 253: loss 0.576529\nbatch 254: loss 0.397664\nbatch 255: loss 0.369326\nbatch 256: loss 0.210746\nbatch 257: loss 0.264947\nbatch 258: loss 0.207313\nbatch 259: loss 0.211112\nbatch 260: loss 0.255969\nbatch 261: loss 0.215782\nbatch 262: loss 0.190437\nbatch 263: loss 0.222935\nbatch 264: loss 0.397785\nbatch 265: loss 0.227694\nbatch 266: loss 0.467060\nbatch 267: loss 0.419647\nbatch 268: loss 0.357369\nbatch 269: loss 0.204077\nbatch 270: loss 0.456494\nbatch 271: loss 0.174073\nbatch 272: loss 0.438463\nbatch 273: loss 0.257310\nbatch 274: loss 0.358364\nbatch 275: loss 0.197801\nbatch 276: loss 0.552193\nbatch 277: loss 0.302756\nbatch 278: loss 0.383897\nbatch 279: loss 0.243372\nbatch 280: loss 0.619185\nbatch 281: loss 0.313286\nbatch 282: loss 0.353051\nbatch 283: loss 0.262350\nbatch 284: loss 0.167927\nbatch 285: loss 0.241061\nbatch 286: loss 0.197949\nbatch 287: loss 0.249585\nbatch 288: loss 0.337045\nbatch 289: loss 0.382001\nbatch 290: loss 0.555709\nbatch 291: loss 0.468115\nbatch 292: loss 0.413247\nbatch 293: loss 0.389037\nbatch 294: loss 0.517556\nbatch 295: loss 0.302482\nbatch 296: loss 0.189868\nbatch 297: loss 0.396580\nbatch 298: loss 0.217572\nbatch 299: loss 0.280075\nbatch 300: loss 0.411861\nbatch 301: loss 0.194347\nbatch 302: loss 0.157477\nbatch 303: loss 0.200695\nbatch 304: loss 0.212443\nbatch 305: loss 0.240730\nbatch 306: loss 0.295195\nbatch 307: loss 0.406676\nbatch 308: loss 0.212255\nbatch 309: loss 0.360117\nbatch 310: loss 0.228627\nbatch 311: loss 0.207296\nbatch 312: loss 0.142424\nbatch 313: loss 0.268343\nbatch 314: loss 0.237308\nbatch 315: loss 0.289460\nbatch 316: loss 0.303796\nbatch 317: loss 0.220824\nbatch 318: loss 0.213275\nbatch 319: loss 0.246066\nbatch 320: loss 0.335618\nbatch 321: loss 0.291521\nbatch 322: loss 0.334310\nbatch 323: loss 0.446926\nbatch 324: loss 0.171458\nbatch 325: loss 0.422111\nbatch 326: loss 0.156345\nbatch 327: loss 0.284695\nbatch 328: loss 0.267974\nbatch 329: loss 0.577840\nbatch 330: loss 0.387926\nbatch 331: loss 0.268040\nbatch 332: loss 0.353480\nbatch 333: loss 0.298518\nbatch 334: loss 0.478521\nbatch 335: loss 0.365600\nbatch 336: loss 0.185804\nbatch 337: loss 0.223569\nbatch 338: loss 0.256348\nbatch 339: loss 0.321289\nbatch 340: loss 0.271853\nbatch 341: loss 0.176548\nbatch 342: loss 0.229647\nbatch 343: loss 0.111074\nbatch 344: loss 0.463467\nbatch 345: loss 0.239988\nbatch 346: loss 0.314291\nbatch 347: loss 0.207193\nbatch 348: loss 0.171432\nbatch 349: loss 0.364189\nbatch 350: loss 0.335221\nbatch 351: loss 0.413151\nbatch 352: loss 0.362950\nbatch 353: loss 0.319661\nbatch 354: loss 0.162014\nbatch 355: loss 0.389689\nbatch 356: loss 0.421089\nbatch 357: loss 0.181052\nbatch 358: loss 0.177759\nbatch 359: loss 0.402913\nbatch 360: loss 0.235380\nbatch 361: loss 0.267268\nbatch 362: loss 0.202916\nbatch 363: loss 0.201494\nbatch 364: loss 0.150469\nbatch 365: loss 0.364969\nbatch 366: loss 0.412862\nbatch 367: loss 0.321997\nbatch 368: loss 0.132545\nbatch 369: loss 0.159412\nbatch 370: loss 0.342442\nbatch 371: loss 0.259415\nbatch 372: loss 0.070327\nbatch 373: loss 0.316670\nbatch 374: loss 0.199430\nbatch 375: loss 0.291386\nbatch 376: loss 0.313575\nbatch 377: loss 0.081056\nbatch 378: loss 0.325014\nbatch 379: loss 0.404023\nbatch 380: loss 0.389470\nbatch 381: loss 0.253171\nbatch 382: loss 0.223153\nbatch 383: loss 0.229689\nbatch 384: loss 0.350770\nbatch 385: loss 0.254723\nbatch 386: loss 0.179526\nbatch 387: loss 0.473149\nbatch 388: loss 0.440840\nbatch 389: loss 0.237252\nbatch 390: loss 0.197012\nbatch 391: loss 0.332625\nbatch 392: loss 0.455114\nbatch 393: loss 0.250911\nbatch 394: loss 0.479606\nbatch 395: loss 0.355097\nbatch 396: loss 0.588226\nbatch 397: loss 0.258520\nbatch 398: loss 0.306099\nbatch 399: loss 0.147707\nbatch 400: loss 0.169863\nbatch 401: loss 0.170929\nbatch 402: loss 0.267700\nbatch 403: loss 0.330947\nbatch 404: loss 0.266987\nbatch 405: loss 0.192440\nbatch 406: loss 0.343603\nbatch 407: loss 0.200144\nbatch 408: loss 0.171768\nbatch 409: loss 0.284261\nbatch 410: loss 0.201596\nbatch 411: loss 0.118803\nbatch 412: loss 0.349311\nbatch 413: loss 0.322410\nbatch 414: loss 0.286590\nbatch 415: loss 0.218352\nbatch 416: loss 0.159007\nbatch 417: loss 0.303618\nbatch 418: loss 0.138437\nbatch 419: loss 0.332913\nbatch 420: loss 0.210851\nbatch 421: loss 0.306345\nbatch 422: loss 0.285776\nbatch 423: loss 0.507604\nbatch 424: loss 0.403712\nbatch 425: loss 0.226156\nbatch 426: loss 0.246216\nbatch 427: loss 0.197353\nbatch 428: loss 0.213131\nbatch 429: loss 0.298204\nbatch 430: loss 0.271106\nbatch 431: loss 0.409738\nbatch 432: loss 0.156217\nbatch 433: loss 0.185648\nbatch 434: loss 0.357938\nbatch 435: loss 0.126928\nbatch 436: loss 0.335029\nbatch 437: loss 0.342921\nbatch 438: loss 0.197034\nbatch 439: loss 0.268930\nbatch 440: loss 0.276491\nbatch 441: loss 0.274351\nbatch 442: loss 0.237839\nbatch 443: loss 0.161867\nbatch 444: loss 0.431481\nbatch 445: loss 0.318385\nbatch 446: loss 0.096307\nbatch 447: loss 0.375171\nbatch 448: loss 0.397831\nbatch 449: loss 0.445601\nbatch 450: loss 0.227258\nbatch 451: loss 0.400308\nbatch 452: loss 0.251131\nbatch 453: loss 0.328621\nbatch 454: loss 0.340671\nbatch 455: loss 0.299543\nbatch 456: loss 0.247674\nbatch 457: loss 0.219753\nbatch 458: loss 0.432150\nbatch 459: loss 0.101117\nbatch 460: loss 0.222552\nbatch 461: loss 0.246256\nbatch 462: loss 0.137580\nbatch 463: loss 0.150216\nbatch 464: loss 0.266098\nbatch 465: loss 0.152606\nbatch 466: loss 0.172253\nbatch 467: loss 0.276848\nbatch 468: loss 0.222936\nbatch 469: loss 0.447961\nbatch 470: loss 0.186578\nbatch 471: loss 0.330791\nbatch 472: loss 0.496173\nbatch 473: loss 0.355145\nbatch 474: loss 0.210110\nbatch 475: loss 0.254539\nbatch 476: loss 0.133599\nbatch 477: loss 0.174432\nbatch 478: loss 0.198371\nbatch 479: loss 0.148955\nbatch 480: loss 0.190068\nbatch 481: loss 0.226166\nbatch 482: loss 0.304002\nbatch 483: loss 0.209559\nbatch 484: loss 0.388254\nbatch 485: loss 0.199749\nbatch 486: loss 0.310077\nbatch 487: loss 0.692423\nbatch 488: loss 0.363514\nbatch 489: loss 0.300715\nbatch 490: loss 0.230611\nbatch 491: loss 0.163196\nbatch 492: loss 0.095125\nbatch 493: loss 0.175935\nbatch 494: loss 0.282566\nbatch 495: loss 0.086311\nbatch 496: loss 0.154789\nbatch 497: loss 0.149378\nbatch 498: loss 0.375296\nbatch 499: loss 0.332906\nbatch 500: loss 0.098054\nbatch 501: loss 0.324982\nbatch 502: loss 0.156018\nbatch 503: loss 0.149343\nbatch 504: loss 0.221093\nbatch 505: loss 0.135403\nbatch 506: loss 0.370418\nbatch 507: loss 0.229524\nbatch 508: loss 0.159202\nbatch 509: loss 0.383838\nbatch 510: loss 0.226361\nbatch 511: loss 0.127674\nbatch 512: loss 0.257506\nbatch 513: loss 0.323373\nbatch 514: loss 0.329182\nbatch 515: loss 0.244790\nbatch 516: loss 0.139769\nbatch 517: loss 0.099394\nbatch 518: loss 0.329889\nbatch 519: loss 0.328298\nbatch 520: loss 0.435600\nbatch 521: loss 0.163262\nbatch 522: loss 0.233294\nbatch 523: loss 0.117601\nbatch 524: loss 0.150298\nbatch 525: loss 0.399950\nbatch 526: loss 0.220615\nbatch 527: loss 0.294963\nbatch 528: loss 0.278380\nbatch 529: loss 0.134269\nbatch 530: loss 0.119274\nbatch 531: loss 0.403507\nbatch 532: loss 0.176568\nbatch 533: loss 0.440408\nbatch 534: loss 0.270867\nbatch 535: loss 0.248501\nbatch 536: loss 0.642812\nbatch 537: loss 0.211954\nbatch 538: loss 0.369788\nbatch 539: loss 0.124490\nbatch 540: loss 0.302076\nbatch 541: loss 0.232399\nbatch 542: loss 0.228947\nbatch 543: loss 0.444461\nbatch 544: loss 0.529427\nbatch 545: loss 0.186108\nbatch 546: loss 0.239489\nbatch 547: loss 0.244953\nbatch 548: loss 0.181797\nbatch 549: loss 0.188976\nbatch 550: loss 0.189450\nbatch 551: loss 0.163070\nbatch 552: loss 0.245811\nbatch 553: loss 0.151188\nbatch 554: loss 0.335199\nbatch 555: loss 0.263940\nbatch 556: loss 0.271129\nbatch 557: loss 0.202406\nbatch 558: loss 0.419538\nbatch 559: loss 0.278002\nbatch 560: loss 0.188402\nbatch 561: loss 0.143863\nbatch 562: loss 0.451175\nbatch 563: loss 0.153855\nbatch 564: loss 0.254996\nbatch 565: loss 0.421723\nbatch 566: loss 0.200114\nbatch 567: loss 0.204463\nbatch 568: loss 0.330874\nbatch 569: loss 0.226188\nbatch 570: loss 0.181282\nbatch 571: loss 0.239751\nbatch 572: loss 0.305427\nbatch 573: loss 0.108541\nbatch 574: loss 0.269573\nbatch 575: loss 0.322800\nbatch 576: loss 0.180970\nbatch 577: loss 0.107506\nbatch 578: loss 0.293277\nbatch 579: loss 0.129323\nbatch 580: loss 0.346278\nbatch 581: loss 0.216237\nbatch 582: loss 0.308874\nbatch 583: loss 0.138837\nbatch 584: loss 0.156226\nbatch 585: loss 0.242788\nbatch 586: loss 0.357992\nbatch 587: loss 0.254993\nbatch 588: loss 0.142026\nbatch 589: loss 0.333713\nbatch 590: loss 0.448854\nbatch 591: loss 0.238323\nbatch 592: loss 0.249784\nbatch 593: loss 0.124006\nbatch 594: loss 0.083847\nbatch 595: loss 0.068844\nbatch 596: loss 0.152750\nbatch 597: loss 0.181854\nbatch 598: loss 0.293189\nbatch 599: loss 0.130568\nbatch 600: loss 0.228575\nbatch 601: loss 0.114651\nbatch 602: loss 0.274828\nbatch 603: loss 0.214390\nbatch 604: loss 0.287496\nbatch 605: loss 0.240209\nbatch 606: loss 0.157249\nbatch 607: loss 0.133174\nbatch 608: loss 0.295633\nbatch 609: loss 0.101599\nbatch 610: loss 0.327761\nbatch 611: loss 0.153859\nbatch 612: loss 0.222140\nbatch 613: loss 0.190587\nbatch 614: loss 0.119984\nbatch 615: loss 0.160714\nbatch 616: loss 0.120944\nbatch 617: loss 0.287635\nbatch 618: loss 0.061606\nbatch 619: loss 0.159002\nbatch 620: loss 0.102159\nbatch 621: loss 0.212164\nbatch 622: loss 0.123733\nbatch 623: loss 0.220980\nbatch 624: loss 0.170258\nbatch 625: loss 0.386752\nbatch 626: loss 0.202990\nbatch 627: loss 0.221710\nbatch 628: loss 0.218060\nbatch 629: loss 0.285306\nbatch 630: loss 0.184506\nbatch 631: loss 0.107576\nbatch 632: loss 0.135285\nbatch 633: loss 0.259310\nbatch 634: loss 0.221828\nbatch 635: loss 0.402952\nbatch 636: loss 0.186237\nbatch 637: loss 0.145068\nbatch 638: loss 0.329077\nbatch 639: loss 0.240307\nbatch 640: loss 0.377551\nbatch 641: loss 0.494494\nbatch 642: loss 0.135959\nbatch 643: loss 0.297693\nbatch 644: loss 0.332268\nbatch 645: loss 0.124169\nbatch 646: loss 0.095379\nbatch 647: loss 0.201203\nbatch 648: loss 0.266056\nbatch 649: loss 0.288723\nbatch 650: loss 0.226291\nbatch 651: loss 0.434054\nbatch 652: loss 0.325442\nbatch 653: loss 0.213596\nbatch 654: loss 0.244598\nbatch 655: loss 0.252400\nbatch 656: loss 0.190792\nbatch 657: loss 0.255896\nbatch 658: loss 0.163001\nbatch 659: loss 0.183375\nbatch 660: loss 0.160560\nbatch 661: loss 0.234111\nbatch 662: loss 0.119940\nbatch 663: loss 0.209808\nbatch 664: loss 0.136872\nbatch 665: loss 0.176688\nbatch 666: loss 0.113685\nbatch 667: loss 0.347155\nbatch 668: loss 0.158519\nbatch 669: loss 0.397002\nbatch 670: loss 0.179305\nbatch 671: loss 0.190632\nbatch 672: loss 0.235130\nbatch 673: loss 0.181187\nbatch 674: loss 0.253867\nbatch 675: loss 0.129148\nbatch 676: loss 0.303930\nbatch 677: loss 0.133495\nbatch 678: loss 0.090633\nbatch 679: loss 0.152214\nbatch 680: loss 0.237603\nbatch 681: loss 0.227588\nbatch 682: loss 0.254605\nbatch 683: loss 0.186847\nbatch 684: loss 0.147803\nbatch 685: loss 0.120886\nbatch 686: loss 0.164892\nbatch 687: loss 0.331868\nbatch 688: loss 0.414535\nbatch 689: loss 0.312438\nbatch 690: loss 0.170866\nbatch 691: loss 0.504608\nbatch 692: loss 0.377827\nbatch 693: loss 0.158408\nbatch 694: loss 0.209521\nbatch 695: loss 0.057885\nbatch 696: loss 0.098620\nbatch 697: loss 0.185616\nbatch 698: loss 0.137314\nbatch 699: loss 0.119743\nbatch 700: loss 0.234409\nbatch 701: loss 0.239075\nbatch 702: loss 0.151472\nbatch 703: loss 0.165083\nbatch 704: loss 0.269955\nbatch 705: loss 0.226257\nbatch 706: loss 0.239466\nbatch 707: loss 0.199373\nbatch 708: loss 0.325505\nbatch 709: loss 0.228041\nbatch 710: loss 0.144823\nbatch 711: loss 0.114232\nbatch 712: loss 0.227160\nbatch 713: loss 0.330252\nbatch 714: loss 0.158552\nbatch 715: loss 0.210313\nbatch 716: loss 0.164293\nbatch 717: loss 0.161685\nbatch 718: loss 0.188547\nbatch 719: loss 0.180156\nbatch 720: loss 0.200851\nbatch 721: loss 0.092565\nbatch 722: loss 0.173485\nbatch 723: loss 0.172144\nbatch 724: loss 0.306569\nbatch 725: loss 0.363967\nbatch 726: loss 0.097801\nbatch 727: loss 0.347186\nbatch 728: loss 0.188027\nbatch 729: loss 0.209128\nbatch 730: loss 0.181954\nbatch 731: loss 0.150047\nbatch 732: loss 0.234956\nbatch 733: loss 0.156357\nbatch 734: loss 0.207043\nbatch 735: loss 0.219058\nbatch 736: loss 0.113917\nbatch 737: loss 0.337811\nbatch 738: loss 0.131980\nbatch 739: loss 0.172769\nbatch 740: loss 0.169920\nbatch 741: loss 0.178208\nbatch 742: loss 0.338216\nbatch 743: loss 0.180800\nbatch 744: loss 0.205118\nbatch 745: loss 0.174190\nbatch 746: loss 0.201298\nbatch 747: loss 0.353466\nbatch 748: loss 0.362657\nbatch 749: loss 0.069622\nbatch 750: loss 0.171295\nbatch 751: loss 0.220336\nbatch 752: loss 0.149348\nbatch 753: loss 0.129534\nbatch 754: loss 0.295249\nbatch 755: loss 0.191030\nbatch 756: loss 0.217733\nbatch 757: loss 0.260932\nbatch 758: loss 0.216347\nbatch 759: loss 0.218694\nbatch 760: loss 0.182442\nbatch 761: loss 0.184780\nbatch 762: loss 0.283169\nbatch 763: loss 0.185836\nbatch 764: loss 0.225887\nbatch 765: loss 0.462201\nbatch 766: loss 0.201778\nbatch 767: loss 0.118126\nbatch 768: loss 0.086200\nbatch 769: loss 0.071274\nbatch 770: loss 0.150370\nbatch 771: loss 0.153042\nbatch 772: loss 0.177931\nbatch 773: loss 0.120760\nbatch 774: loss 0.179441\nbatch 775: loss 0.188710\nbatch 776: loss 0.116437\nbatch 777: loss 0.190658\nbatch 778: loss 0.338074\nbatch 779: loss 0.287869\nbatch 780: loss 0.204810\nbatch 781: loss 0.286212\nbatch 782: loss 0.356835\nbatch 783: loss 0.114364\nbatch 784: loss 0.257239\nbatch 785: loss 0.211892\nbatch 786: loss 0.322811\nbatch 787: loss 0.327907\nbatch 788: loss 0.183872\nbatch 789: loss 0.292791\nbatch 790: loss 0.147020\nbatch 791: loss 0.146361\nbatch 792: loss 0.155734\nbatch 793: loss 0.148340\nbatch 794: loss 0.105872\nbatch 795: loss 0.205811\nbatch 796: loss 0.262269\nbatch 797: loss 0.115890\nbatch 798: loss 0.224688\nbatch 799: loss 0.290177\nbatch 800: loss 0.154820\nbatch 801: loss 0.180000\nbatch 802: loss 0.305846\nbatch 803: loss 0.327215\nbatch 804: loss 0.196993\nbatch 805: loss 0.143069\nbatch 806: loss 0.240165\nbatch 807: loss 0.132887\nbatch 808: loss 0.366762\nbatch 809: loss 0.105375\nbatch 810: loss 0.501361\nbatch 811: loss 0.201997\nbatch 812: loss 0.096392\nbatch 813: loss 0.152439\nbatch 814: loss 0.138672\nbatch 815: loss 0.151600\nbatch 816: loss 0.227888\nbatch 817: loss 0.145324\nbatch 818: loss 0.146025\nbatch 819: loss 0.135243\nbatch 820: loss 0.233483\nbatch 821: loss 0.457414\nbatch 822: loss 0.149514\nbatch 823: loss 0.166615\nbatch 824: loss 0.372353\nbatch 825: loss 0.588749\nbatch 826: loss 0.096479\nbatch 827: loss 0.165670\nbatch 828: loss 0.337628\nbatch 829: loss 0.146481\nbatch 830: loss 0.062156\nbatch 831: loss 0.134446\nbatch 832: loss 0.294032\nbatch 833: loss 0.060535\nbatch 834: loss 0.258834\nbatch 835: loss 0.219695\nbatch 836: loss 0.162427\nbatch 837: loss 0.246701\nbatch 838: loss 0.096755\nbatch 839: loss 0.214149\nbatch 840: loss 0.250912\nbatch 841: loss 0.187933\nbatch 842: loss 0.185414\nbatch 843: loss 0.227188\nbatch 844: loss 0.119188\nbatch 845: loss 0.334929\nbatch 846: loss 0.133930\nbatch 847: loss 0.124146\nbatch 848: loss 0.105841\nbatch 849: loss 0.131862\nbatch 850: loss 0.117265\nbatch 851: loss 0.235734\nbatch 852: loss 0.109177\nbatch 853: loss 0.323216\nbatch 854: loss 0.076868\nbatch 855: loss 0.270427\nbatch 856: loss 0.167292\nbatch 857: loss 0.367964\nbatch 858: loss 0.158193\nbatch 859: loss 0.243538\nbatch 860: loss 0.203438\nbatch 861: loss 0.188599\nbatch 862: loss 0.250619\nbatch 863: loss 0.097269\nbatch 864: loss 0.221504\nbatch 865: loss 0.233273\nbatch 866: loss 0.128052\nbatch 867: loss 0.253130\nbatch 868: loss 0.150896\nbatch 869: loss 0.236543\nbatch 870: loss 0.128203\nbatch 871: loss 0.081954\nbatch 872: loss 0.101310\nbatch 873: loss 0.174750\nbatch 874: loss 0.160408\nbatch 875: loss 0.105250\nbatch 876: loss 0.076193\nbatch 877: loss 0.175928\nbatch 878: loss 0.216347\nbatch 879: loss 0.317106\nbatch 880: loss 0.100854\nbatch 881: loss 0.127127\nbatch 882: loss 0.308445\nbatch 883: loss 0.124931\nbatch 884: loss 0.248890\nbatch 885: loss 0.169293\nbatch 886: loss 0.107638\nbatch 887: loss 0.158109\nbatch 888: loss 0.102283\nbatch 889: loss 0.280424\nbatch 890: loss 0.139364\nbatch 891: loss 0.094142\nbatch 892: loss 0.157169\nbatch 893: loss 0.048151\nbatch 894: loss 0.059561\nbatch 895: loss 0.287735\nbatch 896: loss 0.374580\nbatch 897: loss 0.107784\nbatch 898: loss 0.313108\nbatch 899: loss 0.135252\nbatch 900: loss 0.114187\nbatch 901: loss 0.305683\nbatch 902: loss 0.245307\nbatch 903: loss 0.191814\nbatch 904: loss 0.107314\nbatch 905: loss 0.208177\nbatch 906: loss 0.089450\nbatch 907: loss 0.176495\nbatch 908: loss 0.234527\nbatch 909: loss 0.073686\nbatch 910: loss 0.136289\nbatch 911: loss 0.080171\nbatch 912: loss 0.305031\nbatch 913: loss 0.272383\nbatch 914: loss 0.140583\nbatch 915: loss 0.338411\nbatch 916: loss 0.135546\nbatch 917: loss 0.071687\nbatch 918: loss 0.143426\nbatch 919: loss 0.192598\nbatch 920: loss 0.100333\nbatch 921: loss 0.150586\nbatch 922: loss 0.179805\nbatch 923: loss 0.237417\nbatch 924: loss 0.303762\nbatch 925: loss 0.145758\nbatch 926: loss 0.274656\nbatch 927: loss 0.054583\nbatch 928: loss 0.054210\nbatch 929: loss 0.129085\nbatch 930: loss 0.079100\nbatch 931: loss 0.305413\nbatch 932: loss 0.121410\nbatch 933: loss 0.174353\nbatch 934: loss 0.160843\nbatch 935: loss 0.272624\nbatch 936: loss 0.133957\nbatch 937: loss 0.228590\nbatch 938: loss 0.265474\nbatch 939: loss 0.211100\nbatch 940: loss 0.110170\nbatch 941: loss 0.123010\nbatch 942: loss 0.390093\nbatch 943: loss 0.174213\nbatch 944: loss 0.124961\nbatch 945: loss 0.148612\nbatch 946: loss 0.135782\nbatch 947: loss 0.298315\nbatch 948: loss 0.257022\nbatch 949: loss 0.186655\nbatch 950: loss 0.111080\nbatch 951: loss 0.209432\nbatch 952: loss 0.147841\nbatch 953: loss 0.208873\nbatch 954: loss 0.350158\nbatch 955: loss 0.430361\nbatch 956: loss 0.379979\nbatch 957: loss 0.218984\nbatch 958: loss 0.271433\nbatch 959: loss 0.173331\nbatch 960: loss 0.124364\nbatch 961: loss 0.119645\nbatch 962: loss 0.139107\nbatch 963: loss 0.145757\nbatch 964: loss 0.105806\nbatch 965: loss 0.237184\nbatch 966: loss 0.147279\nbatch 967: loss 0.277448\nbatch 968: loss 0.182009\nbatch 969: loss 0.199019\nbatch 970: loss 0.158343\nbatch 971: loss 0.123158\nbatch 972: loss 0.164376\nbatch 973: loss 0.090123\nbatch 974: loss 0.130950\nbatch 975: loss 0.315210\nbatch 976: loss 0.335807\nbatch 977: loss 0.161216\nbatch 978: loss 0.225412\nbatch 979: loss 0.131215\nbatch 980: loss 0.247282\nbatch 981: loss 0.134753\nbatch 982: loss 0.135581\nbatch 983: loss 0.203192\nbatch 984: loss 0.228613\nbatch 985: loss 0.231663\nbatch 986: loss 0.135012\nbatch 987: loss 0.322604\nbatch 988: loss 0.183648\nbatch 989: loss 0.273781\nbatch 990: loss 0.055497\nbatch 991: loss 0.419740\nbatch 992: loss 0.216128\nbatch 993: loss 0.150260\nbatch 994: loss 0.085838\nbatch 995: loss 0.308117\nbatch 996: loss 0.201595\nbatch 997: loss 0.343333\nbatch 998: loss 0.398500\nbatch 999: loss 0.254225\nbatch 1000: loss 0.086956\nbatch 1001: loss 0.059464\nbatch 1002: loss 0.249137\nbatch 1003: loss 0.188538\nbatch 1004: loss 0.140708\nbatch 1005: loss 0.265630\nbatch 1006: loss 0.256077\nbatch 1007: loss 0.110759\nbatch 1008: loss 0.208575\nbatch 1009: loss 0.207554\nbatch 1010: loss 0.182978\nbatch 1011: loss 0.047142\nbatch 1012: loss 0.234092\nbatch 1013: loss 0.255006\nbatch 1014: loss 0.380593\nbatch 1015: loss 0.382022\nbatch 1016: loss 0.250487\nbatch 1017: loss 0.389435\nbatch 1018: loss 0.138237\nbatch 1019: loss 0.122437\nbatch 1020: loss 0.215960\nbatch 1021: loss 0.144368\nbatch 1022: loss 0.063167\nbatch 1023: loss 0.258749\nbatch 1024: loss 0.215660\nbatch 1025: loss 0.169605\nbatch 1026: loss 0.222371\nbatch 1027: loss 0.160503\nbatch 1028: loss 0.091064\nbatch 1029: loss 0.166862\nbatch 1030: loss 0.256545\nbatch 1031: loss 0.263640\nbatch 1032: loss 0.363329\nbatch 1033: loss 0.132013\nbatch 1034: loss 0.361455\nbatch 1035: loss 0.122579\nbatch 1036: loss 0.255639\nbatch 1037: loss 0.147059\nbatch 1038: loss 0.128041\nbatch 1039: loss 0.097752\nbatch 1040: loss 0.176450\nbatch 1041: loss 0.116697\nbatch 1042: loss 0.073593\nbatch 1043: loss 0.219901\nbatch 1044: loss 0.141889\nbatch 1045: loss 0.219300\nbatch 1046: loss 0.157903\nbatch 1047: loss 0.147873\nbatch 1048: loss 0.132922\nbatch 1049: loss 0.103669\nbatch 1050: loss 0.293736\nbatch 1051: loss 0.195533\nbatch 1052: loss 0.241293\nbatch 1053: loss 0.318838\nbatch 1054: loss 0.238642\nbatch 1055: loss 0.266800\nbatch 1056: loss 0.096591\nbatch 1057: loss 0.108956\nbatch 1058: loss 0.049442\nbatch 1059: loss 0.089660\nbatch 1060: loss 0.059911\nbatch 1061: loss 0.281191\nbatch 1062: loss 0.123201\nbatch 1063: loss 0.113689\nbatch 1064: loss 0.161101\nbatch 1065: loss 0.378393\nbatch 1066: loss 0.163909\nbatch 1067: loss 0.073953\nbatch 1068: loss 0.136903\nbatch 1069: loss 0.148149\nbatch 1070: loss 0.183497\nbatch 1071: loss 0.236094\nbatch 1072: loss 0.101146\nbatch 1073: loss 0.103948\nbatch 1074: loss 0.275163\nbatch 1075: loss 0.185820\nbatch 1076: loss 0.121375\nbatch 1077: loss 0.115727\nbatch 1078: loss 0.130624\nbatch 1079: loss 0.229426\nbatch 1080: loss 0.143067\nbatch 1081: loss 0.097142\nbatch 1082: loss 0.180269\nbatch 1083: loss 0.208798\nbatch 1084: loss 0.340739\nbatch 1085: loss 0.131768\nbatch 1086: loss 0.125420\nbatch 1087: loss 0.152654\nbatch 1088: loss 0.141136\nbatch 1089: loss 0.100431\nbatch 1090: loss 0.135799\nbatch 1091: loss 0.379679\nbatch 1092: loss 0.078721\nbatch 1093: loss 0.391473\nbatch 1094: loss 0.553992\nbatch 1095: loss 0.139589\nbatch 1096: loss 0.083935\nbatch 1097: loss 0.181077\nbatch 1098: loss 0.142036\nbatch 1099: loss 0.026884\nbatch 1100: loss 0.202918\nbatch 1101: loss 0.113825\nbatch 1102: loss 0.100030\nbatch 1103: loss 0.161295\nbatch 1104: loss 0.096025\nbatch 1105: loss 0.133122\nbatch 1106: loss 0.105768\nbatch 1107: loss 0.199275\nbatch 1108: loss 0.092459\nbatch 1109: loss 0.128206\nbatch 1110: loss 0.271589\nbatch 1111: loss 0.180427\nbatch 1112: loss 0.069741\nbatch 1113: loss 0.122419\nbatch 1114: loss 0.099530\nbatch 1115: loss 0.063315\nbatch 1116: loss 0.298018\nbatch 1117: loss 0.130231\nbatch 1118: loss 0.133063\nbatch 1119: loss 0.135472\nbatch 1120: loss 0.121924\nbatch 1121: loss 0.171091\nbatch 1122: loss 0.279224\nbatch 1123: loss 0.307852\nbatch 1124: loss 0.127327\nbatch 1125: loss 0.093573\nbatch 1126: loss 0.071929\nbatch 1127: loss 0.072314\nbatch 1128: loss 0.297164\nbatch 1129: loss 0.141945\nbatch 1130: loss 0.332947\nbatch 1131: loss 0.064779\nbatch 1132: loss 0.188216\nbatch 1133: loss 0.088020\nbatch 1134: loss 0.089753\nbatch 1135: loss 0.226469\nbatch 1136: loss 0.177748\nbatch 1137: loss 0.070747\nbatch 1138: loss 0.065446\nbatch 1139: loss 0.173488\nbatch 1140: loss 0.385899\nbatch 1141: loss 0.221569\nbatch 1142: loss 0.156759\nbatch 1143: loss 0.112594\nbatch 1144: loss 0.422016\nbatch 1145: loss 0.319304\nbatch 1146: loss 0.182611\nbatch 1147: loss 0.133673\nbatch 1148: loss 0.203741\nbatch 1149: loss 0.126668\nbatch 1150: loss 0.119662\nbatch 1151: loss 0.186640\nbatch 1152: loss 0.139905\nbatch 1153: loss 0.219523\nbatch 1154: loss 0.184631\nbatch 1155: loss 0.204487\nbatch 1156: loss 0.076739\nbatch 1157: loss 0.151207\nbatch 1158: loss 0.135014\nbatch 1159: loss 0.119042\nbatch 1160: loss 0.118406\nbatch 1161: loss 0.351821\nbatch 1162: loss 0.061940\nbatch 1163: loss 0.253970\nbatch 1164: loss 0.167846\nbatch 1165: loss 0.139168\nbatch 1166: loss 0.247836\nbatch 1167: loss 0.057595\nbatch 1168: loss 0.129493\nbatch 1169: loss 0.215815\nbatch 1170: loss 0.223963\nbatch 1171: loss 0.339548\nbatch 1172: loss 0.297872\nbatch 1173: loss 0.327555\nbatch 1174: loss 0.433832\nbatch 1175: loss 0.069864\nbatch 1176: loss 0.094290\nbatch 1177: loss 0.125915\nbatch 1178: loss 0.238062\nbatch 1179: loss 0.148865\nbatch 1180: loss 0.152390\nbatch 1181: loss 0.321877\nbatch 1182: loss 0.159312\nbatch 1183: loss 0.121863\nbatch 1184: loss 0.126018\nbatch 1185: loss 0.298994\nbatch 1186: loss 0.197636\nbatch 1187: loss 0.304063\nbatch 1188: loss 0.212336\nbatch 1189: loss 0.087733\nbatch 1190: loss 0.193774\nbatch 1191: loss 0.048554\nbatch 1192: loss 0.204662\nbatch 1193: loss 0.137300\nbatch 1194: loss 0.127973\nbatch 1195: loss 0.056515\nbatch 1196: loss 0.170080\nbatch 1197: loss 0.178005\nbatch 1198: loss 0.201939\nbatch 1199: loss 0.158659\nbatch 1200: loss 0.057527\nbatch 1201: loss 0.066631\nbatch 1202: loss 0.239361\nbatch 1203: loss 0.295863\nbatch 1204: loss 0.202713\nbatch 1205: loss 0.175453\nbatch 1206: loss 0.151113\nbatch 1207: loss 0.111702\nbatch 1208: loss 0.115063\nbatch 1209: loss 0.405141\nbatch 1210: loss 0.194995\nbatch 1211: loss 0.108887\nbatch 1212: loss 0.110585\nbatch 1213: loss 0.178238\nbatch 1214: loss 0.456279\nbatch 1215: loss 0.215396\nbatch 1216: loss 0.375919\nbatch 1217: loss 0.292410\nbatch 1218: loss 0.233430\nbatch 1219: loss 0.082915\nbatch 1220: loss 0.044667\nbatch 1221: loss 0.205689\nbatch 1222: loss 0.153146\nbatch 1223: loss 0.102365\nbatch 1224: loss 0.178039\nbatch 1225: loss 0.204808\nbatch 1226: loss 0.068649\nbatch 1227: loss 0.122761\nbatch 1228: loss 0.168883\nbatch 1229: loss 0.215185\nbatch 1230: loss 0.059585\nbatch 1231: loss 0.099353\nbatch 1232: loss 0.044335\nbatch 1233: loss 0.235951\nbatch 1234: loss 0.119594\nbatch 1235: loss 0.209500\nbatch 1236: loss 0.259497\nbatch 1237: loss 0.183863\nbatch 1238: loss 0.147382\nbatch 1239: loss 0.106911\nbatch 1240: loss 0.143807\nbatch 1241: loss 0.099386\nbatch 1242: loss 0.070967\nbatch 1243: loss 0.061872\nbatch 1244: loss 0.241676\nbatch 1245: loss 0.119739\nbatch 1246: loss 0.078217\nbatch 1247: loss 0.121922\nbatch 1248: loss 0.160528\nbatch 1249: loss 0.079236\nbatch 1250: loss 0.188721\nbatch 1251: loss 0.198645\nbatch 1252: loss 0.216188\nbatch 1253: loss 0.114762\nbatch 1254: loss 0.314956\nbatch 1255: loss 0.084592\nbatch 1256: loss 0.187403\nbatch 1257: loss 0.129780\nbatch 1258: loss 0.185557\nbatch 1259: loss 0.148457\nbatch 1260: loss 0.072056\nbatch 1261: loss 0.109900\nbatch 1262: loss 0.136295\nbatch 1263: loss 0.101410\nbatch 1264: loss 0.060827\nbatch 1265: loss 0.246105\nbatch 1266: loss 0.103243\nbatch 1267: loss 0.180316\nbatch 1268: loss 0.276934\nbatch 1269: loss 0.241241\nbatch 1270: loss 0.101124\nbatch 1271: loss 0.202732\nbatch 1272: loss 0.207178\nbatch 1273: loss 0.192073\nbatch 1274: loss 0.119625\nbatch 1275: loss 0.090027\nbatch 1276: loss 0.181500\nbatch 1277: loss 0.077355\nbatch 1278: loss 0.099186\nbatch 1279: loss 0.076298\nbatch 1280: loss 0.323314\nbatch 1281: loss 0.222952\nbatch 1282: loss 0.090059\nbatch 1283: loss 0.149054\nbatch 1284: loss 0.222186\nbatch 1285: loss 0.230546\nbatch 1286: loss 0.179495\nbatch 1287: loss 0.126828\nbatch 1288: loss 0.042617\nbatch 1289: loss 0.199431\nbatch 1290: loss 0.034311\nbatch 1291: loss 0.162598\nbatch 1292: loss 0.104663\nbatch 1293: loss 0.103817\nbatch 1294: loss 0.149288\nbatch 1295: loss 0.164269\nbatch 1296: loss 0.191383\nbatch 1297: loss 0.179974\nbatch 1298: loss 0.217316\nbatch 1299: loss 0.226904\nbatch 1300: loss 0.233357\nbatch 1301: loss 0.120341\nbatch 1302: loss 0.246057\nbatch 1303: loss 0.212662\nbatch 1304: loss 0.039704\nbatch 1305: loss 0.087358\nbatch 1306: loss 0.102482\nbatch 1307: loss 0.050334\nbatch 1308: loss 0.109127\nbatch 1309: loss 0.076433\nbatch 1310: loss 0.224881\nbatch 1311: loss 0.155667\nbatch 1312: loss 0.290537\nbatch 1313: loss 0.170355\nbatch 1314: loss 0.078326\nbatch 1315: loss 0.116402\nbatch 1316: loss 0.094977\nbatch 1317: loss 0.080414\nbatch 1318: loss 0.182958\nbatch 1319: loss 0.084502\nbatch 1320: loss 0.144475\nbatch 1321: loss 0.314457\nbatch 1322: loss 0.074945\nbatch 1323: loss 0.129656\nbatch 1324: loss 0.202856\nbatch 1325: loss 0.085085\nbatch 1326: loss 0.159788\nbatch 1327: loss 0.088743\nbatch 1328: loss 0.148562\nbatch 1329: loss 0.061326\nbatch 1330: loss 0.067049\nbatch 1331: loss 0.079137\nbatch 1332: loss 0.149813\nbatch 1333: loss 0.156012\nbatch 1334: loss 0.106383\nbatch 1335: loss 0.195407\nbatch 1336: loss 0.117331\nbatch 1337: loss 0.144202\nbatch 1338: loss 0.035008\nbatch 1339: loss 0.151514\nbatch 1340: loss 0.275237\nbatch 1341: loss 0.108586\nbatch 1342: loss 0.285805\nbatch 1343: loss 0.089338\nbatch 1344: loss 0.090068\nbatch 1345: loss 0.173817\nbatch 1346: loss 0.158978\nbatch 1347: loss 0.072520\nbatch 1348: loss 0.238436\nbatch 1349: loss 0.071986\nbatch 1350: loss 0.133349\nbatch 1351: loss 0.158019\nbatch 1352: loss 0.383180\nbatch 1353: loss 0.181066\nbatch 1354: loss 0.088203\nbatch 1355: loss 0.114737\nbatch 1356: loss 0.060488\nbatch 1357: loss 0.072827\nbatch 1358: loss 0.155710\nbatch 1359: loss 0.329212\nbatch 1360: loss 0.365677\nbatch 1361: loss 0.172669\nbatch 1362: loss 0.208262\nbatch 1363: loss 0.101943\nbatch 1364: loss 0.157510\nbatch 1365: loss 0.083603\nbatch 1366: loss 0.087421\nbatch 1367: loss 0.282993\nbatch 1368: loss 0.131696\nbatch 1369: loss 0.049725\nbatch 1370: loss 0.205607\nbatch 1371: loss 0.139240\nbatch 1372: loss 0.067435\nbatch 1373: loss 0.190844\nbatch 1374: loss 0.151582\nbatch 1375: loss 0.123935\nbatch 1376: loss 0.187806\nbatch 1377: loss 0.154925\nbatch 1378: loss 0.144847\nbatch 1379: loss 0.123899\nbatch 1380: loss 0.058784\nbatch 1381: loss 0.095483\nbatch 1382: loss 0.213942\nbatch 1383: loss 0.044768\nbatch 1384: loss 0.026207\nbatch 1385: loss 0.131318\nbatch 1386: loss 0.113887\nbatch 1387: loss 0.161639\nbatch 1388: loss 0.358534\nbatch 1389: loss 0.106709\nbatch 1390: loss 0.124312\nbatch 1391: loss 0.167660\nbatch 1392: loss 0.161695\nbatch 1393: loss 0.090013\nbatch 1394: loss 0.171644\nbatch 1395: loss 0.206010\nbatch 1396: loss 0.151351\nbatch 1397: loss 0.108932\nbatch 1398: loss 0.093417\nbatch 1399: loss 0.043502\nbatch 1400: loss 0.135936\nbatch 1401: loss 0.027241\nbatch 1402: loss 0.147960\nbatch 1403: loss 0.227250\nbatch 1404: loss 0.060430\nbatch 1405: loss 0.113846\nbatch 1406: loss 0.271906\nbatch 1407: loss 0.223470\nbatch 1408: loss 0.126127\nbatch 1409: loss 0.118032\nbatch 1410: loss 0.153849\nbatch 1411: loss 0.067678\nbatch 1412: loss 0.353581\nbatch 1413: loss 0.134555\nbatch 1414: loss 0.085473\nbatch 1415: loss 0.101222\nbatch 1416: loss 0.265397\nbatch 1417: loss 0.121898\nbatch 1418: loss 0.167279\nbatch 1419: loss 0.161637\nbatch 1420: loss 0.064142\nbatch 1421: loss 0.194803\nbatch 1422: loss 0.244527\nbatch 1423: loss 0.208915\nbatch 1424: loss 0.158811\nbatch 1425: loss 0.161438\nbatch 1426: loss 0.148854\nbatch 1427: loss 0.068200\nbatch 1428: loss 0.140407\nbatch 1429: loss 0.041441\nbatch 1430: loss 0.087957\nbatch 1431: loss 0.263506\nbatch 1432: loss 0.136997\nbatch 1433: loss 0.036348\nbatch 1434: loss 0.171637\nbatch 1435: loss 0.170839\nbatch 1436: loss 0.040981\nbatch 1437: loss 0.141913\nbatch 1438: loss 0.058457\nbatch 1439: loss 0.111420\nbatch 1440: loss 0.072916\nbatch 1441: loss 0.134955\nbatch 1442: loss 0.349068\nbatch 1443: loss 0.403454\nbatch 1444: loss 0.046174\nbatch 1445: loss 0.140343\nbatch 1446: loss 0.361651\nbatch 1447: loss 0.073325\nbatch 1448: loss 0.201574\nbatch 1449: loss 0.282763\nbatch 1450: loss 0.166776\nbatch 1451: loss 0.049856\nbatch 1452: loss 0.442807\nbatch 1453: loss 0.056928\nbatch 1454: loss 0.193937\nbatch 1455: loss 0.186638\nbatch 1456: loss 0.212456\nbatch 1457: loss 0.132587\nbatch 1458: loss 0.061240\nbatch 1459: loss 0.125790\nbatch 1460: loss 0.086778\nbatch 1461: loss 0.223342\nbatch 1462: loss 0.073102\nbatch 1463: loss 0.172625\nbatch 1464: loss 0.049398\nbatch 1465: loss 0.056480\nbatch 1466: loss 0.175554\nbatch 1467: loss 0.096673\nbatch 1468: loss 0.138041\nbatch 1469: loss 0.271026\nbatch 1470: loss 0.184490\nbatch 1471: loss 0.334884\nbatch 1472: loss 0.227595\nbatch 1473: loss 0.195948\nbatch 1474: loss 0.104903\nbatch 1475: loss 0.132554\nbatch 1476: loss 0.074199\nbatch 1477: loss 0.083951\nbatch 1478: loss 0.085383\nbatch 1479: loss 0.110166\nbatch 1480: loss 0.085300\nbatch 1481: loss 0.223421\nbatch 1482: loss 0.054771\nbatch 1483: loss 0.200337\nbatch 1484: loss 0.050381\nbatch 1485: loss 0.074962\nbatch 1486: loss 0.246032\nbatch 1487: loss 0.073304\nbatch 1488: loss 0.113620\nbatch 1489: loss 0.111328\nbatch 1490: loss 0.114153\nbatch 1491: loss 0.176752\nbatch 1492: loss 0.056558\nbatch 1493: loss 0.127653\nbatch 1494: loss 0.204782\nbatch 1495: loss 0.098405\nbatch 1496: loss 0.139801\nbatch 1497: loss 0.171365\nbatch 1498: loss 0.065936\nbatch 1499: loss 0.193868\nbatch 1500: loss 0.067943\nbatch 1501: loss 0.373573\nbatch 1502: loss 0.083531\nbatch 1503: loss 0.169613\nbatch 1504: loss 0.031984\nbatch 1505: loss 0.128734\nbatch 1506: loss 0.067587\nbatch 1507: loss 0.160154\nbatch 1508: loss 0.166314\nbatch 1509: loss 0.126860\nbatch 1510: loss 0.244981\nbatch 1511: loss 0.067589\nbatch 1512: loss 0.177220\nbatch 1513: loss 0.242172\nbatch 1514: loss 0.105581\nbatch 1515: loss 0.098557\nbatch 1516: loss 0.032413\nbatch 1517: loss 0.295351\nbatch 1518: loss 0.332770\nbatch 1519: loss 0.106635\nbatch 1520: loss 0.123111\nbatch 1521: loss 0.223616\nbatch 1522: loss 0.166255\nbatch 1523: loss 0.135530\nbatch 1524: loss 0.169617\nbatch 1525: loss 0.319859\nbatch 1526: loss 0.260895\nbatch 1527: loss 0.117953\nbatch 1528: loss 0.169725\nbatch 1529: loss 0.051938\nbatch 1530: loss 0.142080\nbatch 1531: loss 0.070138\nbatch 1532: loss 0.129309\nbatch 1533: loss 0.065802\nbatch 1534: loss 0.310964\nbatch 1535: loss 0.124325\nbatch 1536: loss 0.281955\nbatch 1537: loss 0.205220\nbatch 1538: loss 0.105222\nbatch 1539: loss 0.061550\nbatch 1540: loss 0.094480\nbatch 1541: loss 0.072762\nbatch 1542: loss 0.227720\nbatch 1543: loss 0.101869\nbatch 1544: loss 0.149875\nbatch 1545: loss 0.045095\nbatch 1546: loss 0.124846\nbatch 1547: loss 0.071393\nbatch 1548: loss 0.263223\nbatch 1549: loss 0.066497\nbatch 1550: loss 0.018484\nbatch 1551: loss 0.162232\nbatch 1552: loss 0.073881\nbatch 1553: loss 0.086745\nbatch 1554: loss 0.250906\nbatch 1555: loss 0.316796\nbatch 1556: loss 0.170762\nbatch 1557: loss 0.070687\nbatch 1558: loss 0.226285\nbatch 1559: loss 0.042845\nbatch 1560: loss 0.272982\nbatch 1561: loss 0.175275\nbatch 1562: loss 0.203419\nbatch 1563: loss 0.046091\nbatch 1564: loss 0.085855\nbatch 1565: loss 0.279768\nbatch 1566: loss 0.267686\nbatch 1567: loss 0.098631\nbatch 1568: loss 0.323532\nbatch 1569: loss 0.316258\nbatch 1570: loss 0.128388\nbatch 1571: loss 0.156156\nbatch 1572: loss 0.067399\nbatch 1573: loss 0.084954\nbatch 1574: loss 0.050930\nbatch 1575: loss 0.052628\nbatch 1576: loss 0.252350\nbatch 1577: loss 0.240127\nbatch 1578: loss 0.116356\nbatch 1579: loss 0.059999\nbatch 1580: loss 0.059138\nbatch 1581: loss 0.075477\nbatch 1582: loss 0.156270\nbatch 1583: loss 0.130352\nbatch 1584: loss 0.166613\nbatch 1585: loss 0.177924\nbatch 1586: loss 0.167287\nbatch 1587: loss 0.078113\nbatch 1588: loss 0.132710\nbatch 1589: loss 0.065202\nbatch 1590: loss 0.150624\nbatch 1591: loss 0.138538\nbatch 1592: loss 0.026458\nbatch 1593: loss 0.135037\nbatch 1594: loss 0.145239\nbatch 1595: loss 0.199954\nbatch 1596: loss 0.106114\nbatch 1597: loss 0.166718\nbatch 1598: loss 0.033807\nbatch 1599: loss 0.135695\nbatch 1600: loss 0.105491\nbatch 1601: loss 0.023846\nbatch 1602: loss 0.122538\nbatch 1603: loss 0.282346\nbatch 1604: loss 0.222211\nbatch 1605: loss 0.149137\nbatch 1606: loss 0.215130\nbatch 1607: loss 0.111519\nbatch 1608: loss 0.142803\nbatch 1609: loss 0.091796\nbatch 1610: loss 0.056376\nbatch 1611: loss 0.076025\nbatch 1612: loss 0.158699\nbatch 1613: loss 0.223542\nbatch 1614: loss 0.299938\nbatch 1615: loss 0.051778\nbatch 1616: loss 0.326036\nbatch 1617: loss 0.112040\nbatch 1618: loss 0.182789\nbatch 1619: loss 0.078892\nbatch 1620: loss 0.060284\nbatch 1621: loss 0.043222\nbatch 1622: loss 0.205435\nbatch 1623: loss 0.199029\nbatch 1624: loss 0.188738\nbatch 1625: loss 0.092539\nbatch 1626: loss 0.130759\nbatch 1627: loss 0.137500\nbatch 1628: loss 0.060705\nbatch 1629: loss 0.089530\nbatch 1630: loss 0.100268\nbatch 1631: loss 0.034952\nbatch 1632: loss 0.134042\nbatch 1633: loss 0.043620\nbatch 1634: loss 0.110084\nbatch 1635: loss 0.083792\nbatch 1636: loss 0.264239\nbatch 1637: loss 0.044856\nbatch 1638: loss 0.223906\nbatch 1639: loss 0.299948\nbatch 1640: loss 0.190232\nbatch 1641: loss 0.135617\nbatch 1642: loss 0.094353\nbatch 1643: loss 0.105740\nbatch 1644: loss 0.088110\nbatch 1645: loss 0.138510\nbatch 1646: loss 0.116407\nbatch 1647: loss 0.101016\nbatch 1648: loss 0.197392\nbatch 1649: loss 0.179140\nbatch 1650: loss 0.133098\nbatch 1651: loss 0.170886\nbatch 1652: loss 0.312445\nbatch 1653: loss 0.090581\nbatch 1654: loss 0.173874\nbatch 1655: loss 0.170793\nbatch 1656: loss 0.071192\nbatch 1657: loss 0.070332\nbatch 1658: loss 0.140612\nbatch 1659: loss 0.188475\nbatch 1660: loss 0.116071\nbatch 1661: loss 0.076099\nbatch 1662: loss 0.120539\nbatch 1663: loss 0.090774\nbatch 1664: loss 0.135504\nbatch 1665: loss 0.183301\nbatch 1666: loss 0.180372\nbatch 1667: loss 0.146037\nbatch 1668: loss 0.188491\nbatch 1669: loss 0.160445\nbatch 1670: loss 0.143845\nbatch 1671: loss 0.087139\nbatch 1672: loss 0.040511\nbatch 1673: loss 0.074215\nbatch 1674: loss 0.263080\nbatch 1675: loss 0.045759\nbatch 1676: loss 0.369232\nbatch 1677: loss 0.090095\nbatch 1678: loss 0.137385\nbatch 1679: loss 0.200896\nbatch 1680: loss 0.084518\nbatch 1681: loss 0.089530\nbatch 1682: loss 0.038353\nbatch 1683: loss 0.066038\nbatch 1684: loss 0.366885\nbatch 1685: loss 0.081251\nbatch 1686: loss 0.229301\nbatch 1687: loss 0.089203\nbatch 1688: loss 0.147315\nbatch 1689: loss 0.111134\nbatch 1690: loss 0.105883\nbatch 1691: loss 0.171984\nbatch 1692: loss 0.041543\nbatch 1693: loss 0.129988\nbatch 1694: loss 0.076774\nbatch 1695: loss 0.103276\nbatch 1696: loss 0.034778\nbatch 1697: loss 0.216547\nbatch 1698: loss 0.096075\nbatch 1699: loss 0.056325\nbatch 1700: loss 0.179158\nbatch 1701: loss 0.034523\nbatch 1702: loss 0.102419\nbatch 1703: loss 0.208498\nbatch 1704: loss 0.218125\nbatch 1705: loss 0.313097\nbatch 1706: loss 0.070261\nbatch 1707: loss 0.096690\nbatch 1708: loss 0.172761\nbatch 1709: loss 0.161449\nbatch 1710: loss 0.112637\nbatch 1711: loss 0.035839\nbatch 1712: loss 0.056868\nbatch 1713: loss 0.399204\nbatch 1714: loss 0.109838\nbatch 1715: loss 0.107315\nbatch 1716: loss 0.159676\nbatch 1717: loss 0.257750\nbatch 1718: loss 0.136556\nbatch 1719: loss 0.134750\nbatch 1720: loss 0.224041\nbatch 1721: loss 0.250269\nbatch 1722: loss 0.218350\nbatch 1723: loss 0.140447\nbatch 1724: loss 0.192994\nbatch 1725: loss 0.087426\nbatch 1726: loss 0.090143\nbatch 1727: loss 0.078850\nbatch 1728: loss 0.142503\nbatch 1729: loss 0.119079\nbatch 1730: loss 0.116794\nbatch 1731: loss 0.062663\nbatch 1732: loss 0.099542\nbatch 1733: loss 0.130488\nbatch 1734: loss 0.048341\nbatch 1735: loss 0.107689\nbatch 1736: loss 0.161244\nbatch 1737: loss 0.178909\nbatch 1738: loss 0.190889\nbatch 1739: loss 0.078625\nbatch 1740: loss 0.118031\nbatch 1741: loss 0.117682\nbatch 1742: loss 0.074719\nbatch 1743: loss 0.101574\nbatch 1744: loss 0.105531\nbatch 1745: loss 0.082940\nbatch 1746: loss 0.097528\nbatch 1747: loss 0.127166\nbatch 1748: loss 0.372057\nbatch 1749: loss 0.037591\nbatch 1750: loss 0.135776\nbatch 1751: loss 0.106666\nbatch 1752: loss 0.062776\nbatch 1753: loss 0.163271\nbatch 1754: loss 0.054711\nbatch 1755: loss 0.181468\nbatch 1756: loss 0.118719\nbatch 1757: loss 0.096474\nbatch 1758: loss 0.177636\nbatch 1759: loss 0.117505\nbatch 1760: loss 0.070871\nbatch 1761: loss 0.070299\nbatch 1762: loss 0.093885\nbatch 1763: loss 0.229198\nbatch 1764: loss 0.164013\nbatch 1765: loss 0.050751\nbatch 1766: loss 0.070767\nbatch 1767: loss 0.158210\nbatch 1768: loss 0.053186\nbatch 1769: loss 0.177763\nbatch 1770: loss 0.091989\nbatch 1771: loss 0.124035\nbatch 1772: loss 0.156193\nbatch 1773: loss 0.152763\nbatch 1774: loss 0.058325\nbatch 1775: loss 0.252223\nbatch 1776: loss 0.165917\nbatch 1777: loss 0.156356\nbatch 1778: loss 0.067649\nbatch 1779: loss 0.056683\nbatch 1780: loss 0.113961\nbatch 1781: loss 0.130950\nbatch 1782: loss 0.074563\nbatch 1783: loss 0.090217\nbatch 1784: loss 0.347535\nbatch 1785: loss 0.217289\nbatch 1786: loss 0.052428\nbatch 1787: loss 0.073071\nbatch 1788: loss 0.045778\nbatch 1789: loss 0.143138\nbatch 1790: loss 0.283140\nbatch 1791: loss 0.165144\nbatch 1792: loss 0.160652\nbatch 1793: loss 0.179779\nbatch 1794: loss 0.209323\nbatch 1795: loss 0.185784\nbatch 1796: loss 0.046738\nbatch 1797: loss 0.084297\nbatch 1798: loss 0.073651\nbatch 1799: loss 0.059147\nbatch 1800: loss 0.108352\nbatch 1801: loss 0.199218\nbatch 1802: loss 0.164189\nbatch 1803: loss 0.052120\nbatch 1804: loss 0.041858\nbatch 1805: loss 0.040791\nbatch 1806: loss 0.264792\nbatch 1807: loss 0.039177\nbatch 1808: loss 0.012181\nbatch 1809: loss 0.095402\nbatch 1810: loss 0.081484\nbatch 1811: loss 0.060052\nbatch 1812: loss 0.042358\nbatch 1813: loss 0.214642\nbatch 1814: loss 0.069693\nbatch 1815: loss 0.106516\nbatch 1816: loss 0.118471\nbatch 1817: loss 0.179463\nbatch 1818: loss 0.122413\nbatch 1819: loss 0.051865\nbatch 1820: loss 0.080972\nbatch 1821: loss 0.075685\nbatch 1822: loss 0.056151\nbatch 1823: loss 0.074795\nbatch 1824: loss 0.229037\nbatch 1825: loss 0.125459\nbatch 1826: loss 0.108481\nbatch 1827: loss 0.105910\nbatch 1828: loss 0.069914\nbatch 1829: loss 0.074983\nbatch 1830: loss 0.116140\nbatch 1831: loss 0.165648\nbatch 1832: loss 0.038315\nbatch 1833: loss 0.043347\nbatch 1834: loss 0.064977\nbatch 1835: loss 0.294955\nbatch 1836: loss 0.110416\nbatch 1837: loss 0.068603\nbatch 1838: loss 0.160557\nbatch 1839: loss 0.051925\nbatch 1840: loss 0.130004\nbatch 1841: loss 0.054379\nbatch 1842: loss 0.077859\nbatch 1843: loss 0.195668\nbatch 1844: loss 0.069047\nbatch 1845: loss 0.158818\nbatch 1846: loss 0.180149\nbatch 1847: loss 0.054077\nbatch 1848: loss 0.085065\nbatch 1849: loss 0.153359\nbatch 1850: loss 0.030761\nbatch 1851: loss 0.258434\nbatch 1852: loss 0.072958\nbatch 1853: loss 0.170352\nbatch 1854: loss 0.087569\nbatch 1855: loss 0.245388\nbatch 1856: loss 0.200191\nbatch 1857: loss 0.219491\nbatch 1858: loss 0.176265\nbatch 1859: loss 0.244516\nbatch 1860: loss 0.075336\nbatch 1861: loss 0.249817\nbatch 1862: loss 0.033277\nbatch 1863: loss 0.106167\nbatch 1864: loss 0.125661\nbatch 1865: loss 0.105984\nbatch 1866: loss 0.116379\nbatch 1867: loss 0.038542\nbatch 1868: loss 0.041423\nbatch 1869: loss 0.123254\nbatch 1870: loss 0.219028\nbatch 1871: loss 0.039071\nbatch 1872: loss 0.116431\nbatch 1873: loss 0.179716\nbatch 1874: loss 0.091485\nbatch 1875: loss 0.142536\nbatch 1876: loss 0.053697\nbatch 1877: loss 0.049582\nbatch 1878: loss 0.039171\nbatch 1879: loss 0.147749\nbatch 1880: loss 0.148482\nbatch 1881: loss 0.035305\nbatch 1882: loss 0.149237\nbatch 1883: loss 0.221674\nbatch 1884: loss 0.124611\nbatch 1885: loss 0.047674\nbatch 1886: loss 0.114730\nbatch 1887: loss 0.031070\nbatch 1888: loss 0.249503\nbatch 1889: loss 0.029535\nbatch 1890: loss 0.089287\nbatch 1891: loss 0.161713\nbatch 1892: loss 0.176594\nbatch 1893: loss 0.062994\nbatch 1894: loss 0.045783\nbatch 1895: loss 0.170782\nbatch 1896: loss 0.152454\nbatch 1897: loss 0.159894\nbatch 1898: loss 0.064036\nbatch 1899: loss 0.098752\nbatch 1900: loss 0.079641\nbatch 1901: loss 0.089821\nbatch 1902: loss 0.098600\nbatch 1903: loss 0.077301\nbatch 1904: loss 0.045340\nbatch 1905: loss 0.069823\nbatch 1906: loss 0.117374\nbatch 1907: loss 0.397197\nbatch 1908: loss 0.184773\nbatch 1909: loss 0.128028\nbatch 1910: loss 0.044520\nbatch 1911: loss 0.131747\nbatch 1912: loss 0.055526\nbatch 1913: loss 0.097785\nbatch 1914: loss 0.141876\nbatch 1915: loss 0.094185\nbatch 1916: loss 0.188243\nbatch 1917: loss 0.239114\nbatch 1918: loss 0.087452\nbatch 1919: loss 0.192090\nbatch 1920: loss 0.055119\nbatch 1921: loss 0.075957\nbatch 1922: loss 0.070447\nbatch 1923: loss 0.088847\nbatch 1924: loss 0.232021\nbatch 1925: loss 0.057556\nbatch 1926: loss 0.038216\nbatch 1927: loss 0.160622\nbatch 1928: loss 0.060114\nbatch 1929: loss 0.142777\nbatch 1930: loss 0.103879\nbatch 1931: loss 0.120142\nbatch 1932: loss 0.056373\nbatch 1933: loss 0.168933\nbatch 1934: loss 0.082298\nbatch 1935: loss 0.302873\nbatch 1936: loss 0.050983\nbatch 1937: loss 0.044958\nbatch 1938: loss 0.105662\nbatch 1939: loss 0.068246\nbatch 1940: loss 0.183631\nbatch 1941: loss 0.093929\nbatch 1942: loss 0.078421\nbatch 1943: loss 0.053653\nbatch 1944: loss 0.034445\nbatch 1945: loss 0.105185\nbatch 1946: loss 0.088628\nbatch 1947: loss 0.063155\nbatch 1948: loss 0.055042\nbatch 1949: loss 0.085763\nbatch 1950: loss 0.088806\nbatch 1951: loss 0.107227\nbatch 1952: loss 0.213745\nbatch 1953: loss 0.012995\nbatch 1954: loss 0.250901\nbatch 1955: loss 0.081365\nbatch 1956: loss 0.055996\nbatch 1957: loss 0.101625\nbatch 1958: loss 0.099686\nbatch 1959: loss 0.159460\nbatch 1960: loss 0.068753\nbatch 1961: loss 0.111464\nbatch 1962: loss 0.182887\nbatch 1963: loss 0.129073\nbatch 1964: loss 0.095786\nbatch 1965: loss 0.226592\nbatch 1966: loss 0.107665\nbatch 1967: loss 0.140821\nbatch 1968: loss 0.135071\nbatch 1969: loss 0.108046\nbatch 1970: loss 0.072988\nbatch 1971: loss 0.076133\nbatch 1972: loss 0.409847\nbatch 1973: loss 0.104600\nbatch 1974: loss 0.172850\nbatch 1975: loss 0.156614\nbatch 1976: loss 0.120276\nbatch 1977: loss 0.294092\nbatch 1978: loss 0.040400\nbatch 1979: loss 0.184762\nbatch 1980: loss 0.159166\nbatch 1981: loss 0.125464\nbatch 1982: loss 0.110157\nbatch 1983: loss 0.116699\nbatch 1984: loss 0.179050\nbatch 1985: loss 0.111528\nbatch 1986: loss 0.242980\nbatch 1987: loss 0.101456\nbatch 1988: loss 0.124083\nbatch 1989: loss 0.070688\nbatch 1990: loss 0.068309\nbatch 1991: loss 0.201139\nbatch 1992: loss 0.172714\nbatch 1993: loss 0.260794\nbatch 1994: loss 0.213429\nbatch 1995: loss 0.034231\nbatch 1996: loss 0.103945\nbatch 1997: loss 0.043852\nbatch 1998: loss 0.162212\nbatch 1999: loss 0.118345\nbatch 2000: loss 0.099801\nbatch 2001: loss 0.117550\nbatch 2002: loss 0.121649\nbatch 2003: loss 0.076137\nbatch 2004: loss 0.053568\nbatch 2005: loss 0.106507\nbatch 2006: loss 0.136887\nbatch 2007: loss 0.135367\nbatch 2008: loss 0.080878\nbatch 2009: loss 0.055990\nbatch 2010: loss 0.045424\nbatch 2011: loss 0.088845\nbatch 2012: loss 0.043518\nbatch 2013: loss 0.181097\nbatch 2014: loss 0.112270\nbatch 2015: loss 0.089554\nbatch 2016: loss 0.126148\nbatch 2017: loss 0.213931\nbatch 2018: loss 0.131840\nbatch 2019: loss 0.132186\nbatch 2020: loss 0.127939\nbatch 2021: loss 0.115083\nbatch 2022: loss 0.077384\nbatch 2023: loss 0.047261\nbatch 2024: loss 0.117580\nbatch 2025: loss 0.057511\nbatch 2026: loss 0.100288\nbatch 2027: loss 0.050494\nbatch 2028: loss 0.140605\nbatch 2029: loss 0.085217\nbatch 2030: loss 0.143036\nbatch 2031: loss 0.167178\nbatch 2032: loss 0.055348\nbatch 2033: loss 0.057698\nbatch 2034: loss 0.086289\nbatch 2035: loss 0.065926\nbatch 2036: loss 0.087779\nbatch 2037: loss 0.095253\nbatch 2038: loss 0.046297\nbatch 2039: loss 0.075541\nbatch 2040: loss 0.136054\nbatch 2041: loss 0.052063\nbatch 2042: loss 0.104310\nbatch 2043: loss 0.040114\nbatch 2044: loss 0.105298\nbatch 2045: loss 0.069492\nbatch 2046: loss 0.091104\nbatch 2047: loss 0.086986\nbatch 2048: loss 0.139647\nbatch 2049: loss 0.168638\nbatch 2050: loss 0.029907\nbatch 2051: loss 0.223844\nbatch 2052: loss 0.054940\nbatch 2053: loss 0.071488\nbatch 2054: loss 0.019379\nbatch 2055: loss 0.223432\nbatch 2056: loss 0.188931\nbatch 2057: loss 0.107153\nbatch 2058: loss 0.237211\nbatch 2059: loss 0.040990\nbatch 2060: loss 0.037086\nbatch 2061: loss 0.031749\nbatch 2062: loss 0.189404\nbatch 2063: loss 0.054488\nbatch 2064: loss 0.231051\nbatch 2065: loss 0.035488\nbatch 2066: loss 0.106608\nbatch 2067: loss 0.068480\nbatch 2068: loss 0.098740\nbatch 2069: loss 0.021819\nbatch 2070: loss 0.062418\nbatch 2071: loss 0.109029\nbatch 2072: loss 0.039535\nbatch 2073: loss 0.097327\nbatch 2074: loss 0.085248\nbatch 2075: loss 0.109630\nbatch 2076: loss 0.266461\nbatch 2077: loss 0.077677\nbatch 2078: loss 0.167701\nbatch 2079: loss 0.162781\nbatch 2080: loss 0.054092\nbatch 2081: loss 0.163317\nbatch 2082: loss 0.215065\nbatch 2083: loss 0.028323\nbatch 2084: loss 0.108646\nbatch 2085: loss 0.124446\nbatch 2086: loss 0.102016\nbatch 2087: loss 0.039120\nbatch 2088: loss 0.037539\nbatch 2089: loss 0.186389\nbatch 2090: loss 0.083818\nbatch 2091: loss 0.162352\nbatch 2092: loss 0.171544\nbatch 2093: loss 0.116858\nbatch 2094: loss 0.063018\nbatch 2095: loss 0.077614\nbatch 2096: loss 0.197045\nbatch 2097: loss 0.204948\nbatch 2098: loss 0.190535\nbatch 2099: loss 0.108149\nbatch 2100: loss 0.040935\nbatch 2101: loss 0.088851\nbatch 2102: loss 0.066368\nbatch 2103: loss 0.156774\nbatch 2104: loss 0.160701\nbatch 2105: loss 0.216518\nbatch 2106: loss 0.044270\nbatch 2107: loss 0.112050\nbatch 2108: loss 0.067445\nbatch 2109: loss 0.144916\nbatch 2110: loss 0.323275\nbatch 2111: loss 0.232285\nbatch 2112: loss 0.120042\nbatch 2113: loss 0.068352\nbatch 2114: loss 0.022879\nbatch 2115: loss 0.083588\nbatch 2116: loss 0.049247\nbatch 2117: loss 0.069721\nbatch 2118: loss 0.278008\nbatch 2119: loss 0.207941\nbatch 2120: loss 0.113306\nbatch 2121: loss 0.093082\nbatch 2122: loss 0.052260\nbatch 2123: loss 0.129949\nbatch 2124: loss 0.094349\nbatch 2125: loss 0.101061\nbatch 2126: loss 0.022710\nbatch 2127: loss 0.054315\nbatch 2128: loss 0.099191\nbatch 2129: loss 0.195863\nbatch 2130: loss 0.106258\nbatch 2131: loss 0.084452\nbatch 2132: loss 0.214441\nbatch 2133: loss 0.124368\nbatch 2134: loss 0.076658\nbatch 2135: loss 0.127517\nbatch 2136: loss 0.123186\nbatch 2137: loss 0.028855\nbatch 2138: loss 0.094900\nbatch 2139: loss 0.091339\nbatch 2140: loss 0.117207\nbatch 2141: loss 0.154731\nbatch 2142: loss 0.316819\nbatch 2143: loss 0.127391\nbatch 2144: loss 0.040678\nbatch 2145: loss 0.057417\nbatch 2146: loss 0.028939\nbatch 2147: loss 0.057256\nbatch 2148: loss 0.082537\nbatch 2149: loss 0.270890\nbatch 2150: loss 0.171724\nbatch 2151: loss 0.118603\nbatch 2152: loss 0.046658\nbatch 2153: loss 0.047429\nbatch 2154: loss 0.128118\nbatch 2155: loss 0.174142\nbatch 2156: loss 0.169153\nbatch 2157: loss 0.211276\nbatch 2158: loss 0.054728\nbatch 2159: loss 0.071654\nbatch 2160: loss 0.195510\nbatch 2161: loss 0.099759\nbatch 2162: loss 0.107017\nbatch 2163: loss 0.046577\nbatch 2164: loss 0.048809\nbatch 2165: loss 0.169562\nbatch 2166: loss 0.140188\nbatch 2167: loss 0.177018\nbatch 2168: loss 0.094916\nbatch 2169: loss 0.078423\nbatch 2170: loss 0.046259\nbatch 2171: loss 0.222049\nbatch 2172: loss 0.075821\nbatch 2173: loss 0.031597\nbatch 2174: loss 0.045104\nbatch 2175: loss 0.117715\nbatch 2176: loss 0.097754\nbatch 2177: loss 0.105593\nbatch 2178: loss 0.170776\nbatch 2179: loss 0.047802\nbatch 2180: loss 0.086726\nbatch 2181: loss 0.064146\nbatch 2182: loss 0.053728\nbatch 2183: loss 0.082792\nbatch 2184: loss 0.060306\nbatch 2185: loss 0.164374\nbatch 2186: loss 0.084848\nbatch 2187: loss 0.162745\nbatch 2188: loss 0.047202\nbatch 2189: loss 0.321620\nbatch 2190: loss 0.044670\nbatch 2191: loss 0.054446\nbatch 2192: loss 0.074428\nbatch 2193: loss 0.125714\nbatch 2194: loss 0.105559\nbatch 2195: loss 0.168688\nbatch 2196: loss 0.096728\nbatch 2197: loss 0.175856\nbatch 2198: loss 0.077481\nbatch 2199: loss 0.178769\nbatch 2200: loss 0.050626\nbatch 2201: loss 0.151853\nbatch 2202: loss 0.112574\nbatch 2203: loss 0.112074\nbatch 2204: loss 0.067253\nbatch 2205: loss 0.093848\nbatch 2206: loss 0.190393\nbatch 2207: loss 0.122270\nbatch 2208: loss 0.133001\nbatch 2209: loss 0.117507\nbatch 2210: loss 0.098383\nbatch 2211: loss 0.050633\nbatch 2212: loss 0.094727\nbatch 2213: loss 0.199272\nbatch 2214: loss 0.371252\nbatch 2215: loss 0.044117\nbatch 2216: loss 0.148335\nbatch 2217: loss 0.104065\nbatch 2218: loss 0.025950\nbatch 2219: loss 0.025230\nbatch 2220: loss 0.082230\nbatch 2221: loss 0.072990\nbatch 2222: loss 0.133862\nbatch 2223: loss 0.071249\nbatch 2224: loss 0.095412\nbatch 2225: loss 0.090388\nbatch 2226: loss 0.052083\nbatch 2227: loss 0.070574\nbatch 2228: loss 0.119940\nbatch 2229: loss 0.331446\nbatch 2230: loss 0.048530\nbatch 2231: loss 0.111818\nbatch 2232: loss 0.120275\nbatch 2233: loss 0.070555\nbatch 2234: loss 0.075260\nbatch 2235: loss 0.070783\nbatch 2236: loss 0.135814\nbatch 2237: loss 0.080797\nbatch 2238: loss 0.230642\nbatch 2239: loss 0.117067\nbatch 2240: loss 0.033978\nbatch 2241: loss 0.137043\nbatch 2242: loss 0.160756\nbatch 2243: loss 0.067254\nbatch 2244: loss 0.137866\nbatch 2245: loss 0.013117\nbatch 2246: loss 0.091011\nbatch 2247: loss 0.114157\nbatch 2248: loss 0.145093\nbatch 2249: loss 0.011853\nbatch 2250: loss 0.044016\nbatch 2251: loss 0.203984\nbatch 2252: loss 0.040443\nbatch 2253: loss 0.267859\nbatch 2254: loss 0.045825\nbatch 2255: loss 0.263607\nbatch 2256: loss 0.032824\nbatch 2257: loss 0.035476\nbatch 2258: loss 0.111427\nbatch 2259: loss 0.069930\nbatch 2260: loss 0.078893\nbatch 2261: loss 0.042369\nbatch 2262: loss 0.171617\nbatch 2263: loss 0.162972\nbatch 2264: loss 0.096747\nbatch 2265: loss 0.034975\nbatch 2266: loss 0.115731\nbatch 2267: loss 0.106630\nbatch 2268: loss 0.181339\nbatch 2269: loss 0.137724\nbatch 2270: loss 0.236440\nbatch 2271: loss 0.133825\nbatch 2272: loss 0.048614\nbatch 2273: loss 0.065759\nbatch 2274: loss 0.220944\nbatch 2275: loss 0.233031\nbatch 2276: loss 0.134165\nbatch 2277: loss 0.031560\nbatch 2278: loss 0.087928\nbatch 2279: loss 0.090313\nbatch 2280: loss 0.045827\nbatch 2281: loss 0.242329\nbatch 2282: loss 0.120253\nbatch 2283: loss 0.024921\nbatch 2284: loss 0.019673\nbatch 2285: loss 0.077881\nbatch 2286: loss 0.049019\nbatch 2287: loss 0.308374\nbatch 2288: loss 0.067729\nbatch 2289: loss 0.143257\nbatch 2290: loss 0.208173\nbatch 2291: loss 0.134978\nbatch 2292: loss 0.073019\nbatch 2293: loss 0.158777\nbatch 2294: loss 0.128241\nbatch 2295: loss 0.120979\nbatch 2296: loss 0.192083\nbatch 2297: loss 0.024797\nbatch 2298: loss 0.206165\nbatch 2299: loss 0.081230\nbatch 2300: loss 0.035767\nbatch 2301: loss 0.372301\nbatch 2302: loss 0.083951\nbatch 2303: loss 0.159182\nbatch 2304: loss 0.102079\nbatch 2305: loss 0.132990\nbatch 2306: loss 0.259630\nbatch 2307: loss 0.097127\nbatch 2308: loss 0.061028\nbatch 2309: loss 0.117226\nbatch 2310: loss 0.081015\nbatch 2311: loss 0.105767\nbatch 2312: loss 0.122520\nbatch 2313: loss 0.088505\nbatch 2314: loss 0.262486\nbatch 2315: loss 0.028969\nbatch 2316: loss 0.068813\nbatch 2317: loss 0.057839\nbatch 2318: loss 0.056701\nbatch 2319: loss 0.101041\nbatch 2320: loss 0.134646\nbatch 2321: loss 0.052265\nbatch 2322: loss 0.082732\nbatch 2323: loss 0.104522\nbatch 2324: loss 0.045507\nbatch 2325: loss 0.061100\nbatch 2326: loss 0.164034\nbatch 2327: loss 0.077477\nbatch 2328: loss 0.165566\nbatch 2329: loss 0.056151\nbatch 2330: loss 0.036146\nbatch 2331: loss 0.085672\nbatch 2332: loss 0.049412\nbatch 2333: loss 0.114119\nbatch 2334: loss 0.166560\nbatch 2335: loss 0.200484\nbatch 2336: loss 0.065813\nbatch 2337: loss 0.100295\nbatch 2338: loss 0.045945\nbatch 2339: loss 0.119898\nbatch 2340: loss 0.087072\nbatch 2341: loss 0.049834\nbatch 2342: loss 0.133757\nbatch 2343: loss 0.024311\nbatch 2344: loss 0.138366\nbatch 2345: loss 0.138137\nbatch 2346: loss 0.085360\nbatch 2347: loss 0.168303\nbatch 2348: loss 0.319910\nbatch 2349: loss 0.115932\nbatch 2350: loss 0.285810\nbatch 2351: loss 0.035245\nbatch 2352: loss 0.140166\nbatch 2353: loss 0.039795\nbatch 2354: loss 0.133811\nbatch 2355: loss 0.045648\nbatch 2356: loss 0.118512\nbatch 2357: loss 0.022973\nbatch 2358: loss 0.147371\nbatch 2359: loss 0.089748\nbatch 2360: loss 0.187103\nbatch 2361: loss 0.069024\nbatch 2362: loss 0.265330\nbatch 2363: loss 0.065203\nbatch 2364: loss 0.072959\nbatch 2365: loss 0.088629\nbatch 2366: loss 0.065608\nbatch 2367: loss 0.118344\nbatch 2368: loss 0.132412\nbatch 2369: loss 0.067511\nbatch 2370: loss 0.021516\nbatch 2371: loss 0.078161\nbatch 2372: loss 0.206378\nbatch 2373: loss 0.062835\nbatch 2374: loss 0.057236\nbatch 2375: loss 0.116917\nbatch 2376: loss 0.031234\nbatch 2377: loss 0.127626\nbatch 2378: loss 0.267391\nbatch 2379: loss 0.205021\nbatch 2380: loss 0.078147\nbatch 2381: loss 0.200300\nbatch 2382: loss 0.049659\nbatch 2383: loss 0.025734\nbatch 2384: loss 0.210905\nbatch 2385: loss 0.104394\nbatch 2386: loss 0.396229\nbatch 2387: loss 0.128366\nbatch 2388: loss 0.087249\nbatch 2389: loss 0.042653\nbatch 2390: loss 0.169094\nbatch 2391: loss 0.031525\nbatch 2392: loss 0.108546\nbatch 2393: loss 0.097850\nbatch 2394: loss 0.147849\nbatch 2395: loss 0.296469\nbatch 2396: loss 0.243278\nbatch 2397: loss 0.235298\nbatch 2398: loss 0.073908\nbatch 2399: loss 0.054330\nbatch 2400: loss 0.035316\nbatch 2401: loss 0.099515\nbatch 2402: loss 0.121339\nbatch 2403: loss 0.063011\nbatch 2404: loss 0.144236\nbatch 2405: loss 0.053653\nbatch 2406: loss 0.180597\nbatch 2407: loss 0.074333\nbatch 2408: loss 0.188613\nbatch 2409: loss 0.216947\nbatch 2410: loss 0.037465\nbatch 2411: loss 0.106635\nbatch 2412: loss 0.114755\nbatch 2413: loss 0.162277\nbatch 2414: loss 0.175186\nbatch 2415: loss 0.177908\nbatch 2416: loss 0.110744\nbatch 2417: loss 0.144465\nbatch 2418: loss 0.091488\nbatch 2419: loss 0.174325\nbatch 2420: loss 0.102947\nbatch 2421: loss 0.335942\nbatch 2422: loss 0.060872\nbatch 2423: loss 0.123211\nbatch 2424: loss 0.033299\nbatch 2425: loss 0.049931\nbatch 2426: loss 0.039300\nbatch 2427: loss 0.108649\nbatch 2428: loss 0.130832\nbatch 2429: loss 0.102620\nbatch 2430: loss 0.096865\nbatch 2431: loss 0.073403\nbatch 2432: loss 0.048842\nbatch 2433: loss 0.064237\nbatch 2434: loss 0.120006\nbatch 2435: loss 0.056485\nbatch 2436: loss 0.046867\nbatch 2437: loss 0.106654\nbatch 2438: loss 0.156932\nbatch 2439: loss 0.110637\nbatch 2440: loss 0.067990\nbatch 2441: loss 0.033373\nbatch 2442: loss 0.070552\nbatch 2443: loss 0.152081\nbatch 2444: loss 0.169224\nbatch 2445: loss 0.077853\nbatch 2446: loss 0.102406\nbatch 2447: loss 0.044763\nbatch 2448: loss 0.081227\nbatch 2449: loss 0.047784\nbatch 2450: loss 0.058886\nbatch 2451: loss 0.077766\nbatch 2452: loss 0.058236\nbatch 2453: loss 0.073230\nbatch 2454: loss 0.150986\nbatch 2455: loss 0.057122\nbatch 2456: loss 0.117883\nbatch 2457: loss 0.124631\nbatch 2458: loss 0.304661\nbatch 2459: loss 0.261390\nbatch 2460: loss 0.101651\nbatch 2461: loss 0.041638\nbatch 2462: loss 0.132002\nbatch 2463: loss 0.045885\nbatch 2464: loss 0.065377\nbatch 2465: loss 0.064607\nbatch 2466: loss 0.127290\nbatch 2467: loss 0.131247\nbatch 2468: loss 0.146634\nbatch 2469: loss 0.076913\nbatch 2470: loss 0.148554\nbatch 2471: loss 0.122767\nbatch 2472: loss 0.054948\nbatch 2473: loss 0.222312\nbatch 2474: loss 0.067683\nbatch 2475: loss 0.029403\nbatch 2476: loss 0.161208\nbatch 2477: loss 0.061154\nbatch 2478: loss 0.097040\nbatch 2479: loss 0.092694\nbatch 2480: loss 0.064942\nbatch 2481: loss 0.235496\nbatch 2482: loss 0.060862\nbatch 2483: loss 0.109259\nbatch 2484: loss 0.117387\nbatch 2485: loss 0.065532\nbatch 2486: loss 0.069975\nbatch 2487: loss 0.095729\nbatch 2488: loss 0.077348\nbatch 2489: loss 0.148780\nbatch 2490: loss 0.188306\nbatch 2491: loss 0.114841\nbatch 2492: loss 0.073572\nbatch 2493: loss 0.125296\nbatch 2494: loss 0.030146\nbatch 2495: loss 0.093276\nbatch 2496: loss 0.112917\nbatch 2497: loss 0.190006\nbatch 2498: loss 0.045583\nbatch 2499: loss 0.038891\nbatch 2500: loss 0.096989\nbatch 2501: loss 0.037331\nbatch 2502: loss 0.201616\nbatch 2503: loss 0.183452\nbatch 2504: loss 0.146712\nbatch 2505: loss 0.134766\nbatch 2506: loss 0.060836\nbatch 2507: loss 0.186835\nbatch 2508: loss 0.029473\nbatch 2509: loss 0.340676\nbatch 2510: loss 0.159562\nbatch 2511: loss 0.024262\nbatch 2512: loss 0.059782\nbatch 2513: loss 0.098132\nbatch 2514: loss 0.047379\nbatch 2515: loss 0.025717\nbatch 2516: loss 0.066144\nbatch 2517: loss 0.135343\nbatch 2518: loss 0.057650\nbatch 2519: loss 0.134322\nbatch 2520: loss 0.063340\nbatch 2521: loss 0.176935\nbatch 2522: loss 0.200986\nbatch 2523: loss 0.104974\nbatch 2524: loss 0.104376\nbatch 2525: loss 0.110122\nbatch 2526: loss 0.053304\nbatch 2527: loss 0.059460\nbatch 2528: loss 0.063182\nbatch 2529: loss 0.190996\nbatch 2530: loss 0.052869\nbatch 2531: loss 0.327181\nbatch 2532: loss 0.090784\nbatch 2533: loss 0.142102\nbatch 2534: loss 0.074161\nbatch 2535: loss 0.130836\nbatch 2536: loss 0.145145\nbatch 2537: loss 0.052228\nbatch 2538: loss 0.070141\nbatch 2539: loss 0.059600\nbatch 2540: loss 0.410482\nbatch 2541: loss 0.103141\nbatch 2542: loss 0.107028\nbatch 2543: loss 0.019363\nbatch 2544: loss 0.073347\nbatch 2545: loss 0.169215\nbatch 2546: loss 0.100402\nbatch 2547: loss 0.171854\nbatch 2548: loss 0.047173\nbatch 2549: loss 0.095795\nbatch 2550: loss 0.109635\nbatch 2551: loss 0.104924\nbatch 2552: loss 0.073703\nbatch 2553: loss 0.146292\nbatch 2554: loss 0.028191\nbatch 2555: loss 0.088509\nbatch 2556: loss 0.051441\nbatch 2557: loss 0.136806\nbatch 2558: loss 0.051592\nbatch 2559: loss 0.205888\nbatch 2560: loss 0.031155\nbatch 2561: loss 0.016299\nbatch 2562: loss 0.101633\nbatch 2563: loss 0.104342\nbatch 2564: loss 0.124204\nbatch 2565: loss 0.115037\nbatch 2566: loss 0.121996\nbatch 2567: loss 0.114064\nbatch 2568: loss 0.094038\nbatch 2569: loss 0.072685\nbatch 2570: loss 0.041008\nbatch 2571: loss 0.038755\nbatch 2572: loss 0.027709\nbatch 2573: loss 0.077376\nbatch 2574: loss 0.038834\nbatch 2575: loss 0.110488\nbatch 2576: loss 0.085437\nbatch 2577: loss 0.124497\nbatch 2578: loss 0.086632\nbatch 2579: loss 0.041960\nbatch 2580: loss 0.034672\nbatch 2581: loss 0.112180\nbatch 2582: loss 0.035530\nbatch 2583: loss 0.081249\nbatch 2584: loss 0.030768\nbatch 2585: loss 0.042566\nbatch 2586: loss 0.034475\nbatch 2587: loss 0.043240\nbatch 2588: loss 0.094416\nbatch 2589: loss 0.154302\nbatch 2590: loss 0.136871\nbatch 2591: loss 0.035198\nbatch 2592: loss 0.065246\nbatch 2593: loss 0.291198\nbatch 2594: loss 0.204802\nbatch 2595: loss 0.090625\nbatch 2596: loss 0.092345\nbatch 2597: loss 0.171040\nbatch 2598: loss 0.083133\nbatch 2599: loss 0.137311\nbatch 2600: loss 0.128040\nbatch 2601: loss 0.113079\nbatch 2602: loss 0.186724\nbatch 2603: loss 0.099878\nbatch 2604: loss 0.088291\nbatch 2605: loss 0.077896\nbatch 2606: loss 0.022994\nbatch 2607: loss 0.081213\nbatch 2608: loss 0.010062\nbatch 2609: loss 0.158614\nbatch 2610: loss 0.041458\nbatch 2611: loss 0.119908\nbatch 2612: loss 0.052834\nbatch 2613: loss 0.042188\nbatch 2614: loss 0.095937\nbatch 2615: loss 0.067414\nbatch 2616: loss 0.067680\nbatch 2617: loss 0.078403\nbatch 2618: loss 0.265817\nbatch 2619: loss 0.055515\nbatch 2620: loss 0.114203\nbatch 2621: loss 0.105551\nbatch 2622: loss 0.112139\nbatch 2623: loss 0.041461\nbatch 2624: loss 0.067521\nbatch 2625: loss 0.192606\nbatch 2626: loss 0.238889\nbatch 2627: loss 0.020628\nbatch 2628: loss 0.020481\nbatch 2629: loss 0.034400\nbatch 2630: loss 0.053323\nbatch 2631: loss 0.139760\nbatch 2632: loss 0.051428\nbatch 2633: loss 0.330742\nbatch 2634: loss 0.221004\nbatch 2635: loss 0.117882\nbatch 2636: loss 0.100632\nbatch 2637: loss 0.061541\nbatch 2638: loss 0.097311\nbatch 2639: loss 0.038172\nbatch 2640: loss 0.118863\nbatch 2641: loss 0.030213\nbatch 2642: loss 0.203043\nbatch 2643: loss 0.054331\nbatch 2644: loss 0.053619\nbatch 2645: loss 0.059641\nbatch 2646: loss 0.058335\nbatch 2647: loss 0.053327\nbatch 2648: loss 0.058502\nbatch 2649: loss 0.122648\nbatch 2650: loss 0.035477\nbatch 2651: loss 0.109605\nbatch 2652: loss 0.142805\nbatch 2653: loss 0.071935\nbatch 2654: loss 0.149172\nbatch 2655: loss 0.120208\nbatch 2656: loss 0.062261\nbatch 2657: loss 0.045480\nbatch 2658: loss 0.161215\nbatch 2659: loss 0.018196\nbatch 2660: loss 0.224535\nbatch 2661: loss 0.049440\nbatch 2662: loss 0.136129\nbatch 2663: loss 0.281404\nbatch 2664: loss 0.055708\nbatch 2665: loss 0.060052\nbatch 2666: loss 0.015204\nbatch 2667: loss 0.040329\nbatch 2668: loss 0.088053\nbatch 2669: loss 0.127262\nbatch 2670: loss 0.153834\nbatch 2671: loss 0.065424\nbatch 2672: loss 0.056376\nbatch 2673: loss 0.157774\nbatch 2674: loss 0.160465\nbatch 2675: loss 0.125867\nbatch 2676: loss 0.127164\nbatch 2677: loss 0.063175\nbatch 2678: loss 0.060743\nbatch 2679: loss 0.164843\nbatch 2680: loss 0.057850\nbatch 2681: loss 0.054966\nbatch 2682: loss 0.032494\nbatch 2683: loss 0.072478\nbatch 2684: loss 0.130502\nbatch 2685: loss 0.218493\nbatch 2686: loss 0.014327\nbatch 2687: loss 0.073516\nbatch 2688: loss 0.097198\nbatch 2689: loss 0.165445\nbatch 2690: loss 0.061148\nbatch 2691: loss 0.009922\nbatch 2692: loss 0.063239\nbatch 2693: loss 0.231569\nbatch 2694: loss 0.196506\nbatch 2695: loss 0.051624\nbatch 2696: loss 0.109353\nbatch 2697: loss 0.123137\nbatch 2698: loss 0.141707\nbatch 2699: loss 0.059271\nbatch 2700: loss 0.106449\nbatch 2701: loss 0.123462\nbatch 2702: loss 0.190912\nbatch 2703: loss 0.035056\nbatch 2704: loss 0.091108\nbatch 2705: loss 0.069200\nbatch 2706: loss 0.066386\nbatch 2707: loss 0.109529\nbatch 2708: loss 0.040073\nbatch 2709: loss 0.026390\nbatch 2710: loss 0.261490\nbatch 2711: loss 0.132179\nbatch 2712: loss 0.017715\nbatch 2713: loss 0.068546\nbatch 2714: loss 0.022451\nbatch 2715: loss 0.229155\nbatch 2716: loss 0.166324\nbatch 2717: loss 0.041806\nbatch 2718: loss 0.017503\nbatch 2719: loss 0.222007\nbatch 2720: loss 0.087725\nbatch 2721: loss 0.082828\nbatch 2722: loss 0.074638\nbatch 2723: loss 0.206848\nbatch 2724: loss 0.162949\nbatch 2725: loss 0.153386\nbatch 2726: loss 0.117089\nbatch 2727: loss 0.044066\nbatch 2728: loss 0.048277\nbatch 2729: loss 0.070288\nbatch 2730: loss 0.069885\nbatch 2731: loss 0.050774\nbatch 2732: loss 0.151488\nbatch 2733: loss 0.100890\nbatch 2734: loss 0.314152\nbatch 2735: loss 0.151567\nbatch 2736: loss 0.058626\nbatch 2737: loss 0.156589\nbatch 2738: loss 0.056171\nbatch 2739: loss 0.034370\nbatch 2740: loss 0.025779\nbatch 2741: loss 0.147952\nbatch 2742: loss 0.189554\nbatch 2743: loss 0.098857\nbatch 2744: loss 0.106843\nbatch 2745: loss 0.029075\nbatch 2746: loss 0.255149\nbatch 2747: loss 0.066728\nbatch 2748: loss 0.114790\nbatch 2749: loss 0.226938\nbatch 2750: loss 0.034445\nbatch 2751: loss 0.083574\nbatch 2752: loss 0.124418\nbatch 2753: loss 0.202262\nbatch 2754: loss 0.054189\nbatch 2755: loss 0.069401\nbatch 2756: loss 0.045437\nbatch 2757: loss 0.054367\nbatch 2758: loss 0.069197\nbatch 2759: loss 0.049403\nbatch 2760: loss 0.091467\nbatch 2761: loss 0.147495\nbatch 2762: loss 0.053641\nbatch 2763: loss 0.126574\nbatch 2764: loss 0.093131\nbatch 2765: loss 0.198343\nbatch 2766: loss 0.140443\nbatch 2767: loss 0.185746\nbatch 2768: loss 0.043800\nbatch 2769: loss 0.113297\nbatch 2770: loss 0.045543\nbatch 2771: loss 0.237832\nbatch 2772: loss 0.051720\nbatch 2773: loss 0.035765\nbatch 2774: loss 0.040603\nbatch 2775: loss 0.031047\nbatch 2776: loss 0.091546\nbatch 2777: loss 0.073693\nbatch 2778: loss 0.147553\nbatch 2779: loss 0.112706\nbatch 2780: loss 0.110769\nbatch 2781: loss 0.029223\nbatch 2782: loss 0.059538\nbatch 2783: loss 0.112961\nbatch 2784: loss 0.065100\nbatch 2785: loss 0.046241\nbatch 2786: loss 0.130105\nbatch 2787: loss 0.040576\nbatch 2788: loss 0.086965\nbatch 2789: loss 0.034207\nbatch 2790: loss 0.035452\nbatch 2791: loss 0.060949\nbatch 2792: loss 0.114487\nbatch 2793: loss 0.053699\nbatch 2794: loss 0.183460\nbatch 2795: loss 0.135753\nbatch 2796: loss 0.103838\nbatch 2797: loss 0.201988\nbatch 2798: loss 0.105683\nbatch 2799: loss 0.135567\nbatch 2800: loss 0.094224\nbatch 2801: loss 0.032137\nbatch 2802: loss 0.140710\nbatch 2803: loss 0.025757\nbatch 2804: loss 0.048811\nbatch 2805: loss 0.175533\nbatch 2806: loss 0.041633\nbatch 2807: loss 0.077199\nbatch 2808: loss 0.138269\nbatch 2809: loss 0.118726\nbatch 2810: loss 0.093719\nbatch 2811: loss 0.221910\nbatch 2812: loss 0.033974\nbatch 2813: loss 0.036453\nbatch 2814: loss 0.048721\nbatch 2815: loss 0.246333\nbatch 2816: loss 0.046525\nbatch 2817: loss 0.088407\nbatch 2818: loss 0.148720\nbatch 2819: loss 0.229667\nbatch 2820: loss 0.094067\nbatch 2821: loss 0.060017\nbatch 2822: loss 0.023441\nbatch 2823: loss 0.040260\nbatch 2824: loss 0.120533\nbatch 2825: loss 0.081407\nbatch 2826: loss 0.042574\nbatch 2827: loss 0.117059\nbatch 2828: loss 0.031964\nbatch 2829: loss 0.219405\nbatch 2830: loss 0.061379\nbatch 2831: loss 0.158788\nbatch 2832: loss 0.199451\nbatch 2833: loss 0.121607\nbatch 2834: loss 0.030610\nbatch 2835: loss 0.193763\nbatch 2836: loss 0.053679\nbatch 2837: loss 0.077362\nbatch 2838: loss 0.023715\nbatch 2839: loss 0.066217\nbatch 2840: loss 0.077085\nbatch 2841: loss 0.111193\nbatch 2842: loss 0.034469\nbatch 2843: loss 0.128675\nbatch 2844: loss 0.014871\nbatch 2845: loss 0.105816\nbatch 2846: loss 0.072831\nbatch 2847: loss 0.049402\nbatch 2848: loss 0.067355\nbatch 2849: loss 0.127958\nbatch 2850: loss 0.022007\nbatch 2851: loss 0.092668\nbatch 2852: loss 0.063599\nbatch 2853: loss 0.015110\nbatch 2854: loss 0.036652\nbatch 2855: loss 0.056961\nbatch 2856: loss 0.101932\nbatch 2857: loss 0.150158\nbatch 2858: loss 0.066397\nbatch 2859: loss 0.033692\nbatch 2860: loss 0.044363\nbatch 2861: loss 0.143562\nbatch 2862: loss 0.164431\nbatch 2863: loss 0.016209\nbatch 2864: loss 0.076408\nbatch 2865: loss 0.174671\nbatch 2866: loss 0.147806\nbatch 2867: loss 0.048588\nbatch 2868: loss 0.140864\nbatch 2869: loss 0.117751\nbatch 2870: loss 0.082783\nbatch 2871: loss 0.048790\nbatch 2872: loss 0.016817\nbatch 2873: loss 0.077975\nbatch 2874: loss 0.127769\nbatch 2875: loss 0.026229\nbatch 2876: loss 0.029969\nbatch 2877: loss 0.052575\nbatch 2878: loss 0.018094\nbatch 2879: loss 0.100180\nbatch 2880: loss 0.104185\nbatch 2881: loss 0.240619\nbatch 2882: loss 0.080203\nbatch 2883: loss 0.060011\nbatch 2884: loss 0.160973\nbatch 2885: loss 0.171373\nbatch 2886: loss 0.047513\nbatch 2887: loss 0.133097\nbatch 2888: loss 0.067771\nbatch 2889: loss 0.146652\nbatch 2890: loss 0.160037\nbatch 2891: loss 0.214736\nbatch 2892: loss 0.223683\nbatch 2893: loss 0.072658\nbatch 2894: loss 0.114359\nbatch 2895: loss 0.118364\nbatch 2896: loss 0.126207\nbatch 2897: loss 0.069194\nbatch 2898: loss 0.076250\nbatch 2899: loss 0.094734\nbatch 2900: loss 0.225986\nbatch 2901: loss 0.087582\nbatch 2902: loss 0.029486\nbatch 2903: loss 0.120747\nbatch 2904: loss 0.171919\nbatch 2905: loss 0.014408\nbatch 2906: loss 0.036226\nbatch 2907: loss 0.037396\nbatch 2908: loss 0.069799\nbatch 2909: loss 0.139819\nbatch 2910: loss 0.146117\nbatch 2911: loss 0.134537\nbatch 2912: loss 0.073379\nbatch 2913: loss 0.017941\nbatch 2914: loss 0.108830\nbatch 2915: loss 0.120168\nbatch 2916: loss 0.057294\nbatch 2917: loss 0.086308\nbatch 2918: loss 0.104172\nbatch 2919: loss 0.098884\nbatch 2920: loss 0.131789\nbatch 2921: loss 0.132534\nbatch 2922: loss 0.127908\nbatch 2923: loss 0.071703\nbatch 2924: loss 0.071085\nbatch 2925: loss 0.027933\nbatch 2926: loss 0.071104\nbatch 2927: loss 0.024645\nbatch 2928: loss 0.035569\nbatch 2929: loss 0.051919\nbatch 2930: loss 0.025827\nbatch 2931: loss 0.032096\nbatch 2932: loss 0.078134\nbatch 2933: loss 0.304205\nbatch 2934: loss 0.131509\nbatch 2935: loss 0.037503\nbatch 2936: loss 0.027318\nbatch 2937: loss 0.137880\nbatch 2938: loss 0.028879\nbatch 2939: loss 0.086373\nbatch 2940: loss 0.024296\nbatch 2941: loss 0.149536\nbatch 2942: loss 0.069811\nbatch 2943: loss 0.108039\nbatch 2944: loss 0.101495\nbatch 2945: loss 0.034692\nbatch 2946: loss 0.057642\nbatch 2947: loss 0.141645\nbatch 2948: loss 0.227917\nbatch 2949: loss 0.198968\nbatch 2950: loss 0.045659\nbatch 2951: loss 0.050291\nbatch 2952: loss 0.287447\nbatch 2953: loss 0.138536\nbatch 2954: loss 0.100019\nbatch 2955: loss 0.066797\nbatch 2956: loss 0.040532\nbatch 2957: loss 0.093007\nbatch 2958: loss 0.127129\nbatch 2959: loss 0.180827\nbatch 2960: loss 0.096839\nbatch 2961: loss 0.010205\nbatch 2962: loss 0.028896\nbatch 2963: loss 0.175320\nbatch 2964: loss 0.082548\nbatch 2965: loss 0.108984\nbatch 2966: loss 0.072219\nbatch 2967: loss 0.101981\nbatch 2968: loss 0.018508\nbatch 2969: loss 0.201158\nbatch 2970: loss 0.109368\nbatch 2971: loss 0.040149\nbatch 2972: loss 0.058591\nbatch 2973: loss 0.089099\nbatch 2974: loss 0.080765\nbatch 2975: loss 0.152291\nbatch 2976: loss 0.061699\nbatch 2977: loss 0.038602\nbatch 2978: loss 0.067932\nbatch 2979: loss 0.113907\nbatch 2980: loss 0.040154\nbatch 2981: loss 0.033548\nbatch 2982: loss 0.066104\nbatch 2983: loss 0.060751\nbatch 2984: loss 0.049267\nbatch 2985: loss 0.034609\nbatch 2986: loss 0.021304\nbatch 2987: loss 0.021631\nbatch 2988: loss 0.018803\nbatch 2989: loss 0.090298\nbatch 2990: loss 0.123393\nbatch 2991: loss 0.147768\nbatch 2992: loss 0.122571\nbatch 2993: loss 0.031240\nbatch 2994: loss 0.071979\nbatch 2995: loss 0.054207\nbatch 2996: loss 0.032726\nbatch 2997: loss 0.010670\nbatch 2998: loss 0.024553\nbatch 2999: loss 0.172647\nbatch 3000: loss 0.016162\nbatch 3001: loss 0.094054\nbatch 3002: loss 0.050852\nbatch 3003: loss 0.160226\nbatch 3004: loss 0.126904\nbatch 3005: loss 0.169238\nbatch 3006: loss 0.060816\nbatch 3007: loss 0.084097\nbatch 3008: loss 0.197921\nbatch 3009: loss 0.054555\nbatch 3010: loss 0.072899\nbatch 3011: loss 0.051477\nbatch 3012: loss 0.142266\nbatch 3013: loss 0.035635\nbatch 3014: loss 0.140865\nbatch 3015: loss 0.041906\nbatch 3016: loss 0.035038\nbatch 3017: loss 0.177504\nbatch 3018: loss 0.202994\nbatch 3019: loss 0.044963\nbatch 3020: loss 0.048123\nbatch 3021: loss 0.112737\nbatch 3022: loss 0.020455\nbatch 3023: loss 0.064041\nbatch 3024: loss 0.125332\nbatch 3025: loss 0.036608\nbatch 3026: loss 0.039888\nbatch 3027: loss 0.081957\nbatch 3028: loss 0.019873\nbatch 3029: loss 0.159622\nbatch 3030: loss 0.106787\nbatch 3031: loss 0.062571\nbatch 3032: loss 0.053696\nbatch 3033: loss 0.043005\nbatch 3034: loss 0.077571\nbatch 3035: loss 0.067641\nbatch 3036: loss 0.030307\nbatch 3037: loss 0.046041\nbatch 3038: loss 0.132116\nbatch 3039: loss 0.019853\nbatch 3040: loss 0.041464\nbatch 3041: loss 0.224154\nbatch 3042: loss 0.058418\nbatch 3043: loss 0.109427\nbatch 3044: loss 0.059359\nbatch 3045: loss 0.036182\nbatch 3046: loss 0.077949\nbatch 3047: loss 0.135785\nbatch 3048: loss 0.023465\nbatch 3049: loss 0.049200\nbatch 3050: loss 0.089911\nbatch 3051: loss 0.042653\nbatch 3052: loss 0.073625\nbatch 3053: loss 0.165107\nbatch 3054: loss 0.045451\nbatch 3055: loss 0.042677\nbatch 3056: loss 0.047621\nbatch 3057: loss 0.084458\nbatch 3058: loss 0.008700\nbatch 3059: loss 0.045664\nbatch 3060: loss 0.035346\nbatch 3061: loss 0.073541\nbatch 3062: loss 0.016843\nbatch 3063: loss 0.233342\nbatch 3064: loss 0.068986\nbatch 3065: loss 0.013362\nbatch 3066: loss 0.175059\nbatch 3067: loss 0.053394\nbatch 3068: loss 0.064270\nbatch 3069: loss 0.089015\nbatch 3070: loss 0.014639\nbatch 3071: loss 0.118316\nbatch 3072: loss 0.123039\nbatch 3073: loss 0.041319\nbatch 3074: loss 0.042567\nbatch 3075: loss 0.036776\nbatch 3076: loss 0.088550\nbatch 3077: loss 0.042273\nbatch 3078: loss 0.076253\nbatch 3079: loss 0.034210\nbatch 3080: loss 0.145425\nbatch 3081: loss 0.042392\nbatch 3082: loss 0.167557\nbatch 3083: loss 0.041957\nbatch 3084: loss 0.076271\nbatch 3085: loss 0.058199\nbatch 3086: loss 0.026478\nbatch 3087: loss 0.017120\nbatch 3088: loss 0.049630\nbatch 3089: loss 0.128512\nbatch 3090: loss 0.093828\nbatch 3091: loss 0.118385\nbatch 3092: loss 0.060140\nbatch 3093: loss 0.046071\nbatch 3094: loss 0.054762\nbatch 3095: loss 0.133036\nbatch 3096: loss 0.049396\nbatch 3097: loss 0.073196\nbatch 3098: loss 0.062884\nbatch 3099: loss 0.184181\nbatch 3100: loss 0.085176\nbatch 3101: loss 0.113781\nbatch 3102: loss 0.059160\nbatch 3103: loss 0.252820\nbatch 3104: loss 0.052991\nbatch 3105: loss 0.162628\nbatch 3106: loss 0.030153\nbatch 3107: loss 0.011267\nbatch 3108: loss 0.064862\nbatch 3109: loss 0.071609\nbatch 3110: loss 0.013007\nbatch 3111: loss 0.131425\nbatch 3112: loss 0.042067\nbatch 3113: loss 0.274472\nbatch 3114: loss 0.026204\nbatch 3115: loss 0.123130\nbatch 3116: loss 0.088673\nbatch 3117: loss 0.116034\nbatch 3118: loss 0.047998\nbatch 3119: loss 0.110048\nbatch 3120: loss 0.111168\nbatch 3121: loss 0.063610\nbatch 3122: loss 0.033242\nbatch 3123: loss 0.075100\nbatch 3124: loss 0.089099\nbatch 3125: loss 0.052805\nbatch 3126: loss 0.068871\nbatch 3127: loss 0.030379\nbatch 3128: loss 0.052955\nbatch 3129: loss 0.067916\nbatch 3130: loss 0.034722\nbatch 3131: loss 0.092810\nbatch 3132: loss 0.105122\nbatch 3133: loss 0.036819\nbatch 3134: loss 0.105110\nbatch 3135: loss 0.027324\nbatch 3136: loss 0.092217\nbatch 3137: loss 0.177884\nbatch 3138: loss 0.027181\nbatch 3139: loss 0.208275\nbatch 3140: loss 0.063032\nbatch 3141: loss 0.218680\nbatch 3142: loss 0.199485\nbatch 3143: loss 0.050511\nbatch 3144: loss 0.117661\nbatch 3145: loss 0.155156\nbatch 3146: loss 0.042918\nbatch 3147: loss 0.078106\nbatch 3148: loss 0.018503\nbatch 3149: loss 0.027181\nbatch 3150: loss 0.060958\nbatch 3151: loss 0.015746\nbatch 3152: loss 0.079343\nbatch 3153: loss 0.062721\nbatch 3154: loss 0.022427\nbatch 3155: loss 0.064720\nbatch 3156: loss 0.066936\nbatch 3157: loss 0.072067\nbatch 3158: loss 0.089947\nbatch 3159: loss 0.094338\nbatch 3160: loss 0.219249\nbatch 3161: loss 0.148604\nbatch 3162: loss 0.036357\nbatch 3163: loss 0.055262\nbatch 3164: loss 0.139171\nbatch 3165: loss 0.054798\nbatch 3166: loss 0.022787\nbatch 3167: loss 0.021684\nbatch 3168: loss 0.061234\nbatch 3169: loss 0.052468\nbatch 3170: loss 0.027093\nbatch 3171: loss 0.120282\nbatch 3172: loss 0.060609\nbatch 3173: loss 0.041195\nbatch 3174: loss 0.132234\nbatch 3175: loss 0.170634\nbatch 3176: loss 0.020329\nbatch 3177: loss 0.042300\nbatch 3178: loss 0.084389\nbatch 3179: loss 0.064275\nbatch 3180: loss 0.081518\nbatch 3181: loss 0.098802\nbatch 3182: loss 0.255707\nbatch 3183: loss 0.088179\nbatch 3184: loss 0.051717\nbatch 3185: loss 0.049906\nbatch 3186: loss 0.063391\nbatch 3187: loss 0.016028\nbatch 3188: loss 0.047303\nbatch 3189: loss 0.200987\nbatch 3190: loss 0.272412\nbatch 3191: loss 0.158576\nbatch 3192: loss 0.118790\nbatch 3193: loss 0.048423\nbatch 3194: loss 0.024103\nbatch 3195: loss 0.052175\nbatch 3196: loss 0.100330\nbatch 3197: loss 0.026925\nbatch 3198: loss 0.133947\nbatch 3199: loss 0.120849\nbatch 3200: loss 0.042929\nbatch 3201: loss 0.043758\nbatch 3202: loss 0.031292\nbatch 3203: loss 0.091013\nbatch 3204: loss 0.186015\nbatch 3205: loss 0.104934\nbatch 3206: loss 0.058089\nbatch 3207: loss 0.079310\nbatch 3208: loss 0.096896\nbatch 3209: loss 0.052359\nbatch 3210: loss 0.056829\nbatch 3211: loss 0.196980\nbatch 3212: loss 0.021778\nbatch 3213: loss 0.077032\nbatch 3214: loss 0.014343\nbatch 3215: loss 0.046921\nbatch 3216: loss 0.023875\nbatch 3217: loss 0.008695\nbatch 3218: loss 0.025473\nbatch 3219: loss 0.046102\nbatch 3220: loss 0.020507\nbatch 3221: loss 0.254781\nbatch 3222: loss 0.089769\nbatch 3223: loss 0.038528\nbatch 3224: loss 0.201474\nbatch 3225: loss 0.054082\nbatch 3226: loss 0.157739\nbatch 3227: loss 0.038076\nbatch 3228: loss 0.148559\nbatch 3229: loss 0.063494\nbatch 3230: loss 0.084657\nbatch 3231: loss 0.046188\nbatch 3232: loss 0.049497\nbatch 3233: loss 0.024607\nbatch 3234: loss 0.026146\nbatch 3235: loss 0.078845\nbatch 3236: loss 0.065707\nbatch 3237: loss 0.035892\nbatch 3238: loss 0.156410\nbatch 3239: loss 0.135309\nbatch 3240: loss 0.058482\nbatch 3241: loss 0.066217\nbatch 3242: loss 0.084654\nbatch 3243: loss 0.155717\nbatch 3244: loss 0.075748\nbatch 3245: loss 0.126834\nbatch 3246: loss 0.056946\nbatch 3247: loss 0.121869\nbatch 3248: loss 0.106531\nbatch 3249: loss 0.104069\nbatch 3250: loss 0.023605\nbatch 3251: loss 0.045167\nbatch 3252: loss 0.173588\nbatch 3253: loss 0.021286\nbatch 3254: loss 0.038854\nbatch 3255: loss 0.041912\nbatch 3256: loss 0.069858\nbatch 3257: loss 0.041463\nbatch 3258: loss 0.046608\nbatch 3259: loss 0.018010\nbatch 3260: loss 0.071883\nbatch 3261: loss 0.195806\nbatch 3262: loss 0.062334\nbatch 3263: loss 0.101246\nbatch 3264: loss 0.132293\nbatch 3265: loss 0.135344\nbatch 3266: loss 0.111892\nbatch 3267: loss 0.029102\nbatch 3268: loss 0.019910\nbatch 3269: loss 0.216288\nbatch 3270: loss 0.020685\nbatch 3271: loss 0.182845\nbatch 3272: loss 0.069528\nbatch 3273: loss 0.022999\nbatch 3274: loss 0.023696\nbatch 3275: loss 0.214639\nbatch 3276: loss 0.116951\nbatch 3277: loss 0.191142\nbatch 3278: loss 0.191913\nbatch 3279: loss 0.104663\nbatch 3280: loss 0.059542\nbatch 3281: loss 0.090233\nbatch 3282: loss 0.124980\nbatch 3283: loss 0.121909\nbatch 3284: loss 0.194358\nbatch 3285: loss 0.066071\nbatch 3286: loss 0.135988\nbatch 3287: loss 0.016971\nbatch 3288: loss 0.010091\nbatch 3289: loss 0.062276\nbatch 3290: loss 0.028592\nbatch 3291: loss 0.048167\nbatch 3292: loss 0.065406\nbatch 3293: loss 0.043364\nbatch 3294: loss 0.048738\nbatch 3295: loss 0.061108\nbatch 3296: loss 0.172177\nbatch 3297: loss 0.060781\nbatch 3298: loss 0.257636\nbatch 3299: loss 0.114497\nbatch 3300: loss 0.132669\nbatch 3301: loss 0.040140\nbatch 3302: loss 0.042503\nbatch 3303: loss 0.008979\nbatch 3304: loss 0.032271\nbatch 3305: loss 0.182318\nbatch 3306: loss 0.090444\nbatch 3307: loss 0.076494\nbatch 3308: loss 0.155983\nbatch 3309: loss 0.122372\nbatch 3310: loss 0.158974\nbatch 3311: loss 0.035308\nbatch 3312: loss 0.047203\nbatch 3313: loss 0.159616\nbatch 3314: loss 0.090613\nbatch 3315: loss 0.077015\nbatch 3316: loss 0.034314\nbatch 3317: loss 0.069029\nbatch 3318: loss 0.028625\nbatch 3319: loss 0.059710\nbatch 3320: loss 0.029678\nbatch 3321: loss 0.064543\nbatch 3322: loss 0.059141\nbatch 3323: loss 0.073866\nbatch 3324: loss 0.080895\nbatch 3325: loss 0.083603\nbatch 3326: loss 0.073075\nbatch 3327: loss 0.050972\nbatch 3328: loss 0.259670\nbatch 3329: loss 0.028056\nbatch 3330: loss 0.029354\nbatch 3331: loss 0.144302\nbatch 3332: loss 0.045380\nbatch 3333: loss 0.036232\nbatch 3334: loss 0.097870\nbatch 3335: loss 0.053138\nbatch 3336: loss 0.108772\nbatch 3337: loss 0.095967\nbatch 3338: loss 0.195734\nbatch 3339: loss 0.178373\nbatch 3340: loss 0.052158\nbatch 3341: loss 0.069854\nbatch 3342: loss 0.049561\nbatch 3343: loss 0.016180\nbatch 3344: loss 0.049316\nbatch 3345: loss 0.098863\nbatch 3346: loss 0.072421\nbatch 3347: loss 0.080588\nbatch 3348: loss 0.019947\nbatch 3349: loss 0.025300\nbatch 3350: loss 0.041366\nbatch 3351: loss 0.212370\nbatch 3352: loss 0.179548\nbatch 3353: loss 0.094389\nbatch 3354: loss 0.192425\nbatch 3355: loss 0.160173\nbatch 3356: loss 0.034010\nbatch 3357: loss 0.122845\nbatch 3358: loss 0.049153\nbatch 3359: loss 0.126327\nbatch 3360: loss 0.009727\nbatch 3361: loss 0.023561\nbatch 3362: loss 0.076271\nbatch 3363: loss 0.017475\nbatch 3364: loss 0.135514\nbatch 3365: loss 0.067674\nbatch 3366: loss 0.022927\nbatch 3367: loss 0.039637\nbatch 3368: loss 0.103678\nbatch 3369: loss 0.132143\nbatch 3370: loss 0.065126\nbatch 3371: loss 0.101989\nbatch 3372: loss 0.034707\nbatch 3373: loss 0.071045\nbatch 3374: loss 0.078118\nbatch 3375: loss 0.013556\nbatch 3376: loss 0.095852\nbatch 3377: loss 0.015387\nbatch 3378: loss 0.134044\nbatch 3379: loss 0.026116\nbatch 3380: loss 0.024939\nbatch 3381: loss 0.017225\nbatch 3382: loss 0.064240\nbatch 3383: loss 0.232438\nbatch 3384: loss 0.039557\nbatch 3385: loss 0.021858\nbatch 3386: loss 0.074469\nbatch 3387: loss 0.026312\nbatch 3388: loss 0.119216\nbatch 3389: loss 0.107057\nbatch 3390: loss 0.014083\nbatch 3391: loss 0.019024\nbatch 3392: loss 0.251078\nbatch 3393: loss 0.008355\nbatch 3394: loss 0.184119\nbatch 3395: loss 0.115170\nbatch 3396: loss 0.082333\nbatch 3397: loss 0.113000\nbatch 3398: loss 0.139303\nbatch 3399: loss 0.194184\nbatch 3400: loss 0.084802\nbatch 3401: loss 0.182492\nbatch 3402: loss 0.153221\nbatch 3403: loss 0.167210\nbatch 3404: loss 0.126744\nbatch 3405: loss 0.055653\nbatch 3406: loss 0.036710\nbatch 3407: loss 0.091294\nbatch 3408: loss 0.081764\nbatch 3409: loss 0.183760\nbatch 3410: loss 0.019310\nbatch 3411: loss 0.022952\nbatch 3412: loss 0.022466\nbatch 3413: loss 0.069284\nbatch 3414: loss 0.063209\nbatch 3415: loss 0.237051\nbatch 3416: loss 0.047917\nbatch 3417: loss 0.044946\nbatch 3418: loss 0.056190\nbatch 3419: loss 0.113780\nbatch 3420: loss 0.097001\nbatch 3421: loss 0.048318\nbatch 3422: loss 0.071907\nbatch 3423: loss 0.034713\nbatch 3424: loss 0.088933\nbatch 3425: loss 0.169611\nbatch 3426: loss 0.035414\nbatch 3427: loss 0.045204\nbatch 3428: loss 0.019068\nbatch 3429: loss 0.016449\nbatch 3430: loss 0.120069\nbatch 3431: loss 0.027161\nbatch 3432: loss 0.109154\nbatch 3433: loss 0.124476\nbatch 3434: loss 0.281493\nbatch 3435: loss 0.048781\nbatch 3436: loss 0.103384\nbatch 3437: loss 0.192719\nbatch 3438: loss 0.075531\nbatch 3439: loss 0.035738\nbatch 3440: loss 0.146846\nbatch 3441: loss 0.120652\nbatch 3442: loss 0.195040\nbatch 3443: loss 0.021982\nbatch 3444: loss 0.098390\nbatch 3445: loss 0.148201\nbatch 3446: loss 0.121476\nbatch 3447: loss 0.158515\nbatch 3448: loss 0.091949\nbatch 3449: loss 0.039842\nbatch 3450: loss 0.200253\nbatch 3451: loss 0.099511\nbatch 3452: loss 0.029318\nbatch 3453: loss 0.090771\nbatch 3454: loss 0.237766\nbatch 3455: loss 0.073147\nbatch 3456: loss 0.033224\nbatch 3457: loss 0.141807\nbatch 3458: loss 0.055900\nbatch 3459: loss 0.103876\nbatch 3460: loss 0.142997\nbatch 3461: loss 0.081141\nbatch 3462: loss 0.201085\nbatch 3463: loss 0.206525\nbatch 3464: loss 0.056422\nbatch 3465: loss 0.083155\nbatch 3466: loss 0.164938\nbatch 3467: loss 0.069068\nbatch 3468: loss 0.047836\nbatch 3469: loss 0.077531\nbatch 3470: loss 0.054873\nbatch 3471: loss 0.193195\nbatch 3472: loss 0.017711\nbatch 3473: loss 0.092858\nbatch 3474: loss 0.084633\nbatch 3475: loss 0.122767\nbatch 3476: loss 0.200599\nbatch 3477: loss 0.037120\nbatch 3478: loss 0.031879\nbatch 3479: loss 0.128076\nbatch 3480: loss 0.032407\nbatch 3481: loss 0.055305\nbatch 3482: loss 0.079066\nbatch 3483: loss 0.180599\nbatch 3484: loss 0.080631\nbatch 3485: loss 0.056342\nbatch 3486: loss 0.144444\nbatch 3487: loss 0.049738\nbatch 3488: loss 0.086109\nbatch 3489: loss 0.037311\nbatch 3490: loss 0.119130\nbatch 3491: loss 0.153305\nbatch 3492: loss 0.145259\nbatch 3493: loss 0.016717\nbatch 3494: loss 0.082472\nbatch 3495: loss 0.161581\nbatch 3496: loss 0.044009\nbatch 3497: loss 0.025144\nbatch 3498: loss 0.048358\nbatch 3499: loss 0.029275\nbatch 3500: loss 0.048358\nbatch 3501: loss 0.051142\nbatch 3502: loss 0.071461\nbatch 3503: loss 0.023508\nbatch 3504: loss 0.189850\nbatch 3505: loss 0.066643\nbatch 3506: loss 0.103992\nbatch 3507: loss 0.066446\nbatch 3508: loss 0.283058\nbatch 3509: loss 0.078081\nbatch 3510: loss 0.034566\nbatch 3511: loss 0.058186\nbatch 3512: loss 0.183896\nbatch 3513: loss 0.077557\nbatch 3514: loss 0.109903\nbatch 3515: loss 0.108135\nbatch 3516: loss 0.015999\nbatch 3517: loss 0.033098\nbatch 3518: loss 0.036819\nbatch 3519: loss 0.095139\nbatch 3520: loss 0.131633\nbatch 3521: loss 0.010553\nbatch 3522: loss 0.131866\nbatch 3523: loss 0.145421\nbatch 3524: loss 0.055233\nbatch 3525: loss 0.144425\nbatch 3526: loss 0.113122\nbatch 3527: loss 0.196668\nbatch 3528: loss 0.085435\nbatch 3529: loss 0.114491\nbatch 3530: loss 0.031560\nbatch 3531: loss 0.033445\nbatch 3532: loss 0.067673\nbatch 3533: loss 0.071394\nbatch 3534: loss 0.034131\nbatch 3535: loss 0.045909\nbatch 3536: loss 0.168238\nbatch 3537: loss 0.018725\nbatch 3538: loss 0.186275\nbatch 3539: loss 0.187012\nbatch 3540: loss 0.026744\nbatch 3541: loss 0.010031\nbatch 3542: loss 0.180491\nbatch 3543: loss 0.063098\nbatch 3544: loss 0.251621\nbatch 3545: loss 0.055422\nbatch 3546: loss 0.097997\nbatch 3547: loss 0.092187\nbatch 3548: loss 0.032802\nbatch 3549: loss 0.098702\nbatch 3550: loss 0.047430\nbatch 3551: loss 0.062799\nbatch 3552: loss 0.057412\nbatch 3553: loss 0.056566\nbatch 3554: loss 0.043177\nbatch 3555: loss 0.033105\nbatch 3556: loss 0.200998\nbatch 3557: loss 0.149770\nbatch 3558: loss 0.060750\nbatch 3559: loss 0.158443\nbatch 3560: loss 0.016642\nbatch 3561: loss 0.034679\nbatch 3562: loss 0.224738\nbatch 3563: loss 0.049488\nbatch 3564: loss 0.043503\nbatch 3565: loss 0.016763\nbatch 3566: loss 0.043537\nbatch 3567: loss 0.026693\nbatch 3568: loss 0.085768\nbatch 3569: loss 0.132269\nbatch 3570: loss 0.214066\nbatch 3571: loss 0.039194\nbatch 3572: loss 0.078755\nbatch 3573: loss 0.060788\nbatch 3574: loss 0.052459\nbatch 3575: loss 0.113728\nbatch 3576: loss 0.131991\nbatch 3577: loss 0.129502\nbatch 3578: loss 0.063211\nbatch 3579: loss 0.032442\nbatch 3580: loss 0.014267\nbatch 3581: loss 0.013332\nbatch 3582: loss 0.069659\nbatch 3583: loss 0.032774\nbatch 3584: loss 0.061474\nbatch 3585: loss 0.036999\nbatch 3586: loss 0.043291\nbatch 3587: loss 0.060758\nbatch 3588: loss 0.107970\nbatch 3589: loss 0.013378\nbatch 3590: loss 0.035633\nbatch 3591: loss 0.049024\nbatch 3592: loss 0.250083\nbatch 3593: loss 0.156941\nbatch 3594: loss 0.247165\nbatch 3595: loss 0.039757\nbatch 3596: loss 0.299551\nbatch 3597: loss 0.227633\nbatch 3598: loss 0.102178\nbatch 3599: loss 0.040544\nbatch 3600: loss 0.033012\nbatch 3601: loss 0.070142\nbatch 3602: loss 0.053532\nbatch 3603: loss 0.056480\nbatch 3604: loss 0.032481\nbatch 3605: loss 0.054660\nbatch 3606: loss 0.076942\nbatch 3607: loss 0.074108\nbatch 3608: loss 0.065328\nbatch 3609: loss 0.058044\nbatch 3610: loss 0.083769\nbatch 3611: loss 0.076121\nbatch 3612: loss 0.188364\nbatch 3613: loss 0.039273\nbatch 3614: loss 0.028833\nbatch 3615: loss 0.065790\nbatch 3616: loss 0.134769\nbatch 3617: loss 0.112542\nbatch 3618: loss 0.066866\nbatch 3619: loss 0.015732\nbatch 3620: loss 0.092364\nbatch 3621: loss 0.017026\nbatch 3622: loss 0.018236\nbatch 3623: loss 0.106736\nbatch 3624: loss 0.139937\nbatch 3625: loss 0.044240\nbatch 3626: loss 0.026460\nbatch 3627: loss 0.024805\nbatch 3628: loss 0.107343\nbatch 3629: loss 0.021346\nbatch 3630: loss 0.039589\nbatch 3631: loss 0.017557\nbatch 3632: loss 0.052593\nbatch 3633: loss 0.022290\nbatch 3634: loss 0.087310\nbatch 3635: loss 0.055296\nbatch 3636: loss 0.038513\nbatch 3637: loss 0.015594\nbatch 3638: loss 0.117865\nbatch 3639: loss 0.068039\nbatch 3640: loss 0.035969\nbatch 3641: loss 0.025177\nbatch 3642: loss 0.043916\nbatch 3643: loss 0.080175\nbatch 3644: loss 0.118525\nbatch 3645: loss 0.099121\nbatch 3646: loss 0.041454\nbatch 3647: loss 0.086432\nbatch 3648: loss 0.104241\nbatch 3649: loss 0.094710\nbatch 3650: loss 0.100071\nbatch 3651: loss 0.030274\nbatch 3652: loss 0.058616\nbatch 3653: loss 0.059503\nbatch 3654: loss 0.153517\nbatch 3655: loss 0.056617\nbatch 3656: loss 0.062895\nbatch 3657: loss 0.169437\nbatch 3658: loss 0.145941\nbatch 3659: loss 0.036861\nbatch 3660: loss 0.168873\nbatch 3661: loss 0.176380\nbatch 3662: loss 0.071006\nbatch 3663: loss 0.022692\nbatch 3664: loss 0.086791\nbatch 3665: loss 0.058497\nbatch 3666: loss 0.016474\nbatch 3667: loss 0.072316\nbatch 3668: loss 0.037830\nbatch 3669: loss 0.087887\nbatch 3670: loss 0.057143\nbatch 3671: loss 0.041221\nbatch 3672: loss 0.158496\nbatch 3673: loss 0.133228\nbatch 3674: loss 0.017608\nbatch 3675: loss 0.032679\nbatch 3676: loss 0.039006\nbatch 3677: loss 0.015280\nbatch 3678: loss 0.041864\nbatch 3679: loss 0.073312\nbatch 3680: loss 0.109319\nbatch 3681: loss 0.028197\nbatch 3682: loss 0.099594\nbatch 3683: loss 0.013919\nbatch 3684: loss 0.073064\nbatch 3685: loss 0.047669\nbatch 3686: loss 0.037515\nbatch 3687: loss 0.048307\nbatch 3688: loss 0.028034\nbatch 3689: loss 0.025031\nbatch 3690: loss 0.019115\nbatch 3691: loss 0.015701\nbatch 3692: loss 0.031698\nbatch 3693: loss 0.039488\nbatch 3694: loss 0.036359\nbatch 3695: loss 0.056773\nbatch 3696: loss 0.079049\nbatch 3697: loss 0.104650\nbatch 3698: loss 0.014753\nbatch 3699: loss 0.004488\nbatch 3700: loss 0.060892\nbatch 3701: loss 0.032059\nbatch 3702: loss 0.044741\nbatch 3703: loss 0.050605\nbatch 3704: loss 0.042644\nbatch 3705: loss 0.067575\nbatch 3706: loss 0.133208\nbatch 3707: loss 0.129810\nbatch 3708: loss 0.055719\nbatch 3709: loss 0.099972\nbatch 3710: loss 0.020411\nbatch 3711: loss 0.025230\nbatch 3712: loss 0.032557\nbatch 3713: loss 0.193336\nbatch 3714: loss 0.034531\nbatch 3715: loss 0.022045\nbatch 3716: loss 0.063424\nbatch 3717: loss 0.088888\nbatch 3718: loss 0.239848\nbatch 3719: loss 0.072175\nbatch 3720: loss 0.102680\nbatch 3721: loss 0.037626\nbatch 3722: loss 0.083646\nbatch 3723: loss 0.022967\nbatch 3724: loss 0.044112\nbatch 3725: loss 0.442587\nbatch 3726: loss 0.164225\nbatch 3727: loss 0.115515\nbatch 3728: loss 0.050101\nbatch 3729: loss 0.112223\nbatch 3730: loss 0.073576\nbatch 3731: loss 0.097908\nbatch 3732: loss 0.115102\nbatch 3733: loss 0.028155\nbatch 3734: loss 0.035561\nbatch 3735: loss 0.094800\nbatch 3736: loss 0.029041\nbatch 3737: loss 0.200682\nbatch 3738: loss 0.085091\nbatch 3739: loss 0.100567\nbatch 3740: loss 0.059829\nbatch 3741: loss 0.132616\nbatch 3742: loss 0.133785\nbatch 3743: loss 0.007812\nbatch 3744: loss 0.073601\nbatch 3745: loss 0.111710\nbatch 3746: loss 0.066905\nbatch 3747: loss 0.034427\nbatch 3748: loss 0.022554\nbatch 3749: loss 0.063855\nbatch 3750: loss 0.050146\nbatch 3751: loss 0.099086\nbatch 3752: loss 0.033884\nbatch 3753: loss 0.039453\nbatch 3754: loss 0.061725\nbatch 3755: loss 0.068948\nbatch 3756: loss 0.016616\nbatch 3757: loss 0.062654\nbatch 3758: loss 0.018078\nbatch 3759: loss 0.061073\nbatch 3760: loss 0.055996\nbatch 3761: loss 0.023092\nbatch 3762: loss 0.035831\nbatch 3763: loss 0.033686\nbatch 3764: loss 0.120032\nbatch 3765: loss 0.125890\nbatch 3766: loss 0.036956\nbatch 3767: loss 0.063958\nbatch 3768: loss 0.177221\nbatch 3769: loss 0.052395\nbatch 3770: loss 0.028789\nbatch 3771: loss 0.036930\nbatch 3772: loss 0.099067\nbatch 3773: loss 0.203606\nbatch 3774: loss 0.096664\nbatch 3775: loss 0.038905\nbatch 3776: loss 0.095390\nbatch 3777: loss 0.045465\nbatch 3778: loss 0.031935\nbatch 3779: loss 0.056117\nbatch 3780: loss 0.097239\nbatch 3781: loss 0.050190\nbatch 3782: loss 0.139106\nbatch 3783: loss 0.087736\nbatch 3784: loss 0.058375\nbatch 3785: loss 0.043586\nbatch 3786: loss 0.065183\nbatch 3787: loss 0.084331\nbatch 3788: loss 0.045191\nbatch 3789: loss 0.039591\nbatch 3790: loss 0.130626\nbatch 3791: loss 0.073615\nbatch 3792: loss 0.232690\nbatch 3793: loss 0.082146\nbatch 3794: loss 0.060869\nbatch 3795: loss 0.089446\nbatch 3796: loss 0.038654\nbatch 3797: loss 0.263912\nbatch 3798: loss 0.126201\nbatch 3799: loss 0.048698\nbatch 3800: loss 0.023861\nbatch 3801: loss 0.104328\nbatch 3802: loss 0.048551\nbatch 3803: loss 0.096467\nbatch 3804: loss 0.028295\nbatch 3805: loss 0.118229\nbatch 3806: loss 0.034454\nbatch 3807: loss 0.045606\nbatch 3808: loss 0.090843\nbatch 3809: loss 0.022815\nbatch 3810: loss 0.098546\nbatch 3811: loss 0.223701\nbatch 3812: loss 0.041131\nbatch 3813: loss 0.066936\nbatch 3814: loss 0.030754\nbatch 3815: loss 0.117744\nbatch 3816: loss 0.026638\nbatch 3817: loss 0.035825\nbatch 3818: loss 0.061398\nbatch 3819: loss 0.084128\nbatch 3820: loss 0.060328\nbatch 3821: loss 0.015553\nbatch 3822: loss 0.031687\nbatch 3823: loss 0.117905\nbatch 3824: loss 0.032522\nbatch 3825: loss 0.025695\nbatch 3826: loss 0.054360\nbatch 3827: loss 0.054804\nbatch 3828: loss 0.108450\nbatch 3829: loss 0.133967\nbatch 3830: loss 0.031336\nbatch 3831: loss 0.016750\nbatch 3832: loss 0.046737\nbatch 3833: loss 0.047111\nbatch 3834: loss 0.059076\nbatch 3835: loss 0.018647\nbatch 3836: loss 0.163024\nbatch 3837: loss 0.082595\nbatch 3838: loss 0.125525\nbatch 3839: loss 0.057641\nbatch 3840: loss 0.154439\nbatch 3841: loss 0.012756\nbatch 3842: loss 0.064767\nbatch 3843: loss 0.013256\nbatch 3844: loss 0.048813\nbatch 3845: loss 0.075208\nbatch 3846: loss 0.062405\nbatch 3847: loss 0.043332\nbatch 3848: loss 0.038185\nbatch 3849: loss 0.083498\nbatch 3850: loss 0.250468\nbatch 3851: loss 0.111343\nbatch 3852: loss 0.131680\nbatch 3853: loss 0.015853\nbatch 3854: loss 0.034401\nbatch 3855: loss 0.079763\nbatch 3856: loss 0.062515\nbatch 3857: loss 0.016449\nbatch 3858: loss 0.052742\nbatch 3859: loss 0.108266\nbatch 3860: loss 0.102539\nbatch 3861: loss 0.078697\nbatch 3862: loss 0.131316\nbatch 3863: loss 0.112478\nbatch 3864: loss 0.079659\nbatch 3865: loss 0.080872\nbatch 3866: loss 0.010819\nbatch 3867: loss 0.028492\nbatch 3868: loss 0.032985\nbatch 3869: loss 0.043389\nbatch 3870: loss 0.077653\nbatch 3871: loss 0.074920\nbatch 3872: loss 0.063141\nbatch 3873: loss 0.130628\nbatch 3874: loss 0.129113\nbatch 3875: loss 0.013510\nbatch 3876: loss 0.028198\nbatch 3877: loss 0.028375\nbatch 3878: loss 0.193375\nbatch 3879: loss 0.288241\nbatch 3880: loss 0.112753\nbatch 3881: loss 0.025391\nbatch 3882: loss 0.033296\nbatch 3883: loss 0.083863\nbatch 3884: loss 0.054361\nbatch 3885: loss 0.026203\nbatch 3886: loss 0.024399\nbatch 3887: loss 0.039314\nbatch 3888: loss 0.119826\nbatch 3889: loss 0.033480\nbatch 3890: loss 0.057795\nbatch 3891: loss 0.037860\nbatch 3892: loss 0.050205\nbatch 3893: loss 0.036694\nbatch 3894: loss 0.076026\nbatch 3895: loss 0.033926\nbatch 3896: loss 0.094258\nbatch 3897: loss 0.351842\nbatch 3898: loss 0.018569\nbatch 3899: loss 0.090539\nbatch 3900: loss 0.059572\nbatch 3901: loss 0.031402\nbatch 3902: loss 0.091676\nbatch 3903: loss 0.080421\nbatch 3904: loss 0.060037\nbatch 3905: loss 0.042643\nbatch 3906: loss 0.071234\nbatch 3907: loss 0.026966\nbatch 3908: loss 0.100806\nbatch 3909: loss 0.028551\nbatch 3910: loss 0.016212\nbatch 3911: loss 0.149068\nbatch 3912: loss 0.287697\nbatch 3913: loss 0.021705\nbatch 3914: loss 0.068270\nbatch 3915: loss 0.121027\nbatch 3916: loss 0.026349\nbatch 3917: loss 0.213738\nbatch 3918: loss 0.092184\nbatch 3919: loss 0.025430\nbatch 3920: loss 0.057901\nbatch 3921: loss 0.030061\nbatch 3922: loss 0.052916\nbatch 3923: loss 0.020877\nbatch 3924: loss 0.063215\nbatch 3925: loss 0.055653\nbatch 3926: loss 0.042590\nbatch 3927: loss 0.044100\nbatch 3928: loss 0.160007\nbatch 3929: loss 0.088986\nbatch 3930: loss 0.011925\nbatch 3931: loss 0.046432\nbatch 3932: loss 0.111305\nbatch 3933: loss 0.035642\nbatch 3934: loss 0.071927\nbatch 3935: loss 0.077100\nbatch 3936: loss 0.065505\nbatch 3937: loss 0.019960\nbatch 3938: loss 0.083013\nbatch 3939: loss 0.151002\nbatch 3940: loss 0.081325\nbatch 3941: loss 0.067025\nbatch 3942: loss 0.010614\nbatch 3943: loss 0.107649\nbatch 3944: loss 0.049255\nbatch 3945: loss 0.016715\nbatch 3946: loss 0.031382\nbatch 3947: loss 0.019847\nbatch 3948: loss 0.149461\nbatch 3949: loss 0.092014\nbatch 3950: loss 0.018382\nbatch 3951: loss 0.051366\nbatch 3952: loss 0.073470\nbatch 3953: loss 0.178826\nbatch 3954: loss 0.123364\nbatch 3955: loss 0.061792\nbatch 3956: loss 0.034720\nbatch 3957: loss 0.025862\nbatch 3958: loss 0.015723\nbatch 3959: loss 0.026100\nbatch 3960: loss 0.063927\nbatch 3961: loss 0.022783\nbatch 3962: loss 0.017627\nbatch 3963: loss 0.017300\nbatch 3964: loss 0.034102\nbatch 3965: loss 0.098722\nbatch 3966: loss 0.028329\nbatch 3967: loss 0.017111\nbatch 3968: loss 0.164212\nbatch 3969: loss 0.108116\nbatch 3970: loss 0.010743\nbatch 3971: loss 0.042277\nbatch 3972: loss 0.009545\nbatch 3973: loss 0.060492\nbatch 3974: loss 0.017138\nbatch 3975: loss 0.044878\nbatch 3976: loss 0.198532\nbatch 3977: loss 0.045665\nbatch 3978: loss 0.089341\nbatch 3979: loss 0.165601\nbatch 3980: loss 0.084999\nbatch 3981: loss 0.028329\nbatch 3982: loss 0.079247\nbatch 3983: loss 0.047072\nbatch 3984: loss 0.021180\nbatch 3985: loss 0.092807\nbatch 3986: loss 0.039425\nbatch 3987: loss 0.032426\nbatch 3988: loss 0.039770\nbatch 3989: loss 0.049432\nbatch 3990: loss 0.045745\nbatch 3991: loss 0.105015\nbatch 3992: loss 0.036190\nbatch 3993: loss 0.051569\nbatch 3994: loss 0.030486\nbatch 3995: loss 0.031147\nbatch 3996: loss 0.051694\nbatch 3997: loss 0.078052\nbatch 3998: loss 0.018037\nbatch 3999: loss 0.063878\nbatch 4000: loss 0.107369\nbatch 4001: loss 0.017058\nbatch 4002: loss 0.065315\nbatch 4003: loss 0.133365\nbatch 4004: loss 0.076809\nbatch 4005: loss 0.075350\nbatch 4006: loss 0.021314\nbatch 4007: loss 0.120227\nbatch 4008: loss 0.089818\nbatch 4009: loss 0.027170\nbatch 4010: loss 0.110535\nbatch 4011: loss 0.104919\nbatch 4012: loss 0.086921\nbatch 4013: loss 0.063221\nbatch 4014: loss 0.240878\nbatch 4015: loss 0.034082\nbatch 4016: loss 0.068278\nbatch 4017: loss 0.019798\nbatch 4018: loss 0.061941\nbatch 4019: loss 0.107186\nbatch 4020: loss 0.067445\nbatch 4021: loss 0.024847\nbatch 4022: loss 0.062375\nbatch 4023: loss 0.040711\nbatch 4024: loss 0.088344\nbatch 4025: loss 0.043186\nbatch 4026: loss 0.136265\nbatch 4027: loss 0.009737\nbatch 4028: loss 0.046232\nbatch 4029: loss 0.068902\nbatch 4030: loss 0.240814\nbatch 4031: loss 0.094619\nbatch 4032: loss 0.021039\nbatch 4033: loss 0.029039\nbatch 4034: loss 0.160625\nbatch 4035: loss 0.083235\nbatch 4036: loss 0.144590\nbatch 4037: loss 0.024796\nbatch 4038: loss 0.043132\nbatch 4039: loss 0.039560\nbatch 4040: loss 0.024882\nbatch 4041: loss 0.047459\nbatch 4042: loss 0.165861\nbatch 4043: loss 0.018224\nbatch 4044: loss 0.045064\nbatch 4045: loss 0.282988\nbatch 4046: loss 0.029581\nbatch 4047: loss 0.033211\nbatch 4048: loss 0.085410\nbatch 4049: loss 0.018318\nbatch 4050: loss 0.009715\nbatch 4051: loss 0.015749\nbatch 4052: loss 0.074448\nbatch 4053: loss 0.027714\nbatch 4054: loss 0.151940\nbatch 4055: loss 0.078842\nbatch 4056: loss 0.048110\nbatch 4057: loss 0.013550\nbatch 4058: loss 0.071166\nbatch 4059: loss 0.025840\nbatch 4060: loss 0.010592\nbatch 4061: loss 0.062287\nbatch 4062: loss 0.031426\nbatch 4063: loss 0.095684\nbatch 4064: loss 0.148020\nbatch 4065: loss 0.055006\nbatch 4066: loss 0.096351\nbatch 4067: loss 0.104462\nbatch 4068: loss 0.033827\nbatch 4069: loss 0.025762\nbatch 4070: loss 0.030143\nbatch 4071: loss 0.050860\nbatch 4072: loss 0.035617\nbatch 4073: loss 0.023989\nbatch 4074: loss 0.146956\nbatch 4075: loss 0.068437\nbatch 4076: loss 0.124822\nbatch 4077: loss 0.069257\nbatch 4078: loss 0.083250\nbatch 4079: loss 0.014233\nbatch 4080: loss 0.152549\nbatch 4081: loss 0.077747\nbatch 4082: loss 0.064525\nbatch 4083: loss 0.055905\nbatch 4084: loss 0.116295\nbatch 4085: loss 0.023283\nbatch 4086: loss 0.013147\nbatch 4087: loss 0.016212\nbatch 4088: loss 0.040418\nbatch 4089: loss 0.040283\nbatch 4090: loss 0.115233\nbatch 4091: loss 0.023286\nbatch 4092: loss 0.034804\nbatch 4093: loss 0.011996\nbatch 4094: loss 0.021648\nbatch 4095: loss 0.040073\nbatch 4096: loss 0.040808\nbatch 4097: loss 0.043609\nbatch 4098: loss 0.025492\nbatch 4099: loss 0.029313\nbatch 4100: loss 0.012574\nbatch 4101: loss 0.038563\nbatch 4102: loss 0.036337\nbatch 4103: loss 0.062084\nbatch 4104: loss 0.103595\nbatch 4105: loss 0.011554\nbatch 4106: loss 0.110331\nbatch 4107: loss 0.202486\nbatch 4108: loss 0.156625\nbatch 4109: loss 0.080408\nbatch 4110: loss 0.080215\nbatch 4111: loss 0.035383\nbatch 4112: loss 0.018022\nbatch 4113: loss 0.023502\nbatch 4114: loss 0.243215\nbatch 4115: loss 0.051569\nbatch 4116: loss 0.005764\nbatch 4117: loss 0.037011\nbatch 4118: loss 0.016080\nbatch 4119: loss 0.044110\nbatch 4120: loss 0.037052\nbatch 4121: loss 0.027043\nbatch 4122: loss 0.051199\nbatch 4123: loss 0.056908\nbatch 4124: loss 0.058967\nbatch 4125: loss 0.168250\nbatch 4126: loss 0.043312\nbatch 4127: loss 0.096739\nbatch 4128: loss 0.081905\nbatch 4129: loss 0.069751\nbatch 4130: loss 0.019592\nbatch 4131: loss 0.011159\nbatch 4132: loss 0.056865\nbatch 4133: loss 0.067476\nbatch 4134: loss 0.016793\nbatch 4135: loss 0.073278\nbatch 4136: loss 0.260357\nbatch 4137: loss 0.036116\nbatch 4138: loss 0.040832\nbatch 4139: loss 0.069383\nbatch 4140: loss 0.039017\nbatch 4141: loss 0.134644\nbatch 4142: loss 0.218052\nbatch 4143: loss 0.052090\nbatch 4144: loss 0.033593\nbatch 4145: loss 0.031694\nbatch 4146: loss 0.012201\nbatch 4147: loss 0.142814\nbatch 4148: loss 0.038096\nbatch 4149: loss 0.169943\nbatch 4150: loss 0.202975\nbatch 4151: loss 0.103145\nbatch 4152: loss 0.033626\nbatch 4153: loss 0.022346\nbatch 4154: loss 0.062512\nbatch 4155: loss 0.020932\nbatch 4156: loss 0.057397\nbatch 4157: loss 0.028983\nbatch 4158: loss 0.078143\nbatch 4159: loss 0.009809\nbatch 4160: loss 0.059932\nbatch 4161: loss 0.018184\nbatch 4162: loss 0.027542\nbatch 4163: loss 0.024069\nbatch 4164: loss 0.032677\nbatch 4165: loss 0.031462\nbatch 4166: loss 0.030570\nbatch 4167: loss 0.044136\nbatch 4168: loss 0.009922\nbatch 4169: loss 0.100120\nbatch 4170: loss 0.024348\nbatch 4171: loss 0.049505\nbatch 4172: loss 0.116061\nbatch 4173: loss 0.070022\nbatch 4174: loss 0.073901\nbatch 4175: loss 0.094977\nbatch 4176: loss 0.043858\nbatch 4177: loss 0.027354\nbatch 4178: loss 0.093270\nbatch 4179: loss 0.027614\nbatch 4180: loss 0.110144\nbatch 4181: loss 0.078291\nbatch 4182: loss 0.116596\nbatch 4183: loss 0.094379\nbatch 4184: loss 0.079861\nbatch 4185: loss 0.061025\nbatch 4186: loss 0.052278\nbatch 4187: loss 0.068470\nbatch 4188: loss 0.036438\nbatch 4189: loss 0.121193\nbatch 4190: loss 0.066411\nbatch 4191: loss 0.037054\nbatch 4192: loss 0.033539\nbatch 4193: loss 0.041911\nbatch 4194: loss 0.012619\nbatch 4195: loss 0.121263\nbatch 4196: loss 0.028694\nbatch 4197: loss 0.034576\nbatch 4198: loss 0.031181\nbatch 4199: loss 0.032672\nbatch 4200: loss 0.063967\nbatch 4201: loss 0.055629\nbatch 4202: loss 0.054001\nbatch 4203: loss 0.019090\nbatch 4204: loss 0.016794\nbatch 4205: loss 0.101938\nbatch 4206: loss 0.020136\nbatch 4207: loss 0.010868\nbatch 4208: loss 0.072612\nbatch 4209: loss 0.021893\nbatch 4210: loss 0.024759\nbatch 4211: loss 0.104780\nbatch 4212: loss 0.027515\nbatch 4213: loss 0.038699\nbatch 4214: loss 0.127623\nbatch 4215: loss 0.022926\nbatch 4216: loss 0.027775\nbatch 4217: loss 0.009050\nbatch 4218: loss 0.165356\nbatch 4219: loss 0.056571\nbatch 4220: loss 0.049263\nbatch 4221: loss 0.020511\nbatch 4222: loss 0.057492\nbatch 4223: loss 0.069880\nbatch 4224: loss 0.084668\nbatch 4225: loss 0.114218\nbatch 4226: loss 0.026923\nbatch 4227: loss 0.025806\nbatch 4228: loss 0.056442\nbatch 4229: loss 0.054397\nbatch 4230: loss 0.198230\nbatch 4231: loss 0.030126\nbatch 4232: loss 0.049388\nbatch 4233: loss 0.036379\nbatch 4234: loss 0.070147\nbatch 4235: loss 0.034129\nbatch 4236: loss 0.041817\nbatch 4237: loss 0.054895\nbatch 4238: loss 0.029602\nbatch 4239: loss 0.088970\nbatch 4240: loss 0.035132\nbatch 4241: loss 0.035825\nbatch 4242: loss 0.111460\nbatch 4243: loss 0.039041\nbatch 4244: loss 0.170241\nbatch 4245: loss 0.098304\nbatch 4246: loss 0.038031\nbatch 4247: loss 0.093403\nbatch 4248: loss 0.067439\nbatch 4249: loss 0.024526\nbatch 4250: loss 0.014258\nbatch 4251: loss 0.013984\nbatch 4252: loss 0.097118\nbatch 4253: loss 0.125053\nbatch 4254: loss 0.052694\nbatch 4255: loss 0.082522\nbatch 4256: loss 0.025319\nbatch 4257: loss 0.017925\nbatch 4258: loss 0.033297\nbatch 4259: loss 0.020978\nbatch 4260: loss 0.210877\nbatch 4261: loss 0.051886\nbatch 4262: loss 0.101297\nbatch 4263: loss 0.024880\nbatch 4264: loss 0.045190\nbatch 4265: loss 0.066787\nbatch 4266: loss 0.030417\nbatch 4267: loss 0.145596\nbatch 4268: loss 0.082445\nbatch 4269: loss 0.090376\nbatch 4270: loss 0.056749\nbatch 4271: loss 0.038192\nbatch 4272: loss 0.078953\nbatch 4273: loss 0.027205\nbatch 4274: loss 0.028332\nbatch 4275: loss 0.049005\nbatch 4276: loss 0.081437\nbatch 4277: loss 0.038279\nbatch 4278: loss 0.114312\nbatch 4279: loss 0.045355\nbatch 4280: loss 0.280410\nbatch 4281: loss 0.029365\nbatch 4282: loss 0.171193\nbatch 4283: loss 0.012605\nbatch 4284: loss 0.093855\nbatch 4285: loss 0.023516\nbatch 4286: loss 0.264199\nbatch 4287: loss 0.201072\nbatch 4288: loss 0.109581\nbatch 4289: loss 0.012515\nbatch 4290: loss 0.058061\nbatch 4291: loss 0.106864\nbatch 4292: loss 0.113133\nbatch 4293: loss 0.045321\nbatch 4294: loss 0.121048\nbatch 4295: loss 0.012077\nbatch 4296: loss 0.050099\nbatch 4297: loss 0.074575\nbatch 4298: loss 0.033153\nbatch 4299: loss 0.088799\nbatch 4300: loss 0.098647\nbatch 4301: loss 0.191936\nbatch 4302: loss 0.039692\nbatch 4303: loss 0.077334\nbatch 4304: loss 0.105449\nbatch 4305: loss 0.136374\nbatch 4306: loss 0.186118\nbatch 4307: loss 0.098241\nbatch 4308: loss 0.026751\nbatch 4309: loss 0.022714\nbatch 4310: loss 0.208058\nbatch 4311: loss 0.045072\nbatch 4312: loss 0.042191\nbatch 4313: loss 0.120545\nbatch 4314: loss 0.061863\nbatch 4315: loss 0.080573\nbatch 4316: loss 0.203217\nbatch 4317: loss 0.152934\nbatch 4318: loss 0.032757\nbatch 4319: loss 0.028440\nbatch 4320: loss 0.010329\nbatch 4321: loss 0.022681\nbatch 4322: loss 0.024831\nbatch 4323: loss 0.187937\nbatch 4324: loss 0.071306\nbatch 4325: loss 0.058630\nbatch 4326: loss 0.018656\nbatch 4327: loss 0.064613\nbatch 4328: loss 0.021099\nbatch 4329: loss 0.015910\nbatch 4330: loss 0.196749\nbatch 4331: loss 0.050550\nbatch 4332: loss 0.011712\nbatch 4333: loss 0.029287\nbatch 4334: loss 0.112335\nbatch 4335: loss 0.111313\nbatch 4336: loss 0.063762\nbatch 4337: loss 0.107429\nbatch 4338: loss 0.089416\nbatch 4339: loss 0.016413\nbatch 4340: loss 0.104138\nbatch 4341: loss 0.152222\nbatch 4342: loss 0.026453\nbatch 4343: loss 0.047773\nbatch 4344: loss 0.133173\nbatch 4345: loss 0.035334\nbatch 4346: loss 0.094140\nbatch 4347: loss 0.085653\nbatch 4348: loss 0.064478\nbatch 4349: loss 0.019753\nbatch 4350: loss 0.055455\nbatch 4351: loss 0.093907\nbatch 4352: loss 0.105303\nbatch 4353: loss 0.029370\nbatch 4354: loss 0.025895\nbatch 4355: loss 0.031982\nbatch 4356: loss 0.043547\nbatch 4357: loss 0.020289\nbatch 4358: loss 0.059443\nbatch 4359: loss 0.119490\nbatch 4360: loss 0.121171\nbatch 4361: loss 0.115523\nbatch 4362: loss 0.090860\nbatch 4363: loss 0.089882\nbatch 4364: loss 0.007122\nbatch 4365: loss 0.099395\nbatch 4366: loss 0.018151\nbatch 4367: loss 0.019194\nbatch 4368: loss 0.093040\nbatch 4369: loss 0.086788\nbatch 4370: loss 0.016913\nbatch 4371: loss 0.104821\nbatch 4372: loss 0.032723\nbatch 4373: loss 0.011644\nbatch 4374: loss 0.063435\nbatch 4375: loss 0.178748\nbatch 4376: loss 0.095031\nbatch 4377: loss 0.094182\nbatch 4378: loss 0.060030\nbatch 4379: loss 0.084501\nbatch 4380: loss 0.101152\nbatch 4381: loss 0.064565\nbatch 4382: loss 0.021469\nbatch 4383: loss 0.138676\nbatch 4384: loss 0.028108\nbatch 4385: loss 0.239589\nbatch 4386: loss 0.038974\nbatch 4387: loss 0.026198\nbatch 4388: loss 0.044969\nbatch 4389: loss 0.089225\nbatch 4390: loss 0.042497\nbatch 4391: loss 0.234375\nbatch 4392: loss 0.094983\nbatch 4393: loss 0.071042\nbatch 4394: loss 0.144799\nbatch 4395: loss 0.041227\nbatch 4396: loss 0.122418\nbatch 4397: loss 0.196605\nbatch 4398: loss 0.071087\nbatch 4399: loss 0.042146\nbatch 4400: loss 0.097154\nbatch 4401: loss 0.049896\nbatch 4402: loss 0.079467\nbatch 4403: loss 0.200671\nbatch 4404: loss 0.055155\nbatch 4405: loss 0.084627\nbatch 4406: loss 0.037813\nbatch 4407: loss 0.037378\nbatch 4408: loss 0.104879\nbatch 4409: loss 0.359503\nbatch 4410: loss 0.042335\nbatch 4411: loss 0.050792\nbatch 4412: loss 0.054467\nbatch 4413: loss 0.108014\nbatch 4414: loss 0.025853\nbatch 4415: loss 0.186901\nbatch 4416: loss 0.062161\nbatch 4417: loss 0.085667\nbatch 4418: loss 0.073591\nbatch 4419: loss 0.026807\nbatch 4420: loss 0.029952\nbatch 4421: loss 0.044306\nbatch 4422: loss 0.155360\nbatch 4423: loss 0.026815\nbatch 4424: loss 0.091095\nbatch 4425: loss 0.032507\nbatch 4426: loss 0.016069\nbatch 4427: loss 0.071888\nbatch 4428: loss 0.011043\nbatch 4429: loss 0.028137\nbatch 4430: loss 0.124240\nbatch 4431: loss 0.055434\nbatch 4432: loss 0.034492\nbatch 4433: loss 0.092140\nbatch 4434: loss 0.074054\nbatch 4435: loss 0.100669\nbatch 4436: loss 0.014944\nbatch 4437: loss 0.046123\nbatch 4438: loss 0.084201\nbatch 4439: loss 0.071019\nbatch 4440: loss 0.066251\nbatch 4441: loss 0.040368\nbatch 4442: loss 0.021354\nbatch 4443: loss 0.137964\nbatch 4444: loss 0.273096\nbatch 4445: loss 0.058630\nbatch 4446: loss 0.166808\nbatch 4447: loss 0.049412\nbatch 4448: loss 0.008893\nbatch 4449: loss 0.078823\nbatch 4450: loss 0.160548\nbatch 4451: loss 0.013031\nbatch 4452: loss 0.037854\nbatch 4453: loss 0.044730\nbatch 4454: loss 0.083948\nbatch 4455: loss 0.011996\nbatch 4456: loss 0.015293\nbatch 4457: loss 0.081876\nbatch 4458: loss 0.044806\nbatch 4459: loss 0.045197\nbatch 4460: loss 0.162683\nbatch 4461: loss 0.100964\nbatch 4462: loss 0.079187\nbatch 4463: loss 0.060997\nbatch 4464: loss 0.006516\nbatch 4465: loss 0.041508\nbatch 4466: loss 0.020316\nbatch 4467: loss 0.052016\nbatch 4468: loss 0.014887\nbatch 4469: loss 0.026400\nbatch 4470: loss 0.080016\nbatch 4471: loss 0.032920\nbatch 4472: loss 0.060706\nbatch 4473: loss 0.034913\nbatch 4474: loss 0.007165\nbatch 4475: loss 0.019091\nbatch 4476: loss 0.065910\nbatch 4477: loss 0.021663\nbatch 4478: loss 0.055570\nbatch 4479: loss 0.049860\nbatch 4480: loss 0.086456\nbatch 4481: loss 0.010507\nbatch 4482: loss 0.042307\nbatch 4483: loss 0.066337\nbatch 4484: loss 0.010053\nbatch 4485: loss 0.075453\nbatch 4486: loss 0.053280\nbatch 4487: loss 0.029741\nbatch 4488: loss 0.098131\nbatch 4489: loss 0.095543\nbatch 4490: loss 0.104449\nbatch 4491: loss 0.030720\nbatch 4492: loss 0.077790\nbatch 4493: loss 0.151310\nbatch 4494: loss 0.042771\nbatch 4495: loss 0.163987\nbatch 4496: loss 0.099394\nbatch 4497: loss 0.018110\nbatch 4498: loss 0.046340\nbatch 4499: loss 0.027575\nbatch 4500: loss 0.032341\nbatch 4501: loss 0.010788\nbatch 4502: loss 0.172819\nbatch 4503: loss 0.036104\nbatch 4504: loss 0.065781\nbatch 4505: loss 0.047860\nbatch 4506: loss 0.013137\nbatch 4507: loss 0.108438\nbatch 4508: loss 0.020371\nbatch 4509: loss 0.031772\nbatch 4510: loss 0.025071\nbatch 4511: loss 0.036624\nbatch 4512: loss 0.024859\nbatch 4513: loss 0.065112\nbatch 4514: loss 0.053658\nbatch 4515: loss 0.049479\nbatch 4516: loss 0.111419\nbatch 4517: loss 0.029739\nbatch 4518: loss 0.155259\nbatch 4519: loss 0.064110\nbatch 4520: loss 0.023245\nbatch 4521: loss 0.051879\nbatch 4522: loss 0.015465\nbatch 4523: loss 0.032972\nbatch 4524: loss 0.091544\nbatch 4525: loss 0.096006\nbatch 4526: loss 0.049457\nbatch 4527: loss 0.054215\nbatch 4528: loss 0.070639\nbatch 4529: loss 0.023142\nbatch 4530: loss 0.076602\nbatch 4531: loss 0.085155\nbatch 4532: loss 0.028787\nbatch 4533: loss 0.036044\nbatch 4534: loss 0.055263\nbatch 4535: loss 0.032175\nbatch 4536: loss 0.023407\nbatch 4537: loss 0.008450\nbatch 4538: loss 0.015778\nbatch 4539: loss 0.032573\nbatch 4540: loss 0.010861\nbatch 4541: loss 0.130010\nbatch 4542: loss 0.012192\nbatch 4543: loss 0.019954\nbatch 4544: loss 0.013477\nbatch 4545: loss 0.088395\nbatch 4546: loss 0.038227\nbatch 4547: loss 0.031660\nbatch 4548: loss 0.168496\nbatch 4549: loss 0.036639\nbatch 4550: loss 0.102891\nbatch 4551: loss 0.099097\nbatch 4552: loss 0.094111\nbatch 4553: loss 0.040132\nbatch 4554: loss 0.028471\nbatch 4555: loss 0.028467\nbatch 4556: loss 0.069227\nbatch 4557: loss 0.067762\nbatch 4558: loss 0.008052\nbatch 4559: loss 0.026709\nbatch 4560: loss 0.081124\nbatch 4561: loss 0.047191\nbatch 4562: loss 0.012784\nbatch 4563: loss 0.106652\nbatch 4564: loss 0.023883\nbatch 4565: loss 0.050714\nbatch 4566: loss 0.108784\nbatch 4567: loss 0.066303\nbatch 4568: loss 0.090321\nbatch 4569: loss 0.069097\nbatch 4570: loss 0.060434\nbatch 4571: loss 0.045543\nbatch 4572: loss 0.039802\nbatch 4573: loss 0.223406\nbatch 4574: loss 0.067940\nbatch 4575: loss 0.096096\nbatch 4576: loss 0.030764\nbatch 4577: loss 0.024669\nbatch 4578: loss 0.101815\nbatch 4579: loss 0.140765\nbatch 4580: loss 0.014577\nbatch 4581: loss 0.114143\nbatch 4582: loss 0.055311\nbatch 4583: loss 0.032176\nbatch 4584: loss 0.030481\nbatch 4585: loss 0.072150\nbatch 4586: loss 0.052974\nbatch 4587: loss 0.090104\nbatch 4588: loss 0.013065\nbatch 4589: loss 0.026878\nbatch 4590: loss 0.052569\nbatch 4591: loss 0.024143\nbatch 4592: loss 0.126002\nbatch 4593: loss 0.030466\nbatch 4594: loss 0.016073\nbatch 4595: loss 0.054686\nbatch 4596: loss 0.066967\nbatch 4597: loss 0.056272\nbatch 4598: loss 0.066965\nbatch 4599: loss 0.037827\nbatch 4600: loss 0.046917\nbatch 4601: loss 0.007089\nbatch 4602: loss 0.021880\nbatch 4603: loss 0.057453\nbatch 4604: loss 0.099508\nbatch 4605: loss 0.013380\nbatch 4606: loss 0.016564\nbatch 4607: loss 0.025500\nbatch 4608: loss 0.034941\nbatch 4609: loss 0.110723\nbatch 4610: loss 0.023440\nbatch 4611: loss 0.031124\nbatch 4612: loss 0.044305\nbatch 4613: loss 0.168771\nbatch 4614: loss 0.070487\nbatch 4615: loss 0.012196\nbatch 4616: loss 0.018992\nbatch 4617: loss 0.155992\nbatch 4618: loss 0.039360\nbatch 4619: loss 0.018045\nbatch 4620: loss 0.057273\nbatch 4621: loss 0.055915\nbatch 4622: loss 0.137737\nbatch 4623: loss 0.083681\nbatch 4624: loss 0.094664\nbatch 4625: loss 0.059923\nbatch 4626: loss 0.051896\nbatch 4627: loss 0.052322\nbatch 4628: loss 0.063411\nbatch 4629: loss 0.169422\nbatch 4630: loss 0.051723\nbatch 4631: loss 0.036875\nbatch 4632: loss 0.022083\nbatch 4633: loss 0.121328\nbatch 4634: loss 0.039044\nbatch 4635: loss 0.011390\nbatch 4636: loss 0.104353\nbatch 4637: loss 0.011759\nbatch 4638: loss 0.085270\nbatch 4639: loss 0.056657\nbatch 4640: loss 0.015071\nbatch 4641: loss 0.158927\nbatch 4642: loss 0.052256\nbatch 4643: loss 0.070012\nbatch 4644: loss 0.044866\nbatch 4645: loss 0.050564\nbatch 4646: loss 0.062543\nbatch 4647: loss 0.049503\nbatch 4648: loss 0.112320\nbatch 4649: loss 0.072179\nbatch 4650: loss 0.080378\nbatch 4651: loss 0.088997\nbatch 4652: loss 0.030903\nbatch 4653: loss 0.116215\nbatch 4654: loss 0.023699\nbatch 4655: loss 0.014023\nbatch 4656: loss 0.015343\nbatch 4657: loss 0.123055\nbatch 4658: loss 0.230287\nbatch 4659: loss 0.079738\nbatch 4660: loss 0.034877\nbatch 4661: loss 0.029492\nbatch 4662: loss 0.038927\nbatch 4663: loss 0.056998\nbatch 4664: loss 0.032622\nbatch 4665: loss 0.046884\nbatch 4666: loss 0.047500\nbatch 4667: loss 0.031908\nbatch 4668: loss 0.012713\nbatch 4669: loss 0.069062\nbatch 4670: loss 0.201234\nbatch 4671: loss 0.079138\nbatch 4672: loss 0.106432\nbatch 4673: loss 0.053451\nbatch 4674: loss 0.156960\nbatch 4675: loss 0.118392\nbatch 4676: loss 0.089006\nbatch 4677: loss 0.074952\nbatch 4678: loss 0.080031\nbatch 4679: loss 0.036832\nbatch 4680: loss 0.088770\nbatch 4681: loss 0.088663\nbatch 4682: loss 0.186573\nbatch 4683: loss 0.010726\nbatch 4684: loss 0.012908\nbatch 4685: loss 0.021980\nbatch 4686: loss 0.085684\nbatch 4687: loss 0.019400\nbatch 4688: loss 0.020273\nbatch 4689: loss 0.019536\nbatch 4690: loss 0.046320\nbatch 4691: loss 0.037101\nbatch 4692: loss 0.036982\nbatch 4693: loss 0.062887\nbatch 4694: loss 0.179082\nbatch 4695: loss 0.016728\nbatch 4696: loss 0.036290\nbatch 4697: loss 0.079870\nbatch 4698: loss 0.033611\nbatch 4699: loss 0.040801\nbatch 4700: loss 0.056086\nbatch 4701: loss 0.009776\nbatch 4702: loss 0.050277\nbatch 4703: loss 0.042263\nbatch 4704: loss 0.103989\nbatch 4705: loss 0.047666\nbatch 4706: loss 0.056409\nbatch 4707: loss 0.078563\nbatch 4708: loss 0.092661\nbatch 4709: loss 0.029944\nbatch 4710: loss 0.019457\nbatch 4711: loss 0.146076\nbatch 4712: loss 0.026677\nbatch 4713: loss 0.093151\nbatch 4714: loss 0.019231\nbatch 4715: loss 0.041439\nbatch 4716: loss 0.080136\nbatch 4717: loss 0.010337\nbatch 4718: loss 0.094987\nbatch 4719: loss 0.106420\nbatch 4720: loss 0.034649\nbatch 4721: loss 0.052458\nbatch 4722: loss 0.015950\nbatch 4723: loss 0.020024\nbatch 4724: loss 0.063007\nbatch 4725: loss 0.035840\nbatch 4726: loss 0.025345\nbatch 4727: loss 0.079833\nbatch 4728: loss 0.036193\nbatch 4729: loss 0.085705\nbatch 4730: loss 0.098688\nbatch 4731: loss 0.009309\nbatch 4732: loss 0.052039\nbatch 4733: loss 0.101911\nbatch 4734: loss 0.012058\nbatch 4735: loss 0.010120\nbatch 4736: loss 0.021645\nbatch 4737: loss 0.091934\nbatch 4738: loss 0.038469\nbatch 4739: loss 0.104023\nbatch 4740: loss 0.095530\nbatch 4741: loss 0.125661\nbatch 4742: loss 0.039686\nbatch 4743: loss 0.059877\nbatch 4744: loss 0.046033\nbatch 4745: loss 0.041591\nbatch 4746: loss 0.064125\nbatch 4747: loss 0.019644\nbatch 4748: loss 0.036442\nbatch 4749: loss 0.149722\nbatch 4750: loss 0.055788\nbatch 4751: loss 0.168237\nbatch 4752: loss 0.006602\nbatch 4753: loss 0.008671\nbatch 4754: loss 0.158841\nbatch 4755: loss 0.119690\nbatch 4756: loss 0.261532\nbatch 4757: loss 0.189246\nbatch 4758: loss 0.036806\nbatch 4759: loss 0.044932\nbatch 4760: loss 0.122907\nbatch 4761: loss 0.056965\nbatch 4762: loss 0.015173\nbatch 4763: loss 0.035458\nbatch 4764: loss 0.079013\nbatch 4765: loss 0.097796\nbatch 4766: loss 0.255749\nbatch 4767: loss 0.097251\nbatch 4768: loss 0.137249\nbatch 4769: loss 0.035827\nbatch 4770: loss 0.008004\nbatch 4771: loss 0.096414\nbatch 4772: loss 0.029982\nbatch 4773: loss 0.013900\nbatch 4774: loss 0.052228\nbatch 4775: loss 0.036472\nbatch 4776: loss 0.033692\nbatch 4777: loss 0.053079\nbatch 4778: loss 0.007781\nbatch 4779: loss 0.022241\nbatch 4780: loss 0.035858\nbatch 4781: loss 0.162971\nbatch 4782: loss 0.087733\nbatch 4783: loss 0.112193\nbatch 4784: loss 0.038159\nbatch 4785: loss 0.015901\nbatch 4786: loss 0.026310\nbatch 4787: loss 0.074286\nbatch 4788: loss 0.036621\nbatch 4789: loss 0.119199\nbatch 4790: loss 0.117911\nbatch 4791: loss 0.171902\nbatch 4792: loss 0.014947\nbatch 4793: loss 0.060078\nbatch 4794: loss 0.020346\nbatch 4795: loss 0.036720\nbatch 4796: loss 0.052850\nbatch 4797: loss 0.041447\nbatch 4798: loss 0.032260\nbatch 4799: loss 0.093821\nbatch 4800: loss 0.128712\nbatch 4801: loss 0.013567\nbatch 4802: loss 0.049046\nbatch 4803: loss 0.070926\nbatch 4804: loss 0.016453\nbatch 4805: loss 0.083013\nbatch 4806: loss 0.075990\nbatch 4807: loss 0.016627\nbatch 4808: loss 0.043088\nbatch 4809: loss 0.014281\nbatch 4810: loss 0.105461\nbatch 4811: loss 0.031006\nbatch 4812: loss 0.056301\nbatch 4813: loss 0.044069\nbatch 4814: loss 0.062976\nbatch 4815: loss 0.053726\nbatch 4816: loss 0.187430\nbatch 4817: loss 0.205005\nbatch 4818: loss 0.017967\nbatch 4819: loss 0.037856\nbatch 4820: loss 0.028505\nbatch 4821: loss 0.038099\nbatch 4822: loss 0.084085\nbatch 4823: loss 0.118319\nbatch 4824: loss 0.046616\nbatch 4825: loss 0.049863\nbatch 4826: loss 0.009117\nbatch 4827: loss 0.022149\nbatch 4828: loss 0.016912\nbatch 4829: loss 0.064445\nbatch 4830: loss 0.068601\nbatch 4831: loss 0.064295\nbatch 4832: loss 0.065443\nbatch 4833: loss 0.053534\nbatch 4834: loss 0.066952\nbatch 4835: loss 0.140454\nbatch 4836: loss 0.129848\nbatch 4837: loss 0.028837\nbatch 4838: loss 0.028006\nbatch 4839: loss 0.112528\nbatch 4840: loss 0.144156\nbatch 4841: loss 0.050641\nbatch 4842: loss 0.008825\nbatch 4843: loss 0.041035\nbatch 4844: loss 0.109681\nbatch 4845: loss 0.067921\nbatch 4846: loss 0.040927\nbatch 4847: loss 0.071044\nbatch 4848: loss 0.031649\nbatch 4849: loss 0.064040\nbatch 4850: loss 0.061621\nbatch 4851: loss 0.074186\nbatch 4852: loss 0.019951\nbatch 4853: loss 0.076593\nbatch 4854: loss 0.026399\nbatch 4855: loss 0.009817\nbatch 4856: loss 0.081607\nbatch 4857: loss 0.015654\nbatch 4858: loss 0.065499\nbatch 4859: loss 0.026631\nbatch 4860: loss 0.092057\nbatch 4861: loss 0.050768\nbatch 4862: loss 0.017087\nbatch 4863: loss 0.017769\nbatch 4864: loss 0.084876\nbatch 4865: loss 0.131372\nbatch 4866: loss 0.084049\nbatch 4867: loss 0.036028\nbatch 4868: loss 0.025037\nbatch 4869: loss 0.091521\nbatch 4870: loss 0.019386\nbatch 4871: loss 0.162100\nbatch 4872: loss 0.114902\nbatch 4873: loss 0.051623\nbatch 4874: loss 0.050571\nbatch 4875: loss 0.023215\nbatch 4876: loss 0.021685\nbatch 4877: loss 0.034365\nbatch 4878: loss 0.091946\nbatch 4879: loss 0.021178\nbatch 4880: loss 0.076962\nbatch 4881: loss 0.007043\nbatch 4882: loss 0.067199\nbatch 4883: loss 0.085488\nbatch 4884: loss 0.082923\nbatch 4885: loss 0.021203\nbatch 4886: loss 0.068113\nbatch 4887: loss 0.040221\nbatch 4888: loss 0.020480\nbatch 4889: loss 0.087354\nbatch 4890: loss 0.094778\nbatch 4891: loss 0.034214\nbatch 4892: loss 0.019393\nbatch 4893: loss 0.026096\nbatch 4894: loss 0.118116\nbatch 4895: loss 0.035144\nbatch 4896: loss 0.046021\nbatch 4897: loss 0.047447\nbatch 4898: loss 0.034535\nbatch 4899: loss 0.072640\nbatch 4900: loss 0.099463\nbatch 4901: loss 0.129775\nbatch 4902: loss 0.085363\nbatch 4903: loss 0.021579\nbatch 4904: loss 0.069015\nbatch 4905: loss 0.025396\nbatch 4906: loss 0.115769\nbatch 4907: loss 0.009730\nbatch 4908: loss 0.030019\nbatch 4909: loss 0.115774\nbatch 4910: loss 0.014535\nbatch 4911: loss 0.048896\nbatch 4912: loss 0.018935\nbatch 4913: loss 0.111427\nbatch 4914: loss 0.056371\nbatch 4915: loss 0.132346\nbatch 4916: loss 0.039594\nbatch 4917: loss 0.051369\nbatch 4918: loss 0.086309\nbatch 4919: loss 0.057338\nbatch 4920: loss 0.031405\nbatch 4921: loss 0.248657\nbatch 4922: loss 0.183186\nbatch 4923: loss 0.041257\nbatch 4924: loss 0.009377\nbatch 4925: loss 0.173270\nbatch 4926: loss 0.076990\nbatch 4927: loss 0.112737\nbatch 4928: loss 0.059203\nbatch 4929: loss 0.025857\nbatch 4930: loss 0.049073\nbatch 4931: loss 0.019522\nbatch 4932: loss 0.026895\nbatch 4933: loss 0.022746\nbatch 4934: loss 0.033874\nbatch 4935: loss 0.111467\nbatch 4936: loss 0.064248\nbatch 4937: loss 0.004498\nbatch 4938: loss 0.074024\nbatch 4939: loss 0.020515\nbatch 4940: loss 0.036666\nbatch 4941: loss 0.036736\nbatch 4942: loss 0.129989\nbatch 4943: loss 0.211215\nbatch 4944: loss 0.066839\nbatch 4945: loss 0.070109\nbatch 4946: loss 0.015832\nbatch 4947: loss 0.104254\nbatch 4948: loss 0.017904\nbatch 4949: loss 0.022330\nbatch 4950: loss 0.031925\nbatch 4951: loss 0.037710\nbatch 4952: loss 0.029300\nbatch 4953: loss 0.030525\nbatch 4954: loss 0.047843\nbatch 4955: loss 0.018322\nbatch 4956: loss 0.009292\nbatch 4957: loss 0.019295\nbatch 4958: loss 0.178928\nbatch 4959: loss 0.135022\nbatch 4960: loss 0.122249\nbatch 4961: loss 0.037201\nbatch 4962: loss 0.089647\nbatch 4963: loss 0.022553\nbatch 4964: loss 0.095265\nbatch 4965: loss 0.199133\nbatch 4966: loss 0.134087\nbatch 4967: loss 0.122174\nbatch 4968: loss 0.097500\nbatch 4969: loss 0.099123\nbatch 4970: loss 0.104263\nbatch 4971: loss 0.008501\nbatch 4972: loss 0.009429\nbatch 4973: loss 0.052673\nbatch 4974: loss 0.025128\nbatch 4975: loss 0.073957\nbatch 4976: loss 0.071531\nbatch 4977: loss 0.073035\nbatch 4978: loss 0.112723\nbatch 4979: loss 0.025720\nbatch 4980: loss 0.131755\nbatch 4981: loss 0.020989\nbatch 4982: loss 0.024024\nbatch 4983: loss 0.073854\nbatch 4984: loss 0.175505\nbatch 4985: loss 0.038962\nbatch 4986: loss 0.030334\nbatch 4987: loss 0.073001\nbatch 4988: loss 0.105511\nbatch 4989: loss 0.102569\nbatch 4990: loss 0.068215\nbatch 4991: loss 0.008326\nbatch 4992: loss 0.023286\nbatch 4993: loss 0.135186\nbatch 4994: loss 0.056828\nbatch 4995: loss 0.042246\nbatch 4996: loss 0.055637\nbatch 4997: loss 0.178904\nbatch 4998: loss 0.066551\nbatch 4999: loss 0.053441\nbatch 5000: loss 0.028850\nbatch 5001: loss 0.088131\nbatch 5002: loss 0.040721\nbatch 5003: loss 0.085497\nbatch 5004: loss 0.009506\nbatch 5005: loss 0.009876\nbatch 5006: loss 0.011460\nbatch 5007: loss 0.031110\nbatch 5008: loss 0.058142\nbatch 5009: loss 0.026400\nbatch 5010: loss 0.010011\nbatch 5011: loss 0.092928\nbatch 5012: loss 0.138878\nbatch 5013: loss 0.138896\nbatch 5014: loss 0.086096\nbatch 5015: loss 0.028122\nbatch 5016: loss 0.034773\nbatch 5017: loss 0.036653\nbatch 5018: loss 0.033446\nbatch 5019: loss 0.027146\nbatch 5020: loss 0.126757\nbatch 5021: loss 0.012749\nbatch 5022: loss 0.054300\nbatch 5023: loss 0.036602\nbatch 5024: loss 0.013189\nbatch 5025: loss 0.017937\nbatch 5026: loss 0.044180\nbatch 5027: loss 0.071706\nbatch 5028: loss 0.032391\nbatch 5029: loss 0.081595\nbatch 5030: loss 0.028434\nbatch 5031: loss 0.014479\nbatch 5032: loss 0.006957\nbatch 5033: loss 0.017652\nbatch 5034: loss 0.007032\nbatch 5035: loss 0.166440\nbatch 5036: loss 0.097836\nbatch 5037: loss 0.024540\nbatch 5038: loss 0.188763\nbatch 5039: loss 0.032752\nbatch 5040: loss 0.019572\nbatch 5041: loss 0.052951\nbatch 5042: loss 0.052240\nbatch 5043: loss 0.018665\nbatch 5044: loss 0.032368\nbatch 5045: loss 0.077310\nbatch 5046: loss 0.028545\nbatch 5047: loss 0.089569\nbatch 5048: loss 0.085755\nbatch 5049: loss 0.010440\nbatch 5050: loss 0.004824\nbatch 5051: loss 0.251458\nbatch 5052: loss 0.173320\nbatch 5053: loss 0.121966\nbatch 5054: loss 0.081305\nbatch 5055: loss 0.073943\nbatch 5056: loss 0.025946\nbatch 5057: loss 0.042060\nbatch 5058: loss 0.041102\nbatch 5059: loss 0.063516\nbatch 5060: loss 0.016129\nbatch 5061: loss 0.296195\nbatch 5062: loss 0.013997\nbatch 5063: loss 0.011040\nbatch 5064: loss 0.118559\nbatch 5065: loss 0.030121\nbatch 5066: loss 0.019794\nbatch 5067: loss 0.086454\nbatch 5068: loss 0.017165\nbatch 5069: loss 0.066462\nbatch 5070: loss 0.049082\nbatch 5071: loss 0.011881\nbatch 5072: loss 0.180335\nbatch 5073: loss 0.029341\nbatch 5074: loss 0.216979\nbatch 5075: loss 0.045997\nbatch 5076: loss 0.026935\nbatch 5077: loss 0.073494\nbatch 5078: loss 0.118646\nbatch 5079: loss 0.025956\nbatch 5080: loss 0.057727\nbatch 5081: loss 0.070503\nbatch 5082: loss 0.093224\nbatch 5083: loss 0.070749\nbatch 5084: loss 0.019833\nbatch 5085: loss 0.058947\nbatch 5086: loss 0.040708\nbatch 5087: loss 0.020880\nbatch 5088: loss 0.063114\nbatch 5089: loss 0.020471\nbatch 5090: loss 0.026924\nbatch 5091: loss 0.080842\nbatch 5092: loss 0.022111\nbatch 5093: loss 0.034976\nbatch 5094: loss 0.172243\nbatch 5095: loss 0.023813\nbatch 5096: loss 0.075317\nbatch 5097: loss 0.011822\nbatch 5098: loss 0.045877\nbatch 5099: loss 0.069413\nbatch 5100: loss 0.025283\nbatch 5101: loss 0.098571\nbatch 5102: loss 0.066243\nbatch 5103: loss 0.039222\nbatch 5104: loss 0.117883\nbatch 5105: loss 0.055316\nbatch 5106: loss 0.084937\nbatch 5107: loss 0.009594\nbatch 5108: loss 0.142995\nbatch 5109: loss 0.041185\nbatch 5110: loss 0.005035\nbatch 5111: loss 0.011483\nbatch 5112: loss 0.068976\nbatch 5113: loss 0.066691\nbatch 5114: loss 0.039711\nbatch 5115: loss 0.014225\nbatch 5116: loss 0.133180\nbatch 5117: loss 0.021184\nbatch 5118: loss 0.045953\nbatch 5119: loss 0.004237\nbatch 5120: loss 0.103790\nbatch 5121: loss 0.044221\nbatch 5122: loss 0.010225\nbatch 5123: loss 0.153761\nbatch 5124: loss 0.076567\nbatch 5125: loss 0.102062\nbatch 5126: loss 0.138778\nbatch 5127: loss 0.041004\nbatch 5128: loss 0.041107\nbatch 5129: loss 0.045074\nbatch 5130: loss 0.035944\nbatch 5131: loss 0.238629\nbatch 5132: loss 0.016975\nbatch 5133: loss 0.049462\nbatch 5134: loss 0.014096\nbatch 5135: loss 0.156871\nbatch 5136: loss 0.072294\nbatch 5137: loss 0.103469\nbatch 5138: loss 0.074299\nbatch 5139: loss 0.029697\nbatch 5140: loss 0.031770\nbatch 5141: loss 0.068154\nbatch 5142: loss 0.163474\nbatch 5143: loss 0.037851\nbatch 5144: loss 0.094527\nbatch 5145: loss 0.111045\nbatch 5146: loss 0.039679\nbatch 5147: loss 0.029374\nbatch 5148: loss 0.025441\nbatch 5149: loss 0.049318\nbatch 5150: loss 0.084789\nbatch 5151: loss 0.041355\nbatch 5152: loss 0.034036\nbatch 5153: loss 0.038907\nbatch 5154: loss 0.040578\nbatch 5155: loss 0.069858\nbatch 5156: loss 0.068751\nbatch 5157: loss 0.148221\nbatch 5158: loss 0.065327\nbatch 5159: loss 0.133723\nbatch 5160: loss 0.013196\nbatch 5161: loss 0.101538\nbatch 5162: loss 0.058671\nbatch 5163: loss 0.038952\nbatch 5164: loss 0.028130\nbatch 5165: loss 0.058064\nbatch 5166: loss 0.010085\nbatch 5167: loss 0.021379\nbatch 5168: loss 0.016377\nbatch 5169: loss 0.008404\nbatch 5170: loss 0.084116\nbatch 5171: loss 0.025994\nbatch 5172: loss 0.017264\nbatch 5173: loss 0.014514\nbatch 5174: loss 0.041260\nbatch 5175: loss 0.047394\nbatch 5176: loss 0.047622\nbatch 5177: loss 0.041940\nbatch 5178: loss 0.015520\nbatch 5179: loss 0.023892\nbatch 5180: loss 0.018186\nbatch 5181: loss 0.134075\nbatch 5182: loss 0.048091\nbatch 5183: loss 0.047962\nbatch 5184: loss 0.020889\nbatch 5185: loss 0.048969\nbatch 5186: loss 0.062193\nbatch 5187: loss 0.019590\nbatch 5188: loss 0.042603\nbatch 5189: loss 0.030934\nbatch 5190: loss 0.020449\nbatch 5191: loss 0.066746\nbatch 5192: loss 0.084068\nbatch 5193: loss 0.043223\nbatch 5194: loss 0.121985\nbatch 5195: loss 0.016259\nbatch 5196: loss 0.027487\nbatch 5197: loss 0.053082\nbatch 5198: loss 0.011938\nbatch 5199: loss 0.028845\nbatch 5200: loss 0.023638\nbatch 5201: loss 0.125653\nbatch 5202: loss 0.155187\nbatch 5203: loss 0.040461\nbatch 5204: loss 0.009429\nbatch 5205: loss 0.056667\nbatch 5206: loss 0.014658\nbatch 5207: loss 0.009943\nbatch 5208: loss 0.005042\nbatch 5209: loss 0.051747\nbatch 5210: loss 0.044722\nbatch 5211: loss 0.071055\nbatch 5212: loss 0.074056\nbatch 5213: loss 0.109041\nbatch 5214: loss 0.044122\nbatch 5215: loss 0.014322\nbatch 5216: loss 0.029651\nbatch 5217: loss 0.027404\nbatch 5218: loss 0.079532\nbatch 5219: loss 0.007698\nbatch 5220: loss 0.187837\nbatch 5221: loss 0.034397\nbatch 5222: loss 0.100966\nbatch 5223: loss 0.043697\nbatch 5224: loss 0.015990\nbatch 5225: loss 0.066768\nbatch 5226: loss 0.067146\nbatch 5227: loss 0.035725\nbatch 5228: loss 0.016225\nbatch 5229: loss 0.013803\nbatch 5230: loss 0.111041\nbatch 5231: loss 0.006096\nbatch 5232: loss 0.058905\nbatch 5233: loss 0.060596\nbatch 5234: loss 0.023276\nbatch 5235: loss 0.085979\nbatch 5236: loss 0.027131\nbatch 5237: loss 0.080005\nbatch 5238: loss 0.221334\nbatch 5239: loss 0.104414\nbatch 5240: loss 0.012608\nbatch 5241: loss 0.052190\nbatch 5242: loss 0.021938\nbatch 5243: loss 0.050845\nbatch 5244: loss 0.117278\nbatch 5245: loss 0.038269\nbatch 5246: loss 0.125928\nbatch 5247: loss 0.048462\nbatch 5248: loss 0.042149\nbatch 5249: loss 0.021579\nbatch 5250: loss 0.021766\nbatch 5251: loss 0.075002\nbatch 5252: loss 0.022815\nbatch 5253: loss 0.015873\nbatch 5254: loss 0.031299\nbatch 5255: loss 0.060922\nbatch 5256: loss 0.012103\nbatch 5257: loss 0.079146\nbatch 5258: loss 0.067185\nbatch 5259: loss 0.045768\nbatch 5260: loss 0.029637\nbatch 5261: loss 0.185773\nbatch 5262: loss 0.113803\nbatch 5263: loss 0.043896\nbatch 5264: loss 0.017143\nbatch 5265: loss 0.087825\nbatch 5266: loss 0.025921\nbatch 5267: loss 0.014981\nbatch 5268: loss 0.044332\nbatch 5269: loss 0.043275\nbatch 5270: loss 0.196432\nbatch 5271: loss 0.136639\nbatch 5272: loss 0.031565\nbatch 5273: loss 0.028806\nbatch 5274: loss 0.019694\nbatch 5275: loss 0.063021\nbatch 5276: loss 0.199911\nbatch 5277: loss 0.047468\nbatch 5278: loss 0.037735\nbatch 5279: loss 0.030374\nbatch 5280: loss 0.044902\nbatch 5281: loss 0.115441\nbatch 5282: loss 0.263368\nbatch 5283: loss 0.014849\nbatch 5284: loss 0.027878\nbatch 5285: loss 0.033405\nbatch 5286: loss 0.031633\nbatch 5287: loss 0.038310\nbatch 5288: loss 0.131500\nbatch 5289: loss 0.096931\nbatch 5290: loss 0.047118\nbatch 5291: loss 0.028651\nbatch 5292: loss 0.014588\nbatch 5293: loss 0.189578\nbatch 5294: loss 0.029751\nbatch 5295: loss 0.041506\nbatch 5296: loss 0.123325\nbatch 5297: loss 0.017476\nbatch 5298: loss 0.009465\nbatch 5299: loss 0.056575\nbatch 5300: loss 0.028832\nbatch 5301: loss 0.031757\nbatch 5302: loss 0.113808\nbatch 5303: loss 0.021797\nbatch 5304: loss 0.007932\nbatch 5305: loss 0.012684\nbatch 5306: loss 0.018981\nbatch 5307: loss 0.037556\nbatch 5308: loss 0.039677\nbatch 5309: loss 0.050398\nbatch 5310: loss 0.098582\nbatch 5311: loss 0.031698\nbatch 5312: loss 0.011967\nbatch 5313: loss 0.034085\nbatch 5314: loss 0.034769\nbatch 5315: loss 0.094262\nbatch 5316: loss 0.017703\nbatch 5317: loss 0.010949\nbatch 5318: loss 0.011917\nbatch 5319: loss 0.044551\nbatch 5320: loss 0.043121\nbatch 5321: loss 0.010501\nbatch 5322: loss 0.044159\nbatch 5323: loss 0.033110\nbatch 5324: loss 0.008135\nbatch 5325: loss 0.226942\nbatch 5326: loss 0.012742\nbatch 5327: loss 0.011417\nbatch 5328: loss 0.094163\nbatch 5329: loss 0.035434\nbatch 5330: loss 0.099491\nbatch 5331: loss 0.019964\nbatch 5332: loss 0.071496\nbatch 5333: loss 0.016375\nbatch 5334: loss 0.091471\nbatch 5335: loss 0.012270\nbatch 5336: loss 0.028635\nbatch 5337: loss 0.042597\nbatch 5338: loss 0.015870\nbatch 5339: loss 0.050971\nbatch 5340: loss 0.114931\nbatch 5341: loss 0.049966\nbatch 5342: loss 0.026458\nbatch 5343: loss 0.049191\nbatch 5344: loss 0.018867\nbatch 5345: loss 0.012995\nbatch 5346: loss 0.055489\nbatch 5347: loss 0.013709\nbatch 5348: loss 0.025409\nbatch 5349: loss 0.017114\nbatch 5350: loss 0.024518\nbatch 5351: loss 0.016778\nbatch 5352: loss 0.016821\nbatch 5353: loss 0.104263\nbatch 5354: loss 0.149062\nbatch 5355: loss 0.054057\nbatch 5356: loss 0.047562\nbatch 5357: loss 0.134043\nbatch 5358: loss 0.014384\nbatch 5359: loss 0.069940\nbatch 5360: loss 0.030339\nbatch 5361: loss 0.036374\nbatch 5362: loss 0.029280\nbatch 5363: loss 0.005946\nbatch 5364: loss 0.060558\nbatch 5365: loss 0.090080\nbatch 5366: loss 0.006535\nbatch 5367: loss 0.063761\nbatch 5368: loss 0.047283\nbatch 5369: loss 0.065209\nbatch 5370: loss 0.046113\nbatch 5371: loss 0.022774\nbatch 5372: loss 0.018150\nbatch 5373: loss 0.021401\nbatch 5374: loss 0.041338\nbatch 5375: loss 0.032243\nbatch 5376: loss 0.105306\nbatch 5377: loss 0.035627\nbatch 5378: loss 0.047912\nbatch 5379: loss 0.024647\nbatch 5380: loss 0.023630\nbatch 5381: loss 0.050649\nbatch 5382: loss 0.118397\nbatch 5383: loss 0.060716\nbatch 5384: loss 0.011577\nbatch 5385: loss 0.058204\nbatch 5386: loss 0.047690\nbatch 5387: loss 0.041513\nbatch 5388: loss 0.031384\nbatch 5389: loss 0.010281\nbatch 5390: loss 0.040012\nbatch 5391: loss 0.011467\nbatch 5392: loss 0.057782\nbatch 5393: loss 0.113466\nbatch 5394: loss 0.010212\nbatch 5395: loss 0.035058\nbatch 5396: loss 0.098784\nbatch 5397: loss 0.047868\nbatch 5398: loss 0.045673\nbatch 5399: loss 0.049021\nbatch 5400: loss 0.059740\nbatch 5401: loss 0.039984\nbatch 5402: loss 0.036000\nbatch 5403: loss 0.046787\nbatch 5404: loss 0.022980\nbatch 5405: loss 0.047528\nbatch 5406: loss 0.034952\nbatch 5407: loss 0.046270\nbatch 5408: loss 0.022087\nbatch 5409: loss 0.020762\nbatch 5410: loss 0.031906\nbatch 5411: loss 0.005755\nbatch 5412: loss 0.021576\nbatch 5413: loss 0.058837\nbatch 5414: loss 0.027836\nbatch 5415: loss 0.033715\nbatch 5416: loss 0.013729\nbatch 5417: loss 0.159253\nbatch 5418: loss 0.029742\nbatch 5419: loss 0.013221\nbatch 5420: loss 0.016345\nbatch 5421: loss 0.009831\nbatch 5422: loss 0.036451\nbatch 5423: loss 0.056323\nbatch 5424: loss 0.048562\nbatch 5425: loss 0.089491\nbatch 5426: loss 0.124031\nbatch 5427: loss 0.008275\nbatch 5428: loss 0.071261\nbatch 5429: loss 0.062116\nbatch 5430: loss 0.020111\nbatch 5431: loss 0.042261\nbatch 5432: loss 0.060780\nbatch 5433: loss 0.025681\nbatch 5434: loss 0.105348\nbatch 5435: loss 0.058634\nbatch 5436: loss 0.011927\nbatch 5437: loss 0.030048\nbatch 5438: loss 0.061002\nbatch 5439: loss 0.023110\nbatch 5440: loss 0.018622\nbatch 5441: loss 0.028063\nbatch 5442: loss 0.030642\nbatch 5443: loss 0.059055\nbatch 5444: loss 0.028386\nbatch 5445: loss 0.081186\nbatch 5446: loss 0.090848\nbatch 5447: loss 0.085245\nbatch 5448: loss 0.036731\nbatch 5449: loss 0.068969\nbatch 5450: loss 0.016909\nbatch 5451: loss 0.130783\nbatch 5452: loss 0.053821\nbatch 5453: loss 0.012836\nbatch 5454: loss 0.048596\nbatch 5455: loss 0.016801\nbatch 5456: loss 0.020469\nbatch 5457: loss 0.006711\nbatch 5458: loss 0.010587\nbatch 5459: loss 0.019333\nbatch 5460: loss 0.178319\nbatch 5461: loss 0.078078\nbatch 5462: loss 0.041693\nbatch 5463: loss 0.158783\nbatch 5464: loss 0.085376\nbatch 5465: loss 0.008839\nbatch 5466: loss 0.021944\nbatch 5467: loss 0.050234\nbatch 5468: loss 0.067978\nbatch 5469: loss 0.142236\nbatch 5470: loss 0.011384\nbatch 5471: loss 0.045686\nbatch 5472: loss 0.015635\nbatch 5473: loss 0.116318\nbatch 5474: loss 0.054032\nbatch 5475: loss 0.080744\nbatch 5476: loss 0.036811\nbatch 5477: loss 0.030946\nbatch 5478: loss 0.072183\nbatch 5479: loss 0.032425\nbatch 5480: loss 0.021616\nbatch 5481: loss 0.039092\nbatch 5482: loss 0.091669\nbatch 5483: loss 0.018965\nbatch 5484: loss 0.133598\nbatch 5485: loss 0.134469\nbatch 5486: loss 0.084380\nbatch 5487: loss 0.007067\nbatch 5488: loss 0.021859\nbatch 5489: loss 0.021816\nbatch 5490: loss 0.018905\nbatch 5491: loss 0.043204\nbatch 5492: loss 0.047717\nbatch 5493: loss 0.048740\nbatch 5494: loss 0.069465\nbatch 5495: loss 0.009737\nbatch 5496: loss 0.073883\nbatch 5497: loss 0.063999\nbatch 5498: loss 0.007109\nbatch 5499: loss 0.092888\nbatch 5500: loss 0.008262\nbatch 5501: loss 0.003872\nbatch 5502: loss 0.018548\nbatch 5503: loss 0.076378\nbatch 5504: loss 0.073810\nbatch 5505: loss 0.038688\nbatch 5506: loss 0.033700\nbatch 5507: loss 0.125476\nbatch 5508: loss 0.050596\nbatch 5509: loss 0.095771\nbatch 5510: loss 0.128830\nbatch 5511: loss 0.058562\nbatch 5512: loss 0.021338\nbatch 5513: loss 0.049048\nbatch 5514: loss 0.022056\nbatch 5515: loss 0.017519\nbatch 5516: loss 0.062283\nbatch 5517: loss 0.009962\nbatch 5518: loss 0.033396\nbatch 5519: loss 0.050326\nbatch 5520: loss 0.099038\nbatch 5521: loss 0.036746\nbatch 5522: loss 0.030480\nbatch 5523: loss 0.068563\nbatch 5524: loss 0.028735\nbatch 5525: loss 0.149290\nbatch 5526: loss 0.041628\nbatch 5527: loss 0.114755\nbatch 5528: loss 0.047158\nbatch 5529: loss 0.068015\nbatch 5530: loss 0.023177\nbatch 5531: loss 0.050686\nbatch 5532: loss 0.042581\nbatch 5533: loss 0.132746\nbatch 5534: loss 0.016320\nbatch 5535: loss 0.021432\nbatch 5536: loss 0.012736\nbatch 5537: loss 0.097171\nbatch 5538: loss 0.090396\nbatch 5539: loss 0.131856\nbatch 5540: loss 0.065298\nbatch 5541: loss 0.025324\nbatch 5542: loss 0.021691\nbatch 5543: loss 0.017477\nbatch 5544: loss 0.036063\nbatch 5545: loss 0.078253\nbatch 5546: loss 0.016440\nbatch 5547: loss 0.073619\nbatch 5548: loss 0.017778\nbatch 5549: loss 0.018028\nbatch 5550: loss 0.010777\nbatch 5551: loss 0.061480\nbatch 5552: loss 0.042252\nbatch 5553: loss 0.014163\nbatch 5554: loss 0.289101\nbatch 5555: loss 0.042543\nbatch 5556: loss 0.032538\nbatch 5557: loss 0.042285\nbatch 5558: loss 0.033102\nbatch 5559: loss 0.112102\nbatch 5560: loss 0.009775\nbatch 5561: loss 0.024630\nbatch 5562: loss 0.006485\nbatch 5563: loss 0.145416\nbatch 5564: loss 0.088937\nbatch 5565: loss 0.068903\nbatch 5566: loss 0.035230\nbatch 5567: loss 0.222641\nbatch 5568: loss 0.025545\nbatch 5569: loss 0.013517\nbatch 5570: loss 0.026259\nbatch 5571: loss 0.044684\nbatch 5572: loss 0.127517\nbatch 5573: loss 0.055221\nbatch 5574: loss 0.070744\nbatch 5575: loss 0.121658\nbatch 5576: loss 0.135889\nbatch 5577: loss 0.016416\nbatch 5578: loss 0.031202\nbatch 5579: loss 0.016097\nbatch 5580: loss 0.166100\nbatch 5581: loss 0.015914\nbatch 5582: loss 0.072538\nbatch 5583: loss 0.085634\nbatch 5584: loss 0.055336\nbatch 5585: loss 0.080997\nbatch 5586: loss 0.064136\nbatch 5587: loss 0.031893\nbatch 5588: loss 0.038358\nbatch 5589: loss 0.010157\nbatch 5590: loss 0.061361\nbatch 5591: loss 0.025177\nbatch 5592: loss 0.021897\nbatch 5593: loss 0.032213\nbatch 5594: loss 0.081480\nbatch 5595: loss 0.191158\nbatch 5596: loss 0.095689\nbatch 5597: loss 0.012946\nbatch 5598: loss 0.060195\nbatch 5599: loss 0.082038\nbatch 5600: loss 0.017194\nbatch 5601: loss 0.022762\nbatch 5602: loss 0.008987\nbatch 5603: loss 0.006971\nbatch 5604: loss 0.035762\nbatch 5605: loss 0.117454\nbatch 5606: loss 0.024319\nbatch 5607: loss 0.103382\nbatch 5608: loss 0.009653\nbatch 5609: loss 0.047799\nbatch 5610: loss 0.024652\nbatch 5611: loss 0.058850\nbatch 5612: loss 0.068445\nbatch 5613: loss 0.050544\nbatch 5614: loss 0.042579\nbatch 5615: loss 0.128287\nbatch 5616: loss 0.044630\nbatch 5617: loss 0.047136\nbatch 5618: loss 0.127633\nbatch 5619: loss 0.032039\nbatch 5620: loss 0.228841\nbatch 5621: loss 0.040782\nbatch 5622: loss 0.072660\nbatch 5623: loss 0.050331\nbatch 5624: loss 0.102315\nbatch 5625: loss 0.011928\nbatch 5626: loss 0.038430\nbatch 5627: loss 0.035746\nbatch 5628: loss 0.064970\nbatch 5629: loss 0.040175\nbatch 5630: loss 0.155042\nbatch 5631: loss 0.062531\nbatch 5632: loss 0.017721\nbatch 5633: loss 0.041254\nbatch 5634: loss 0.058866\nbatch 5635: loss 0.015560\nbatch 5636: loss 0.038743\nbatch 5637: loss 0.027683\nbatch 5638: loss 0.013523\nbatch 5639: loss 0.026396\nbatch 5640: loss 0.007339\nbatch 5641: loss 0.011638\nbatch 5642: loss 0.097889\nbatch 5643: loss 0.155634\nbatch 5644: loss 0.040471\nbatch 5645: loss 0.035392\nbatch 5646: loss 0.130170\nbatch 5647: loss 0.041929\nbatch 5648: loss 0.009079\nbatch 5649: loss 0.048280\nbatch 5650: loss 0.024288\nbatch 5651: loss 0.015509\nbatch 5652: loss 0.029018\nbatch 5653: loss 0.018779\nbatch 5654: loss 0.045191\nbatch 5655: loss 0.055012\nbatch 5656: loss 0.065206\nbatch 5657: loss 0.032879\nbatch 5658: loss 0.020553\nbatch 5659: loss 0.006807\nbatch 5660: loss 0.025637\nbatch 5661: loss 0.007869\nbatch 5662: loss 0.057162\nbatch 5663: loss 0.105517\nbatch 5664: loss 0.013553\nbatch 5665: loss 0.036666\nbatch 5666: loss 0.024671\nbatch 5667: loss 0.151983\nbatch 5668: loss 0.061997\nbatch 5669: loss 0.021984\nbatch 5670: loss 0.082796\nbatch 5671: loss 0.068134\nbatch 5672: loss 0.007532\nbatch 5673: loss 0.014194\nbatch 5674: loss 0.047751\nbatch 5675: loss 0.015430\nbatch 5676: loss 0.050169\nbatch 5677: loss 0.015618\nbatch 5678: loss 0.027916\nbatch 5679: loss 0.061821\nbatch 5680: loss 0.366698\nbatch 5681: loss 0.025614\nbatch 5682: loss 0.012070\nbatch 5683: loss 0.042222\nbatch 5684: loss 0.035186\nbatch 5685: loss 0.020575\nbatch 5686: loss 0.011986\nbatch 5687: loss 0.023524\nbatch 5688: loss 0.024434\nbatch 5689: loss 0.050086\nbatch 5690: loss 0.033140\nbatch 5691: loss 0.136872\nbatch 5692: loss 0.058871\nbatch 5693: loss 0.043681\nbatch 5694: loss 0.024123\nbatch 5695: loss 0.065432\nbatch 5696: loss 0.047144\nbatch 5697: loss 0.132143\nbatch 5698: loss 0.034263\nbatch 5699: loss 0.070547\nbatch 5700: loss 0.022289\nbatch 5701: loss 0.027041\nbatch 5702: loss 0.049774\nbatch 5703: loss 0.015165\nbatch 5704: loss 0.041095\nbatch 5705: loss 0.042420\nbatch 5706: loss 0.025288\nbatch 5707: loss 0.083823\nbatch 5708: loss 0.149506\nbatch 5709: loss 0.022332\nbatch 5710: loss 0.078117\nbatch 5711: loss 0.028635\nbatch 5712: loss 0.016131\nbatch 5713: loss 0.016988\nbatch 5714: loss 0.058594\nbatch 5715: loss 0.049941\nbatch 5716: loss 0.058673\nbatch 5717: loss 0.006045\nbatch 5718: loss 0.006084\nbatch 5719: loss 0.069791\nbatch 5720: loss 0.014360\nbatch 5721: loss 0.008886\nbatch 5722: loss 0.008067\nbatch 5723: loss 0.154745\nbatch 5724: loss 0.077300\nbatch 5725: loss 0.042662\nbatch 5726: loss 0.023662\nbatch 5727: loss 0.003589\nbatch 5728: loss 0.079309\nbatch 5729: loss 0.003501\nbatch 5730: loss 0.015382\nbatch 5731: loss 0.064861\nbatch 5732: loss 0.041572\nbatch 5733: loss 0.103591\nbatch 5734: loss 0.015152\nbatch 5735: loss 0.060318\nbatch 5736: loss 0.120438\nbatch 5737: loss 0.026421\nbatch 5738: loss 0.151007\nbatch 5739: loss 0.091101\nbatch 5740: loss 0.063436\nbatch 5741: loss 0.026835\nbatch 5742: loss 0.006698\nbatch 5743: loss 0.057030\nbatch 5744: loss 0.141537\nbatch 5745: loss 0.022894\nbatch 5746: loss 0.072800\nbatch 5747: loss 0.023535\nbatch 5748: loss 0.043831\nbatch 5749: loss 0.051107\nbatch 5750: loss 0.062881\nbatch 5751: loss 0.040806\nbatch 5752: loss 0.122049\nbatch 5753: loss 0.026684\nbatch 5754: loss 0.071633\nbatch 5755: loss 0.093752\nbatch 5756: loss 0.034386\nbatch 5757: loss 0.065737\nbatch 5758: loss 0.010755\nbatch 5759: loss 0.067810\nbatch 5760: loss 0.056465\nbatch 5761: loss 0.009939\nbatch 5762: loss 0.096591\nbatch 5763: loss 0.036973\nbatch 5764: loss 0.025380\nbatch 5765: loss 0.007160\nbatch 5766: loss 0.036381\nbatch 5767: loss 0.023027\nbatch 5768: loss 0.052809\nbatch 5769: loss 0.054649\nbatch 5770: loss 0.021423\nbatch 5771: loss 0.044901\nbatch 5772: loss 0.034439\nbatch 5773: loss 0.011092\nbatch 5774: loss 0.039149\nbatch 5775: loss 0.028322\nbatch 5776: loss 0.021590\nbatch 5777: loss 0.084340\nbatch 5778: loss 0.019431\nbatch 5779: loss 0.088469\nbatch 5780: loss 0.087616\nbatch 5781: loss 0.125727\nbatch 5782: loss 0.172571\nbatch 5783: loss 0.016598\nbatch 5784: loss 0.052399\nbatch 5785: loss 0.043908\nbatch 5786: loss 0.033501\nbatch 5787: loss 0.088589\nbatch 5788: loss 0.012024\nbatch 5789: loss 0.025669\nbatch 5790: loss 0.008051\nbatch 5791: loss 0.044581\nbatch 5792: loss 0.010305\nbatch 5793: loss 0.070647\nbatch 5794: loss 0.012524\nbatch 5795: loss 0.045576\nbatch 5796: loss 0.014061\nbatch 5797: loss 0.024105\nbatch 5798: loss 0.055043\nbatch 5799: loss 0.047484\nbatch 5800: loss 0.020454\nbatch 5801: loss 0.025495\nbatch 5802: loss 0.058372\nbatch 5803: loss 0.040768\nbatch 5804: loss 0.105227\nbatch 5805: loss 0.050387\nbatch 5806: loss 0.069810\nbatch 5807: loss 0.020016\nbatch 5808: loss 0.214380\nbatch 5809: loss 0.022578\nbatch 5810: loss 0.006505\nbatch 5811: loss 0.184085\nbatch 5812: loss 0.159036\nbatch 5813: loss 0.014327\nbatch 5814: loss 0.043697\nbatch 5815: loss 0.081793\nbatch 5816: loss 0.007090\nbatch 5817: loss 0.027643\nbatch 5818: loss 0.110865\nbatch 5819: loss 0.015673\nbatch 5820: loss 0.116397\nbatch 5821: loss 0.005229\nbatch 5822: loss 0.113874\nbatch 5823: loss 0.084433\nbatch 5824: loss 0.121834\nbatch 5825: loss 0.015066\nbatch 5826: loss 0.009601\nbatch 5827: loss 0.030337\nbatch 5828: loss 0.051750\nbatch 5829: loss 0.097990\nbatch 5830: loss 0.009115\nbatch 5831: loss 0.022647\nbatch 5832: loss 0.117070\nbatch 5833: loss 0.039509\nbatch 5834: loss 0.056335\nbatch 5835: loss 0.036227\nbatch 5836: loss 0.086372\nbatch 5837: loss 0.071478\nbatch 5838: loss 0.022099\nbatch 5839: loss 0.006681\nbatch 5840: loss 0.020492\nbatch 5841: loss 0.102006\nbatch 5842: loss 0.046540\nbatch 5843: loss 0.170319\nbatch 5844: loss 0.044385\nbatch 5845: loss 0.029887\nbatch 5846: loss 0.021446\nbatch 5847: loss 0.063949\nbatch 5848: loss 0.119589\nbatch 5849: loss 0.052909\nbatch 5850: loss 0.011821\nbatch 5851: loss 0.049450\nbatch 5852: loss 0.046914\nbatch 5853: loss 0.018364\nbatch 5854: loss 0.045158\nbatch 5855: loss 0.026421\nbatch 5856: loss 0.097313\nbatch 5857: loss 0.103442\nbatch 5858: loss 0.045115\nbatch 5859: loss 0.140003\nbatch 5860: loss 0.006781\nbatch 5861: loss 0.106588\nbatch 5862: loss 0.031728\nbatch 5863: loss 0.026505\nbatch 5864: loss 0.046442\nbatch 5865: loss 0.419527\nbatch 5866: loss 0.044328\nbatch 5867: loss 0.007951\nbatch 5868: loss 0.008843\nbatch 5869: loss 0.083347\nbatch 5870: loss 0.027698\nbatch 5871: loss 0.082519\nbatch 5872: loss 0.059761\nbatch 5873: loss 0.073613\nbatch 5874: loss 0.095204\nbatch 5875: loss 0.032202\nbatch 5876: loss 0.065316\nbatch 5877: loss 0.077422\nbatch 5878: loss 0.012743\nbatch 5879: loss 0.015157\nbatch 5880: loss 0.006283\nbatch 5881: loss 0.017501\nbatch 5882: loss 0.021098\nbatch 5883: loss 0.053772\nbatch 5884: loss 0.103134\nbatch 5885: loss 0.095139\nbatch 5886: loss 0.114110\nbatch 5887: loss 0.023619\nbatch 5888: loss 0.007666\nbatch 5889: loss 0.035593\nbatch 5890: loss 0.017376\nbatch 5891: loss 0.018199\nbatch 5892: loss 0.004334\nbatch 5893: loss 0.010680\nbatch 5894: loss 0.087260\nbatch 5895: loss 0.032905\nbatch 5896: loss 0.025602\nbatch 5897: loss 0.044130\nbatch 5898: loss 0.019488\nbatch 5899: loss 0.019539\nbatch 5900: loss 0.012399\nbatch 5901: loss 0.066325\nbatch 5902: loss 0.004783\nbatch 5903: loss 0.014306\nbatch 5904: loss 0.055025\nbatch 5905: loss 0.033073\nbatch 5906: loss 0.093589\nbatch 5907: loss 0.030501\nbatch 5908: loss 0.011838\nbatch 5909: loss 0.023106\nbatch 5910: loss 0.004261\nbatch 5911: loss 0.032819\nbatch 5912: loss 0.054443\nbatch 5913: loss 0.028543\nbatch 5914: loss 0.045290\nbatch 5915: loss 0.041801\nbatch 5916: loss 0.013849\nbatch 5917: loss 0.024994\nbatch 5918: loss 0.035086\nbatch 5919: loss 0.019547\nbatch 5920: loss 0.049572\nbatch 5921: loss 0.071710\nbatch 5922: loss 0.047604\nbatch 5923: loss 0.016937\nbatch 5924: loss 0.023682\nbatch 5925: loss 0.033678\nbatch 5926: loss 0.008400\nbatch 5927: loss 0.041706\nbatch 5928: loss 0.008495\nbatch 5929: loss 0.015540\nbatch 5930: loss 0.040528\nbatch 5931: loss 0.079341\nbatch 5932: loss 0.020230\nbatch 5933: loss 0.096160\nbatch 5934: loss 0.030741\nbatch 5935: loss 0.007231\nbatch 5936: loss 0.014083\nbatch 5937: loss 0.016426\nbatch 5938: loss 0.059377\nbatch 5939: loss 0.020526\nbatch 5940: loss 0.037242\nbatch 5941: loss 0.062530\nbatch 5942: loss 0.133277\nbatch 5943: loss 0.015095\nbatch 5944: loss 0.012379\nbatch 5945: loss 0.035021\nbatch 5946: loss 0.038277\nbatch 5947: loss 0.012293\nbatch 5948: loss 0.032851\nbatch 5949: loss 0.046117\nbatch 5950: loss 0.005167\nbatch 5951: loss 0.139379\nbatch 5952: loss 0.047683\nbatch 5953: loss 0.027055\nbatch 5954: loss 0.033997\nbatch 5955: loss 0.006297\nbatch 5956: loss 0.033070\nbatch 5957: loss 0.023040\nbatch 5958: loss 0.140323\nbatch 5959: loss 0.032970\nbatch 5960: loss 0.026770\nbatch 5961: loss 0.008854\nbatch 5962: loss 0.095583\nbatch 5963: loss 0.074820\nbatch 5964: loss 0.121828\nbatch 5965: loss 0.151749\nbatch 5966: loss 0.033811\nbatch 5967: loss 0.118380\nbatch 5968: loss 0.070386\nbatch 5969: loss 0.095430\nbatch 5970: loss 0.022959\nbatch 5971: loss 0.013712\nbatch 5972: loss 0.056306\nbatch 5973: loss 0.009865\nbatch 5974: loss 0.013615\nbatch 5975: loss 0.119400\nbatch 5976: loss 0.031232\nbatch 5977: loss 0.150520\nbatch 5978: loss 0.006416\nbatch 5979: loss 0.081510\nbatch 5980: loss 0.170064\nbatch 5981: loss 0.098980\nbatch 5982: loss 0.041630\nbatch 5983: loss 0.003886\nbatch 5984: loss 0.025790\nbatch 5985: loss 0.049279\nbatch 5986: loss 0.077624\nbatch 5987: loss 0.011060\nbatch 5988: loss 0.096650\nbatch 5989: loss 0.108936\nbatch 5990: loss 0.013117\nbatch 5991: loss 0.050317\nbatch 5992: loss 0.085866\nbatch 5993: loss 0.017701\nbatch 5994: loss 0.010873\nbatch 5995: loss 0.062688\nbatch 5996: loss 0.046317\nbatch 5997: loss 0.059086\nbatch 5998: loss 0.017112\nbatch 5999: loss 0.144052\n")])]),s._v(" "),a("h1",{attrs:{id:"模型的评估：-tf-keras-metrics"}},[s._v("模型的评估： tf.keras.metrics")]),s._v(" "),a("p",[s._v("最后，我们使用测试集评估模型的性能。这里，我们使用 tf.keras.metrics 中的 SparseCategoricalAccuracy 评估器来评估模型在测试集上的性能，该评估器能够对模型预测的结果与真实结果进行比较，并输出预测正确的样本数占总样本数的比例。我们迭代测试数据集，每次通过 update_state() 方法向评估器输入两个参数： y_pred 和 y_true ，即模型预测出的结果和真实结果。评估器具有内部变量来保存当前评估指标相关的参数数值（例如当前已传入的累计样本数和当前预测正确的样本数）。迭代结束后，我们使用 result() 方法输出最终的评估指标值（预测正确的样本数占总样本数的比例）。")]),s._v(" "),a("p",[s._v("在以下代码中，我们实例化了一个 tf.keras.metrics.SparseCategoricalAccuracy 评估器，并使用 For 循环迭代分批次传入了测试集数据的预测结果与真实结果，并输出训练后的模型在测试数据集上的准确率。")]),s._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("sparse_categorical_accuracy "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" tf"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("keras"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("metrics"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("SparseCategoricalAccuracy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\nnum_batches "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("int")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("data_loader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("num_test_data "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("//")]),s._v(" batch_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("for")]),s._v(" batch_index "),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("in")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token builtin"}},[s._v("range")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("num_batches"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("\n  start_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" end_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" batch_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" batch_size"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("batch_index "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("+")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token number"}},[s._v("1")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("*")]),s._v(" batch_size\n  y_pred "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v(" model"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("predict"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("data_loader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test_data"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("start_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v(" end_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n  sparse_categorical_accuracy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("update_state"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),s._v("y_true"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("data_loader"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("test_label"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("[")]),s._v("start_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(":")]),s._v("end_index"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("]")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(",")]),s._v(" y_pred"),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("=")]),s._v("y_pred"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n"),a("span",{pre:!0,attrs:{class:"token keyword"}},[s._v("print")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token string"}},[s._v('"test accuracy: %f"')]),s._v(" "),a("span",{pre:!0,attrs:{class:"token operator"}},[s._v("%")]),s._v(" sparse_categorical_accuracy"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(".")]),s._v("result"),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v("(")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),a("span",{pre:!0,attrs:{class:"token punctuation"}},[s._v(")")]),s._v("\n")])])]),a("pre",[a("code",[s._v("test accuracy: 0.973400\n")])]),s._v(" "),a("div",{staticClass:"language-python extra-class"},[a("pre",{pre:!0,attrs:{class:"language-python"}},[a("code",[s._v("\n")])])])])}],!1,null,null,null);t.default=o.exports}}]);