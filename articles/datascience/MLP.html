<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>多层感知机（MLP） | Mark He</title>
    <meta name="description" content="">
    
    
    <link rel="preload" href="/assets/css/0.styles.9854472c.css" as="style"><link rel="preload" href="/assets/js/app.265cfc62.js" as="script"><link rel="preload" href="/assets/js/20.25d4abb7.js" as="script"><link rel="prefetch" href="/assets/js/10.aeb08d6d.js"><link rel="prefetch" href="/assets/js/11.c47163b6.js"><link rel="prefetch" href="/assets/js/12.915cb852.js"><link rel="prefetch" href="/assets/js/13.eff9359d.js"><link rel="prefetch" href="/assets/js/14.a7c6041b.js"><link rel="prefetch" href="/assets/js/15.d636104a.js"><link rel="prefetch" href="/assets/js/16.5692cdd2.js"><link rel="prefetch" href="/assets/js/17.69126d01.js"><link rel="prefetch" href="/assets/js/18.6eeb36a9.js"><link rel="prefetch" href="/assets/js/19.58fca24d.js"><link rel="prefetch" href="/assets/js/2.6ce74938.js"><link rel="prefetch" href="/assets/js/21.60fbe128.js"><link rel="prefetch" href="/assets/js/22.1c918ce7.js"><link rel="prefetch" href="/assets/js/23.67e27de0.js"><link rel="prefetch" href="/assets/js/24.22e1499d.js"><link rel="prefetch" href="/assets/js/25.02c31893.js"><link rel="prefetch" href="/assets/js/26.ec379253.js"><link rel="prefetch" href="/assets/js/27.df1dfb7c.js"><link rel="prefetch" href="/assets/js/28.e5f55e62.js"><link rel="prefetch" href="/assets/js/29.4ab53bc8.js"><link rel="prefetch" href="/assets/js/3.89d1e693.js"><link rel="prefetch" href="/assets/js/30.13a47086.js"><link rel="prefetch" href="/assets/js/31.3e08bbf0.js"><link rel="prefetch" href="/assets/js/32.49e8d2f1.js"><link rel="prefetch" href="/assets/js/33.34b0a6b7.js"><link rel="prefetch" href="/assets/js/34.6c1ac4bc.js"><link rel="prefetch" href="/assets/js/35.521abc2d.js"><link rel="prefetch" href="/assets/js/36.c1ba9b6b.js"><link rel="prefetch" href="/assets/js/37.1b798d69.js"><link rel="prefetch" href="/assets/js/38.9409643a.js"><link rel="prefetch" href="/assets/js/39.8178cfd3.js"><link rel="prefetch" href="/assets/js/4.93bfe19f.js"><link rel="prefetch" href="/assets/js/40.72bda93a.js"><link rel="prefetch" href="/assets/js/41.0a2d9067.js"><link rel="prefetch" href="/assets/js/42.1e3cfe94.js"><link rel="prefetch" href="/assets/js/43.2a8a6955.js"><link rel="prefetch" href="/assets/js/44.f0df4491.js"><link rel="prefetch" href="/assets/js/45.0ec4ef64.js"><link rel="prefetch" href="/assets/js/46.d8ab0cfb.js"><link rel="prefetch" href="/assets/js/47.101b564b.js"><link rel="prefetch" href="/assets/js/48.10290b31.js"><link rel="prefetch" href="/assets/js/49.6f0c6b8b.js"><link rel="prefetch" href="/assets/js/5.7c652252.js"><link rel="prefetch" href="/assets/js/50.48ff2154.js"><link rel="prefetch" href="/assets/js/51.8c372db5.js"><link rel="prefetch" href="/assets/js/52.1188b68c.js"><link rel="prefetch" href="/assets/js/53.a04fdb75.js"><link rel="prefetch" href="/assets/js/54.66b64b21.js"><link rel="prefetch" href="/assets/js/55.923fa6e5.js"><link rel="prefetch" href="/assets/js/56.f51a4e44.js"><link rel="prefetch" href="/assets/js/57.ee90ffbd.js"><link rel="prefetch" href="/assets/js/58.a0fb6737.js"><link rel="prefetch" href="/assets/js/59.5581418f.js"><link rel="prefetch" href="/assets/js/6.397be193.js"><link rel="prefetch" href="/assets/js/60.cba6de65.js"><link rel="prefetch" href="/assets/js/61.6046c511.js"><link rel="prefetch" href="/assets/js/62.9dce1a90.js"><link rel="prefetch" href="/assets/js/63.d8a483b0.js"><link rel="prefetch" href="/assets/js/7.f883906b.js"><link rel="prefetch" href="/assets/js/8.e9363c12.js"><link rel="prefetch" href="/assets/js/9.e10c76be.js">
    <link rel="stylesheet" href="/assets/css/0.styles.9854472c.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/" class="home-link router-link-active"><!----> <span class="site-name">Mark He</span></a> <div class="links" style="max-width:nullpx;"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/" class="nav-link">Home</a></div><div class="nav-item"><a href="/articles/datascience/" class="nav-link router-link-active">Data Science</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title">Development</span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>Full-Stack</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/articles/development/python/" class="nav-link">Python</a></li><li class="dropdown-subitem"><a href="/articles/development/java/" class="nav-link">Java</a></li><li class="dropdown-subitem"><a href="/articles/development/vue/" class="nav-link">Vue</a></li><li class="dropdown-subitem"><a href="/articles/development/Git/" class="nav-link">Git</a></li><li class="dropdown-subitem"><a href="/articles/development/tool/" class="nav-link">Tools</a></li></ul></li></ul></div></div><div class="nav-item"><a href="/articles/algorithm/" class="nav-link">Algorithm</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title">Network</span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/articles/network/R_S/" class="nav-link">Router&amp;Switch</a></li><li class="dropdown-item"><!----> <a href="/articles/network/malfunction/" class="nav-link">Malfunction</a></li></ul></div></div><div class="nav-item"><a href="/articles/speech/" class="nav-link">Speech</a></div><div class="nav-item"><a href="/articles/goals.html" class="nav-link">Goals</a></div><div class="nav-item"><a href="/articles/prepforjob.html" class="nav-link">PrepForJobs</a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <div class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/" class="nav-link">Home</a></div><div class="nav-item"><a href="/articles/datascience/" class="nav-link router-link-active">Data Science</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title">Development</span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><h4>Full-Stack</h4> <ul class="dropdown-subitem-wrapper"><li class="dropdown-subitem"><a href="/articles/development/python/" class="nav-link">Python</a></li><li class="dropdown-subitem"><a href="/articles/development/java/" class="nav-link">Java</a></li><li class="dropdown-subitem"><a href="/articles/development/vue/" class="nav-link">Vue</a></li><li class="dropdown-subitem"><a href="/articles/development/Git/" class="nav-link">Git</a></li><li class="dropdown-subitem"><a href="/articles/development/tool/" class="nav-link">Tools</a></li></ul></li></ul></div></div><div class="nav-item"><a href="/articles/algorithm/" class="nav-link">Algorithm</a></div><div class="nav-item"><div class="dropdown-wrapper"><a class="dropdown-title"><span class="title">Network</span> <span class="arrow right"></span></a> <ul class="nav-dropdown" style="display:none;"><li class="dropdown-item"><!----> <a href="/articles/network/R_S/" class="nav-link">Router&amp;Switch</a></li><li class="dropdown-item"><!----> <a href="/articles/network/malfunction/" class="nav-link">Malfunction</a></li></ul></div></div><div class="nav-item"><a href="/articles/speech/" class="nav-link">Speech</a></div><div class="nav-item"><a href="/articles/goals.html" class="nav-link">Goals</a></div><div class="nav-item"><a href="/articles/prepforjob.html" class="nav-link">PrepForJobs</a></div> <!----></nav>  <ul class="sidebar-links"><li><div class="sidebar-group first collapsable"><p class="sidebar-heading"><span>Data Science</span> <span class="arrow right"></span></p> <!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading open"><span>Deep Learning</span> <span class="arrow down"></span></p> <ul class="sidebar-group-items"><li><a href="/articles/datascience/Deeplearning.html" class="sidebar-link">Deep Learning</a></li><li><a href="/articles/datascience/MLP.html" class="active sidebar-link">多层感知机（MLP）</a></li><li><a href="/articles/datascience/NLP.html" class="sidebar-link">NLP</a></li></ul></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>Machine Learning</span> <span class="arrow right"></span></p> <!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>Big-Data</span> <span class="arrow right"></span></p> <!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>SQL</span> <span class="arrow right"></span></p> <!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>Data Warehouse</span> <span class="arrow right"></span></p> <!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>R</span> <span class="arrow right"></span></p> <!----></div></li><li><div class="sidebar-group collapsable"><p class="sidebar-heading"><span>Resource</span> <span class="arrow right"></span></p> <!----></div></li></ul> </div> <div class="page"> <div class="content"><h1 id="多层感知机（mlp）">多层感知机（MLP）</h1> <ul><li>使用 tf.keras.datasets 获得数据集并预处理</li> <li>使用 tf.keras.Model 和 tf.keras.layers 构建模型</li> <li>构建模型训练流程，使用 tf.keras.losses 计算损失函数，并使用 tf.keras.optimizer 优化模型</li> <li>构建模型评估流程，使用 tf.keras.metrics 计算评估指标</li></ul> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">import</span> tensorflow <span class="token keyword">as</span> tf
<span class="token keyword">import</span> numpy <span class="token keyword">as</span> np
<span class="token keyword">print</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span>
</code></pre></div><pre><code>2.0.0
</code></pre> <div class="language-python extra-class"><pre class="language-python"><code>!pip uninstall tensorflow
</code></pre></div><pre><code>Uninstalling tensorflow-1.15.0rc3:
  Would remove:
    /usr/local/bin/estimator_ckpt_converter
    /usr/local/bin/freeze_graph
    /usr/local/bin/saved_model_cli
    /usr/local/bin/tensorboard
    /usr/local/bin/tf_upgrade_v2
    /usr/local/bin/tflite_convert
    /usr/local/bin/toco
    /usr/local/bin/toco_from_protos
    /usr/local/lib/python3.6/dist-packages/tensorflow-1.15.0rc3.dist-info/*
    /usr/local/lib/python3.6/dist-packages/tensorflow/*
    /usr/local/lib/python3.6/dist-packages/tensorflow_core/*
Proceed (y/n)? y
  Successfully uninstalled tensorflow-1.15.0rc3
</code></pre> <div class="language-python extra-class"><pre class="language-python"><code>!pip install tensorflow<span class="token operator">==</span><span class="token number">2.0</span><span class="token number">.0</span>
</code></pre></div><pre><code>Collecting tensorflow==2.0.0
[?25l  Downloading https://files.pythonhosted.org/packages/46/0f/7bd55361168bb32796b360ad15a25de6966c9c1beb58a8e30c01c8279862/tensorflow-2.0.0-cp36-cp36m-manylinux2010_x86_64.whl (86.3MB)
[K     |████████████████████████████████| 86.3MB 386kB/s 
[?25hRequirement already satisfied: astor&gt;=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.0)
Requirement already satisfied: wrapt&gt;=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.11.2)
Requirement already satisfied: wheel&gt;=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.33.6)
Requirement already satisfied: keras-preprocessing&gt;=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)
Requirement already satisfied: keras-applications&gt;=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.0.8)
Collecting tensorflow-estimator&lt;2.1.0,&gt;=2.0.0 (from tensorflow==2.0.0)
[?25l  Downloading https://files.pythonhosted.org/packages/95/00/5e6cdf86190a70d7382d320b2b04e4ff0f8191a37d90a422a2f8ff0705bb/tensorflow_estimator-2.0.0-py2.py3-none-any.whl (449kB)
[K     |████████████████████████████████| 450kB 34.9MB/s 
[?25hCollecting tensorboard&lt;2.1.0,&gt;=2.0.0 (from tensorflow==2.0.0)
[?25l  Downloading https://files.pythonhosted.org/packages/9b/a6/e8ffa4e2ddb216449d34cfcb825ebb38206bee5c4553d69e7bc8bc2c5d64/tensorboard-2.0.0-py3-none-any.whl (3.8MB)
[K     |████████████████████████████████| 3.8MB 29.5MB/s 
[?25hRequirement already satisfied: numpy&lt;2.0,&gt;=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.16.5)
Requirement already satisfied: google-pasta&gt;=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.1.7)
Requirement already satisfied: opt-einsum&gt;=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.1.0)
Requirement already satisfied: protobuf&gt;=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (3.7.1)
Requirement already satisfied: termcolor&gt;=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.1.0)
Requirement already satisfied: grpcio&gt;=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.15.0)
Requirement already satisfied: six&gt;=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (1.12.0)
Requirement already satisfied: absl-py&gt;=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.8.0)
Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0) (0.2.2)
Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications&gt;=1.0.8-&gt;tensorflow==2.0.0) (2.8.0)
Requirement already satisfied: markdown&gt;=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow==2.0.0) (3.1.1)
Requirement already satisfied: setuptools&gt;=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow==2.0.0) (41.2.0)
Requirement already satisfied: werkzeug&gt;=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard&lt;2.1.0,&gt;=2.0.0-&gt;tensorflow==2.0.0) (0.16.0)
Installing collected packages: tensorflow-estimator, tensorboard, tensorflow
  Found existing installation: tensorflow-estimator 1.15.1
    Uninstalling tensorflow-estimator-1.15.1:
      Successfully uninstalled tensorflow-estimator-1.15.1
  Found existing installation: tensorboard 1.15.0
    Uninstalling tensorboard-1.15.0:
      Successfully uninstalled tensorboard-1.15.0
Successfully installed tensorboard-2.0.0 tensorflow-2.0.0 tensorflow-estimator-2.0.0
</code></pre> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">print</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>__version__<span class="token punctuation">)</span>
</code></pre></div><pre><code>2.0.0
</code></pre> <div class="language-python extra-class"><pre class="language-python"><code>tf<span class="token punctuation">.</span>executing_eagerly<span class="token punctuation">(</span><span class="token punctuation">)</span> 
<span class="token comment">#tf.enable_eager_execution()</span>
</code></pre></div><pre><code>True
</code></pre> <h1 id="数据获取及预处理：-tf-keras-datasets">数据获取及预处理： tf.keras.datasets</h1> <p>先进行预备工作，实现一个简单的 MNISTLoader 类来读取 MNIST 数据集数据。这里使用了 tf.keras.datasets 快速载入 MNIST 数据集。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">MNISTLoader</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    mnist <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>datasets<span class="token punctuation">.</span>mnist
    <span class="token punctuation">(</span>self<span class="token punctuation">.</span>train_data<span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_label<span class="token punctuation">)</span><span class="token punctuation">,</span> <span class="token punctuation">(</span>self<span class="token punctuation">.</span>test_data<span class="token punctuation">,</span> self<span class="token punctuation">.</span>test_label<span class="token punctuation">)</span> <span class="token operator">=</span> mnist<span class="token punctuation">.</span>load_data<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>train_data <span class="token operator">=</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>self<span class="token punctuation">.</span>train_data<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>test_data <span class="token operator">=</span> np<span class="token punctuation">.</span>expand_dims<span class="token punctuation">(</span>self<span class="token punctuation">.</span>test_data<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>float32<span class="token punctuation">)</span> <span class="token operator">/</span> <span class="token number">255.0</span><span class="token punctuation">,</span> axis<span class="token operator">=</span><span class="token operator">-</span><span class="token number">1</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>train_label <span class="token operator">=</span> self<span class="token punctuation">.</span>train_label<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>    <span class="token comment"># [60000]</span>
    self<span class="token punctuation">.</span>test_label <span class="token operator">=</span> self<span class="token punctuation">.</span>test_label<span class="token punctuation">.</span>astype<span class="token punctuation">(</span>np<span class="token punctuation">.</span>int32<span class="token punctuation">)</span>      <span class="token comment"># [10000]</span>
    self<span class="token punctuation">.</span>num_train_data<span class="token punctuation">,</span> self<span class="token punctuation">.</span>num_test_data <span class="token operator">=</span> self<span class="token punctuation">.</span>train_data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>test_data<span class="token punctuation">.</span>shape<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span>
  
  <span class="token keyword">def</span> <span class="token function">get_batch</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span><span class="token punctuation">:</span>
    index <span class="token operator">=</span> np<span class="token punctuation">.</span>random<span class="token punctuation">.</span>randint<span class="token punctuation">(</span><span class="token number">0</span><span class="token punctuation">,</span> np<span class="token punctuation">.</span>shape<span class="token punctuation">(</span>self<span class="token punctuation">.</span>train_data<span class="token punctuation">)</span><span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> batch_size<span class="token punctuation">)</span>
    <span class="token keyword">return</span> self<span class="token punctuation">.</span>train_data<span class="token punctuation">[</span>index<span class="token punctuation">,</span> <span class="token punctuation">:</span><span class="token punctuation">]</span><span class="token punctuation">,</span> self<span class="token punctuation">.</span>train_label<span class="token punctuation">[</span>index<span class="token punctuation">]</span>
    
</code></pre></div><h1 id="模型的构建：-tf-keras-model-和-tf-keras-layers">模型的构建： tf.keras.Model 和 tf.keras.layers</h1> <p>多层感知机的模型类实现与上面的线性模型类似，使用 tf.keras.Model 和 tf.keras.layers 构建，所不同的地方在于层数增加了（顾名思义，“多层” 感知机），以及引入了非线性激活函数（这里使用了 ReLU 函数 ， 即下方的 activation=tf.nn.relu ）。该模型输入一个向量（比如这里是拉直的 1×784 手写体数字图片），输出 10 维的向量，分别代表这张图片属于 0 到 9 的概率。</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">MLP</span><span class="token punctuation">(</span>tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>Model<span class="token punctuation">)</span><span class="token punctuation">:</span>
  <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token builtin">super</span><span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">.</span>__init__<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>flatten <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Flatten<span class="token punctuation">(</span><span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>dense1 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>units<span class="token operator">=</span> <span class="token number">100</span><span class="token punctuation">,</span> activation<span class="token operator">=</span>tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>relu<span class="token punctuation">)</span>
    self<span class="token punctuation">.</span>dense2 <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>layers<span class="token punctuation">.</span>Dense<span class="token punctuation">(</span>units <span class="token operator">=</span> <span class="token number">10</span><span class="token punctuation">)</span>
    
  <span class="token keyword">def</span> <span class="token function">call</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> inputs<span class="token punctuation">)</span><span class="token punctuation">:</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>flatten<span class="token punctuation">(</span>inputs<span class="token punctuation">)</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>dense1<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    x <span class="token operator">=</span> self<span class="token punctuation">.</span>dense2<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    output <span class="token operator">=</span> tf<span class="token punctuation">.</span>nn<span class="token punctuation">.</span>softmax<span class="token punctuation">(</span>x<span class="token punctuation">)</span>
    <span class="token keyword">return</span> output
</code></pre></div><h1 id="模型的训练：-tf-keras-losses-和-tf-keras-optimizer">模型的训练： tf.keras.losses 和 tf.keras.optimizer</h1> <div class="language-python extra-class"><pre class="language-python"><code>num_epochs <span class="token operator">=</span> <span class="token number">5</span>
batch_size <span class="token operator">=</span> <span class="token number">50</span>
learning_rate <span class="token operator">=</span> <span class="token number">0.001</span>
</code></pre></div><div class="language-python extra-class"><pre class="language-python"><code>model <span class="token operator">=</span> MLP<span class="token punctuation">(</span><span class="token punctuation">)</span>
data_loader <span class="token operator">=</span> MNISTLoader<span class="token punctuation">(</span><span class="token punctuation">)</span>
optimizer <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>optimizers<span class="token punctuation">.</span>Adam<span class="token punctuation">(</span>learning_rate<span class="token operator">=</span>learning_rate<span class="token punctuation">)</span>
</code></pre></div><p>然后迭代进行以下步骤：</p> <p>从 DataLoader 中随机取一批训练数据；</p> <p>将这批数据送入模型，计算出模型的预测值；</p> <p>将模型预测值与真实值进行比较，计算损失函数（loss）。这里使用 tf.keras.losses 中的交叉熵函数作为损失函数；</p> <p>计算损失函数关于模型变量的导数；</p> <p>将求出的导数值传入优化器，使用优化器的 apply_gradients 方法更新模型参数以最小化损失函数</p> <div class="language-python extra-class"><pre class="language-python"><code>num_batches <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">.</span>num_train_data <span class="token operator">//</span> batch_size <span class="token operator">*</span> num_epochs<span class="token punctuation">)</span>
<span class="token keyword">for</span> batch_index <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_batches<span class="token punctuation">)</span><span class="token punctuation">:</span>
  X<span class="token punctuation">,</span> y <span class="token operator">=</span> data_loader<span class="token punctuation">.</span>get_batch<span class="token punctuation">(</span>batch_size<span class="token punctuation">)</span>
  <span class="token keyword">with</span> tf<span class="token punctuation">.</span>GradientTape<span class="token punctuation">(</span><span class="token punctuation">)</span> <span class="token keyword">as</span> tape<span class="token punctuation">:</span>
    y_pred <span class="token operator">=</span> model<span class="token punctuation">(</span>X<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>losses<span class="token punctuation">.</span>sparse_categorical_crossentropy<span class="token punctuation">(</span>y_true<span class="token operator">=</span>y<span class="token punctuation">,</span> y_pred<span class="token operator">=</span>y_pred<span class="token punctuation">)</span>
    loss <span class="token operator">=</span> tf<span class="token punctuation">.</span>reduce_mean<span class="token punctuation">(</span>loss<span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;batch %d: loss %f&quot;</span> <span class="token operator">%</span> <span class="token punctuation">(</span>batch_index<span class="token punctuation">,</span> loss<span class="token punctuation">.</span>numpy<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
  grads <span class="token operator">=</span> tape<span class="token punctuation">.</span>gradient<span class="token punctuation">(</span>loss<span class="token punctuation">,</span> model<span class="token punctuation">.</span>variables<span class="token punctuation">)</span>
  optimizer<span class="token punctuation">.</span>apply_gradients<span class="token punctuation">(</span>grads_and_vars<span class="token operator">=</span><span class="token builtin">zip</span><span class="token punctuation">(</span>grads<span class="token punctuation">,</span> model<span class="token punctuation">.</span>variables<span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><pre><code>batch 0: loss 2.285256
batch 1: loss 2.205372
batch 2: loss 2.334929
batch 3: loss 2.056121
batch 4: loss 2.081305
batch 5: loss 1.941437
batch 6: loss 2.061041
batch 7: loss 1.972376
batch 8: loss 1.754983
batch 9: loss 1.749182
batch 10: loss 1.632017
batch 11: loss 1.592685
batch 12: loss 1.543856
batch 13: loss 1.629244
batch 14: loss 1.514880
batch 15: loss 1.428778
batch 16: loss 1.384761
batch 17: loss 1.380737
batch 18: loss 1.174533
batch 19: loss 1.367718
batch 20: loss 1.302076
batch 21: loss 1.189340
batch 22: loss 1.042583
batch 23: loss 1.025285
batch 24: loss 1.220943
batch 25: loss 1.032526
batch 26: loss 0.888587
batch 27: loss 1.059231
batch 28: loss 1.008494
batch 29: loss 0.836554
batch 30: loss 0.864243
batch 31: loss 0.854915
batch 32: loss 0.907002
batch 33: loss 1.022292
batch 34: loss 0.848349
batch 35: loss 0.761802
batch 36: loss 0.874089
batch 37: loss 0.739989
batch 38: loss 0.718120
batch 39: loss 0.734513
batch 40: loss 0.776238
batch 41: loss 0.595228
batch 42: loss 0.819337
batch 43: loss 0.679247
batch 44: loss 0.594760
batch 45: loss 0.811826
batch 46: loss 0.661954
batch 47: loss 0.793975
batch 48: loss 0.486048
batch 49: loss 0.591936
batch 50: loss 0.585743
batch 51: loss 0.614658
batch 52: loss 0.406398
batch 53: loss 0.634467
batch 54: loss 0.614022
batch 55: loss 0.672396
batch 56: loss 0.476113
batch 57: loss 0.537915
batch 58: loss 0.513415
batch 59: loss 0.550884
batch 60: loss 0.583696
batch 61: loss 0.651275
batch 62: loss 0.506319
batch 63: loss 0.502315
batch 64: loss 0.688968
batch 65: loss 0.448250
batch 66: loss 0.605824
batch 67: loss 0.489055
batch 68: loss 0.405522
batch 69: loss 0.525403
batch 70: loss 0.635453
batch 71: loss 0.301678
batch 72: loss 0.635552
batch 73: loss 0.568243
batch 74: loss 0.697631
batch 75: loss 0.460927
batch 76: loss 0.476688
batch 77: loss 0.357887
batch 78: loss 0.431935
batch 79: loss 0.565880
batch 80: loss 0.566409
batch 81: loss 0.468039
batch 82: loss 0.622586
batch 83: loss 0.487366
batch 84: loss 0.498076
batch 85: loss 0.385445
batch 86: loss 0.478501
batch 87: loss 0.558496
batch 88: loss 0.526953
batch 89: loss 0.547417
batch 90: loss 0.547640
batch 91: loss 0.557868
batch 92: loss 0.358667
batch 93: loss 0.409599
batch 94: loss 0.561230
batch 95: loss 0.668404
batch 96: loss 0.559714
batch 97: loss 0.524012
batch 98: loss 0.474580
batch 99: loss 0.399339
batch 100: loss 0.475849
batch 101: loss 0.324837
batch 102: loss 0.389984
batch 103: loss 0.473782
batch 104: loss 0.419292
batch 105: loss 0.416162
batch 106: loss 0.509604
batch 107: loss 0.346391
batch 108: loss 0.352209
batch 109: loss 0.488571
batch 110: loss 0.419189
batch 111: loss 0.437918
batch 112: loss 0.350439
batch 113: loss 0.315364
batch 114: loss 0.351902
batch 115: loss 0.549310
batch 116: loss 0.420044
batch 117: loss 0.357893
batch 118: loss 0.327098
batch 119: loss 0.293401
batch 120: loss 0.291696
batch 121: loss 0.294370
batch 122: loss 0.379132
batch 123: loss 0.437771
batch 124: loss 0.364326
batch 125: loss 0.363065
batch 126: loss 0.515139
batch 127: loss 0.221047
batch 128: loss 0.207894
batch 129: loss 0.304511
batch 130: loss 0.569853
batch 131: loss 0.434927
batch 132: loss 0.402055
batch 133: loss 0.551857
batch 134: loss 0.451924
batch 135: loss 0.390041
batch 136: loss 0.205925
batch 137: loss 0.300728
batch 138: loss 0.279311
batch 139: loss 0.254808
batch 140: loss 0.406166
batch 141: loss 0.510225
batch 142: loss 0.336390
batch 143: loss 0.286932
batch 144: loss 0.334305
batch 145: loss 0.505116
batch 146: loss 0.296393
batch 147: loss 0.498919
batch 148: loss 0.507318
batch 149: loss 0.261467
batch 150: loss 0.298310
batch 151: loss 0.411035
batch 152: loss 0.361716
batch 153: loss 0.277789
batch 154: loss 0.288142
batch 155: loss 0.430213
batch 156: loss 0.242033
batch 157: loss 0.438750
batch 158: loss 0.418472
batch 159: loss 0.703795
batch 160: loss 0.331291
batch 161: loss 0.392003
batch 162: loss 0.503135
batch 163: loss 0.416751
batch 164: loss 0.649559
batch 165: loss 0.284626
batch 166: loss 0.253399
batch 167: loss 0.320030
batch 168: loss 0.521847
batch 169: loss 0.307325
batch 170: loss 0.274674
batch 171: loss 0.358530
batch 172: loss 0.518101
batch 173: loss 0.390567
batch 174: loss 0.174029
batch 175: loss 0.539849
batch 176: loss 0.428587
batch 177: loss 0.329828
batch 178: loss 0.812834
batch 179: loss 0.396530
batch 180: loss 0.224222
batch 181: loss 0.258882
batch 182: loss 0.207767
batch 183: loss 0.155181
batch 184: loss 0.377799
batch 185: loss 0.284999
batch 186: loss 0.323863
batch 187: loss 0.338120
batch 188: loss 0.188672
batch 189: loss 0.327143
batch 190: loss 0.504763
batch 191: loss 0.337281
batch 192: loss 0.262181
batch 193: loss 0.223661
batch 194: loss 0.283222
batch 195: loss 0.190049
batch 196: loss 0.274772
batch 197: loss 0.282986
batch 198: loss 0.372376
batch 199: loss 0.440532
batch 200: loss 0.427018
batch 201: loss 0.316213
batch 202: loss 0.460297
batch 203: loss 0.486977
batch 204: loss 0.261368
batch 205: loss 0.321267
batch 206: loss 0.350332
batch 207: loss 0.225021
batch 208: loss 0.342973
batch 209: loss 0.244674
batch 210: loss 0.441389
batch 211: loss 0.378784
batch 212: loss 0.413090
batch 213: loss 0.252654
batch 214: loss 0.414331
batch 215: loss 0.536494
batch 216: loss 0.192939
batch 217: loss 0.213020
batch 218: loss 0.297483
batch 219: loss 0.343482
batch 220: loss 0.412594
batch 221: loss 0.324474
batch 222: loss 0.318865
batch 223: loss 0.234760
batch 224: loss 0.272884
batch 225: loss 0.242461
batch 226: loss 0.545350
batch 227: loss 0.312405
batch 228: loss 0.391701
batch 229: loss 0.372519
batch 230: loss 0.472101
batch 231: loss 0.377553
batch 232: loss 0.216674
batch 233: loss 0.271941
batch 234: loss 0.558961
batch 235: loss 0.170775
batch 236: loss 0.262034
batch 237: loss 0.225955
batch 238: loss 0.212476
batch 239: loss 0.260501
batch 240: loss 0.214531
batch 241: loss 0.323311
batch 242: loss 0.478120
batch 243: loss 0.236381
batch 244: loss 0.410681
batch 245: loss 0.263916
batch 246: loss 0.323504
batch 247: loss 0.399251
batch 248: loss 0.294628
batch 249: loss 0.326098
batch 250: loss 0.372162
batch 251: loss 0.495363
batch 252: loss 0.221127
batch 253: loss 0.576529
batch 254: loss 0.397664
batch 255: loss 0.369326
batch 256: loss 0.210746
batch 257: loss 0.264947
batch 258: loss 0.207313
batch 259: loss 0.211112
batch 260: loss 0.255969
batch 261: loss 0.215782
batch 262: loss 0.190437
batch 263: loss 0.222935
batch 264: loss 0.397785
batch 265: loss 0.227694
batch 266: loss 0.467060
batch 267: loss 0.419647
batch 268: loss 0.357369
batch 269: loss 0.204077
batch 270: loss 0.456494
batch 271: loss 0.174073
batch 272: loss 0.438463
batch 273: loss 0.257310
batch 274: loss 0.358364
batch 275: loss 0.197801
batch 276: loss 0.552193
batch 277: loss 0.302756
batch 278: loss 0.383897
batch 279: loss 0.243372
batch 280: loss 0.619185
batch 281: loss 0.313286
batch 282: loss 0.353051
batch 283: loss 0.262350
batch 284: loss 0.167927
batch 285: loss 0.241061
batch 286: loss 0.197949
batch 287: loss 0.249585
batch 288: loss 0.337045
batch 289: loss 0.382001
batch 290: loss 0.555709
batch 291: loss 0.468115
batch 292: loss 0.413247
batch 293: loss 0.389037
batch 294: loss 0.517556
batch 295: loss 0.302482
batch 296: loss 0.189868
batch 297: loss 0.396580
batch 298: loss 0.217572
batch 299: loss 0.280075
batch 300: loss 0.411861
batch 301: loss 0.194347
batch 302: loss 0.157477
batch 303: loss 0.200695
batch 304: loss 0.212443
batch 305: loss 0.240730
batch 306: loss 0.295195
batch 307: loss 0.406676
batch 308: loss 0.212255
batch 309: loss 0.360117
batch 310: loss 0.228627
batch 311: loss 0.207296
batch 312: loss 0.142424
batch 313: loss 0.268343
batch 314: loss 0.237308
batch 315: loss 0.289460
batch 316: loss 0.303796
batch 317: loss 0.220824
batch 318: loss 0.213275
batch 319: loss 0.246066
batch 320: loss 0.335618
batch 321: loss 0.291521
batch 322: loss 0.334310
batch 323: loss 0.446926
batch 324: loss 0.171458
batch 325: loss 0.422111
batch 326: loss 0.156345
batch 327: loss 0.284695
batch 328: loss 0.267974
batch 329: loss 0.577840
batch 330: loss 0.387926
batch 331: loss 0.268040
batch 332: loss 0.353480
batch 333: loss 0.298518
batch 334: loss 0.478521
batch 335: loss 0.365600
batch 336: loss 0.185804
batch 337: loss 0.223569
batch 338: loss 0.256348
batch 339: loss 0.321289
batch 340: loss 0.271853
batch 341: loss 0.176548
batch 342: loss 0.229647
batch 343: loss 0.111074
batch 344: loss 0.463467
batch 345: loss 0.239988
batch 346: loss 0.314291
batch 347: loss 0.207193
batch 348: loss 0.171432
batch 349: loss 0.364189
batch 350: loss 0.335221
batch 351: loss 0.413151
batch 352: loss 0.362950
batch 353: loss 0.319661
batch 354: loss 0.162014
batch 355: loss 0.389689
batch 356: loss 0.421089
batch 357: loss 0.181052
batch 358: loss 0.177759
batch 359: loss 0.402913
batch 360: loss 0.235380
batch 361: loss 0.267268
batch 362: loss 0.202916
batch 363: loss 0.201494
batch 364: loss 0.150469
batch 365: loss 0.364969
batch 366: loss 0.412862
batch 367: loss 0.321997
batch 368: loss 0.132545
batch 369: loss 0.159412
batch 370: loss 0.342442
batch 371: loss 0.259415
batch 372: loss 0.070327
batch 373: loss 0.316670
batch 374: loss 0.199430
batch 375: loss 0.291386
batch 376: loss 0.313575
batch 377: loss 0.081056
batch 378: loss 0.325014
batch 379: loss 0.404023
batch 380: loss 0.389470
batch 381: loss 0.253171
batch 382: loss 0.223153
batch 383: loss 0.229689
batch 384: loss 0.350770
batch 385: loss 0.254723
batch 386: loss 0.179526
batch 387: loss 0.473149
batch 388: loss 0.440840
batch 389: loss 0.237252
batch 390: loss 0.197012
batch 391: loss 0.332625
batch 392: loss 0.455114
batch 393: loss 0.250911
batch 394: loss 0.479606
batch 395: loss 0.355097
batch 396: loss 0.588226
batch 397: loss 0.258520
batch 398: loss 0.306099
batch 399: loss 0.147707
batch 400: loss 0.169863
batch 401: loss 0.170929
batch 402: loss 0.267700
batch 403: loss 0.330947
batch 404: loss 0.266987
batch 405: loss 0.192440
batch 406: loss 0.343603
batch 407: loss 0.200144
batch 408: loss 0.171768
batch 409: loss 0.284261
batch 410: loss 0.201596
batch 411: loss 0.118803
batch 412: loss 0.349311
batch 413: loss 0.322410
batch 414: loss 0.286590
batch 415: loss 0.218352
batch 416: loss 0.159007
batch 417: loss 0.303618
batch 418: loss 0.138437
batch 419: loss 0.332913
batch 420: loss 0.210851
batch 421: loss 0.306345
batch 422: loss 0.285776
batch 423: loss 0.507604
batch 424: loss 0.403712
batch 425: loss 0.226156
batch 426: loss 0.246216
batch 427: loss 0.197353
batch 428: loss 0.213131
batch 429: loss 0.298204
batch 430: loss 0.271106
batch 431: loss 0.409738
batch 432: loss 0.156217
batch 433: loss 0.185648
batch 434: loss 0.357938
batch 435: loss 0.126928
batch 436: loss 0.335029
batch 437: loss 0.342921
batch 438: loss 0.197034
batch 439: loss 0.268930
batch 440: loss 0.276491
batch 441: loss 0.274351
batch 442: loss 0.237839
batch 443: loss 0.161867
batch 444: loss 0.431481
batch 445: loss 0.318385
batch 446: loss 0.096307
batch 447: loss 0.375171
batch 448: loss 0.397831
batch 449: loss 0.445601
batch 450: loss 0.227258
batch 451: loss 0.400308
batch 452: loss 0.251131
batch 453: loss 0.328621
batch 454: loss 0.340671
batch 455: loss 0.299543
batch 456: loss 0.247674
batch 457: loss 0.219753
batch 458: loss 0.432150
batch 459: loss 0.101117
batch 460: loss 0.222552
batch 461: loss 0.246256
batch 462: loss 0.137580
batch 463: loss 0.150216
batch 464: loss 0.266098
batch 465: loss 0.152606
batch 466: loss 0.172253
batch 467: loss 0.276848
batch 468: loss 0.222936
batch 469: loss 0.447961
batch 470: loss 0.186578
batch 471: loss 0.330791
batch 472: loss 0.496173
batch 473: loss 0.355145
batch 474: loss 0.210110
batch 475: loss 0.254539
batch 476: loss 0.133599
batch 477: loss 0.174432
batch 478: loss 0.198371
batch 479: loss 0.148955
batch 480: loss 0.190068
batch 481: loss 0.226166
batch 482: loss 0.304002
batch 483: loss 0.209559
batch 484: loss 0.388254
batch 485: loss 0.199749
batch 486: loss 0.310077
batch 487: loss 0.692423
batch 488: loss 0.363514
batch 489: loss 0.300715
batch 490: loss 0.230611
batch 491: loss 0.163196
batch 492: loss 0.095125
batch 493: loss 0.175935
batch 494: loss 0.282566
batch 495: loss 0.086311
batch 496: loss 0.154789
batch 497: loss 0.149378
batch 498: loss 0.375296
batch 499: loss 0.332906
batch 500: loss 0.098054
batch 501: loss 0.324982
batch 502: loss 0.156018
batch 503: loss 0.149343
batch 504: loss 0.221093
batch 505: loss 0.135403
batch 506: loss 0.370418
batch 507: loss 0.229524
batch 508: loss 0.159202
batch 509: loss 0.383838
batch 510: loss 0.226361
batch 511: loss 0.127674
batch 512: loss 0.257506
batch 513: loss 0.323373
batch 514: loss 0.329182
batch 515: loss 0.244790
batch 516: loss 0.139769
batch 517: loss 0.099394
batch 518: loss 0.329889
batch 519: loss 0.328298
batch 520: loss 0.435600
batch 521: loss 0.163262
batch 522: loss 0.233294
batch 523: loss 0.117601
batch 524: loss 0.150298
batch 525: loss 0.399950
batch 526: loss 0.220615
batch 527: loss 0.294963
batch 528: loss 0.278380
batch 529: loss 0.134269
batch 530: loss 0.119274
batch 531: loss 0.403507
batch 532: loss 0.176568
batch 533: loss 0.440408
batch 534: loss 0.270867
batch 535: loss 0.248501
batch 536: loss 0.642812
batch 537: loss 0.211954
batch 538: loss 0.369788
batch 539: loss 0.124490
batch 540: loss 0.302076
batch 541: loss 0.232399
batch 542: loss 0.228947
batch 543: loss 0.444461
batch 544: loss 0.529427
batch 545: loss 0.186108
batch 546: loss 0.239489
batch 547: loss 0.244953
batch 548: loss 0.181797
batch 549: loss 0.188976
batch 550: loss 0.189450
batch 551: loss 0.163070
batch 552: loss 0.245811
batch 553: loss 0.151188
batch 554: loss 0.335199
batch 555: loss 0.263940
batch 556: loss 0.271129
batch 557: loss 0.202406
batch 558: loss 0.419538
batch 559: loss 0.278002
batch 560: loss 0.188402
batch 561: loss 0.143863
batch 562: loss 0.451175
batch 563: loss 0.153855
batch 564: loss 0.254996
batch 565: loss 0.421723
batch 566: loss 0.200114
batch 567: loss 0.204463
batch 568: loss 0.330874
batch 569: loss 0.226188
batch 570: loss 0.181282
batch 571: loss 0.239751
batch 572: loss 0.305427
batch 573: loss 0.108541
batch 574: loss 0.269573
batch 575: loss 0.322800
batch 576: loss 0.180970
batch 577: loss 0.107506
batch 578: loss 0.293277
batch 579: loss 0.129323
batch 580: loss 0.346278
batch 581: loss 0.216237
batch 582: loss 0.308874
batch 583: loss 0.138837
batch 584: loss 0.156226
batch 585: loss 0.242788
batch 586: loss 0.357992
batch 587: loss 0.254993
batch 588: loss 0.142026
batch 589: loss 0.333713
batch 590: loss 0.448854
batch 591: loss 0.238323
batch 592: loss 0.249784
batch 593: loss 0.124006
batch 594: loss 0.083847
batch 595: loss 0.068844
batch 596: loss 0.152750
batch 597: loss 0.181854
batch 598: loss 0.293189
batch 599: loss 0.130568
batch 600: loss 0.228575
batch 601: loss 0.114651
batch 602: loss 0.274828
batch 603: loss 0.214390
batch 604: loss 0.287496
batch 605: loss 0.240209
batch 606: loss 0.157249
batch 607: loss 0.133174
batch 608: loss 0.295633
batch 609: loss 0.101599
batch 610: loss 0.327761
batch 611: loss 0.153859
batch 612: loss 0.222140
batch 613: loss 0.190587
batch 614: loss 0.119984
batch 615: loss 0.160714
batch 616: loss 0.120944
batch 617: loss 0.287635
batch 618: loss 0.061606
batch 619: loss 0.159002
batch 620: loss 0.102159
batch 621: loss 0.212164
batch 622: loss 0.123733
batch 623: loss 0.220980
batch 624: loss 0.170258
batch 625: loss 0.386752
batch 626: loss 0.202990
batch 627: loss 0.221710
batch 628: loss 0.218060
batch 629: loss 0.285306
batch 630: loss 0.184506
batch 631: loss 0.107576
batch 632: loss 0.135285
batch 633: loss 0.259310
batch 634: loss 0.221828
batch 635: loss 0.402952
batch 636: loss 0.186237
batch 637: loss 0.145068
batch 638: loss 0.329077
batch 639: loss 0.240307
batch 640: loss 0.377551
batch 641: loss 0.494494
batch 642: loss 0.135959
batch 643: loss 0.297693
batch 644: loss 0.332268
batch 645: loss 0.124169
batch 646: loss 0.095379
batch 647: loss 0.201203
batch 648: loss 0.266056
batch 649: loss 0.288723
batch 650: loss 0.226291
batch 651: loss 0.434054
batch 652: loss 0.325442
batch 653: loss 0.213596
batch 654: loss 0.244598
batch 655: loss 0.252400
batch 656: loss 0.190792
batch 657: loss 0.255896
batch 658: loss 0.163001
batch 659: loss 0.183375
batch 660: loss 0.160560
batch 661: loss 0.234111
batch 662: loss 0.119940
batch 663: loss 0.209808
batch 664: loss 0.136872
batch 665: loss 0.176688
batch 666: loss 0.113685
batch 667: loss 0.347155
batch 668: loss 0.158519
batch 669: loss 0.397002
batch 670: loss 0.179305
batch 671: loss 0.190632
batch 672: loss 0.235130
batch 673: loss 0.181187
batch 674: loss 0.253867
batch 675: loss 0.129148
batch 676: loss 0.303930
batch 677: loss 0.133495
batch 678: loss 0.090633
batch 679: loss 0.152214
batch 680: loss 0.237603
batch 681: loss 0.227588
batch 682: loss 0.254605
batch 683: loss 0.186847
batch 684: loss 0.147803
batch 685: loss 0.120886
batch 686: loss 0.164892
batch 687: loss 0.331868
batch 688: loss 0.414535
batch 689: loss 0.312438
batch 690: loss 0.170866
batch 691: loss 0.504608
batch 692: loss 0.377827
batch 693: loss 0.158408
batch 694: loss 0.209521
batch 695: loss 0.057885
batch 696: loss 0.098620
batch 697: loss 0.185616
batch 698: loss 0.137314
batch 699: loss 0.119743
batch 700: loss 0.234409
batch 701: loss 0.239075
batch 702: loss 0.151472
batch 703: loss 0.165083
batch 704: loss 0.269955
batch 705: loss 0.226257
batch 706: loss 0.239466
batch 707: loss 0.199373
batch 708: loss 0.325505
batch 709: loss 0.228041
batch 710: loss 0.144823
batch 711: loss 0.114232
batch 712: loss 0.227160
batch 713: loss 0.330252
batch 714: loss 0.158552
batch 715: loss 0.210313
batch 716: loss 0.164293
batch 717: loss 0.161685
batch 718: loss 0.188547
batch 719: loss 0.180156
batch 720: loss 0.200851
batch 721: loss 0.092565
batch 722: loss 0.173485
batch 723: loss 0.172144
batch 724: loss 0.306569
batch 725: loss 0.363967
batch 726: loss 0.097801
batch 727: loss 0.347186
batch 728: loss 0.188027
batch 729: loss 0.209128
batch 730: loss 0.181954
batch 731: loss 0.150047
batch 732: loss 0.234956
batch 733: loss 0.156357
batch 734: loss 0.207043
batch 735: loss 0.219058
batch 736: loss 0.113917
batch 737: loss 0.337811
batch 738: loss 0.131980
batch 739: loss 0.172769
batch 740: loss 0.169920
batch 741: loss 0.178208
batch 742: loss 0.338216
batch 743: loss 0.180800
batch 744: loss 0.205118
batch 745: loss 0.174190
batch 746: loss 0.201298
batch 747: loss 0.353466
batch 748: loss 0.362657
batch 749: loss 0.069622
batch 750: loss 0.171295
batch 751: loss 0.220336
batch 752: loss 0.149348
batch 753: loss 0.129534
batch 754: loss 0.295249
batch 755: loss 0.191030
batch 756: loss 0.217733
batch 757: loss 0.260932
batch 758: loss 0.216347
batch 759: loss 0.218694
batch 760: loss 0.182442
batch 761: loss 0.184780
batch 762: loss 0.283169
batch 763: loss 0.185836
batch 764: loss 0.225887
batch 765: loss 0.462201
batch 766: loss 0.201778
batch 767: loss 0.118126
batch 768: loss 0.086200
batch 769: loss 0.071274
batch 770: loss 0.150370
batch 771: loss 0.153042
batch 772: loss 0.177931
batch 773: loss 0.120760
batch 774: loss 0.179441
batch 775: loss 0.188710
batch 776: loss 0.116437
batch 777: loss 0.190658
batch 778: loss 0.338074
batch 779: loss 0.287869
batch 780: loss 0.204810
batch 781: loss 0.286212
batch 782: loss 0.356835
batch 783: loss 0.114364
batch 784: loss 0.257239
batch 785: loss 0.211892
batch 786: loss 0.322811
batch 787: loss 0.327907
batch 788: loss 0.183872
batch 789: loss 0.292791
batch 790: loss 0.147020
batch 791: loss 0.146361
batch 792: loss 0.155734
batch 793: loss 0.148340
batch 794: loss 0.105872
batch 795: loss 0.205811
batch 796: loss 0.262269
batch 797: loss 0.115890
batch 798: loss 0.224688
batch 799: loss 0.290177
batch 800: loss 0.154820
batch 801: loss 0.180000
batch 802: loss 0.305846
batch 803: loss 0.327215
batch 804: loss 0.196993
batch 805: loss 0.143069
batch 806: loss 0.240165
batch 807: loss 0.132887
batch 808: loss 0.366762
batch 809: loss 0.105375
batch 810: loss 0.501361
batch 811: loss 0.201997
batch 812: loss 0.096392
batch 813: loss 0.152439
batch 814: loss 0.138672
batch 815: loss 0.151600
batch 816: loss 0.227888
batch 817: loss 0.145324
batch 818: loss 0.146025
batch 819: loss 0.135243
batch 820: loss 0.233483
batch 821: loss 0.457414
batch 822: loss 0.149514
batch 823: loss 0.166615
batch 824: loss 0.372353
batch 825: loss 0.588749
batch 826: loss 0.096479
batch 827: loss 0.165670
batch 828: loss 0.337628
batch 829: loss 0.146481
batch 830: loss 0.062156
batch 831: loss 0.134446
batch 832: loss 0.294032
batch 833: loss 0.060535
batch 834: loss 0.258834
batch 835: loss 0.219695
batch 836: loss 0.162427
batch 837: loss 0.246701
batch 838: loss 0.096755
batch 839: loss 0.214149
batch 840: loss 0.250912
batch 841: loss 0.187933
batch 842: loss 0.185414
batch 843: loss 0.227188
batch 844: loss 0.119188
batch 845: loss 0.334929
batch 846: loss 0.133930
batch 847: loss 0.124146
batch 848: loss 0.105841
batch 849: loss 0.131862
batch 850: loss 0.117265
batch 851: loss 0.235734
batch 852: loss 0.109177
batch 853: loss 0.323216
batch 854: loss 0.076868
batch 855: loss 0.270427
batch 856: loss 0.167292
batch 857: loss 0.367964
batch 858: loss 0.158193
batch 859: loss 0.243538
batch 860: loss 0.203438
batch 861: loss 0.188599
batch 862: loss 0.250619
batch 863: loss 0.097269
batch 864: loss 0.221504
batch 865: loss 0.233273
batch 866: loss 0.128052
batch 867: loss 0.253130
batch 868: loss 0.150896
batch 869: loss 0.236543
batch 870: loss 0.128203
batch 871: loss 0.081954
batch 872: loss 0.101310
batch 873: loss 0.174750
batch 874: loss 0.160408
batch 875: loss 0.105250
batch 876: loss 0.076193
batch 877: loss 0.175928
batch 878: loss 0.216347
batch 879: loss 0.317106
batch 880: loss 0.100854
batch 881: loss 0.127127
batch 882: loss 0.308445
batch 883: loss 0.124931
batch 884: loss 0.248890
batch 885: loss 0.169293
batch 886: loss 0.107638
batch 887: loss 0.158109
batch 888: loss 0.102283
batch 889: loss 0.280424
batch 890: loss 0.139364
batch 891: loss 0.094142
batch 892: loss 0.157169
batch 893: loss 0.048151
batch 894: loss 0.059561
batch 895: loss 0.287735
batch 896: loss 0.374580
batch 897: loss 0.107784
batch 898: loss 0.313108
batch 899: loss 0.135252
batch 900: loss 0.114187
batch 901: loss 0.305683
batch 902: loss 0.245307
batch 903: loss 0.191814
batch 904: loss 0.107314
batch 905: loss 0.208177
batch 906: loss 0.089450
batch 907: loss 0.176495
batch 908: loss 0.234527
batch 909: loss 0.073686
batch 910: loss 0.136289
batch 911: loss 0.080171
batch 912: loss 0.305031
batch 913: loss 0.272383
batch 914: loss 0.140583
batch 915: loss 0.338411
batch 916: loss 0.135546
batch 917: loss 0.071687
batch 918: loss 0.143426
batch 919: loss 0.192598
batch 920: loss 0.100333
batch 921: loss 0.150586
batch 922: loss 0.179805
batch 923: loss 0.237417
batch 924: loss 0.303762
batch 925: loss 0.145758
batch 926: loss 0.274656
batch 927: loss 0.054583
batch 928: loss 0.054210
batch 929: loss 0.129085
batch 930: loss 0.079100
batch 931: loss 0.305413
batch 932: loss 0.121410
batch 933: loss 0.174353
batch 934: loss 0.160843
batch 935: loss 0.272624
batch 936: loss 0.133957
batch 937: loss 0.228590
batch 938: loss 0.265474
batch 939: loss 0.211100
batch 940: loss 0.110170
batch 941: loss 0.123010
batch 942: loss 0.390093
batch 943: loss 0.174213
batch 944: loss 0.124961
batch 945: loss 0.148612
batch 946: loss 0.135782
batch 947: loss 0.298315
batch 948: loss 0.257022
batch 949: loss 0.186655
batch 950: loss 0.111080
batch 951: loss 0.209432
batch 952: loss 0.147841
batch 953: loss 0.208873
batch 954: loss 0.350158
batch 955: loss 0.430361
batch 956: loss 0.379979
batch 957: loss 0.218984
batch 958: loss 0.271433
batch 959: loss 0.173331
batch 960: loss 0.124364
batch 961: loss 0.119645
batch 962: loss 0.139107
batch 963: loss 0.145757
batch 964: loss 0.105806
batch 965: loss 0.237184
batch 966: loss 0.147279
batch 967: loss 0.277448
batch 968: loss 0.182009
batch 969: loss 0.199019
batch 970: loss 0.158343
batch 971: loss 0.123158
batch 972: loss 0.164376
batch 973: loss 0.090123
batch 974: loss 0.130950
batch 975: loss 0.315210
batch 976: loss 0.335807
batch 977: loss 0.161216
batch 978: loss 0.225412
batch 979: loss 0.131215
batch 980: loss 0.247282
batch 981: loss 0.134753
batch 982: loss 0.135581
batch 983: loss 0.203192
batch 984: loss 0.228613
batch 985: loss 0.231663
batch 986: loss 0.135012
batch 987: loss 0.322604
batch 988: loss 0.183648
batch 989: loss 0.273781
batch 990: loss 0.055497
batch 991: loss 0.419740
batch 992: loss 0.216128
batch 993: loss 0.150260
batch 994: loss 0.085838
batch 995: loss 0.308117
batch 996: loss 0.201595
batch 997: loss 0.343333
batch 998: loss 0.398500
batch 999: loss 0.254225
batch 1000: loss 0.086956
batch 1001: loss 0.059464
batch 1002: loss 0.249137
batch 1003: loss 0.188538
batch 1004: loss 0.140708
batch 1005: loss 0.265630
batch 1006: loss 0.256077
batch 1007: loss 0.110759
batch 1008: loss 0.208575
batch 1009: loss 0.207554
batch 1010: loss 0.182978
batch 1011: loss 0.047142
batch 1012: loss 0.234092
batch 1013: loss 0.255006
batch 1014: loss 0.380593
batch 1015: loss 0.382022
batch 1016: loss 0.250487
batch 1017: loss 0.389435
batch 1018: loss 0.138237
batch 1019: loss 0.122437
batch 1020: loss 0.215960
batch 1021: loss 0.144368
batch 1022: loss 0.063167
batch 1023: loss 0.258749
batch 1024: loss 0.215660
batch 1025: loss 0.169605
batch 1026: loss 0.222371
batch 1027: loss 0.160503
batch 1028: loss 0.091064
batch 1029: loss 0.166862
batch 1030: loss 0.256545
batch 1031: loss 0.263640
batch 1032: loss 0.363329
batch 1033: loss 0.132013
batch 1034: loss 0.361455
batch 1035: loss 0.122579
batch 1036: loss 0.255639
batch 1037: loss 0.147059
batch 1038: loss 0.128041
batch 1039: loss 0.097752
batch 1040: loss 0.176450
batch 1041: loss 0.116697
batch 1042: loss 0.073593
batch 1043: loss 0.219901
batch 1044: loss 0.141889
batch 1045: loss 0.219300
batch 1046: loss 0.157903
batch 1047: loss 0.147873
batch 1048: loss 0.132922
batch 1049: loss 0.103669
batch 1050: loss 0.293736
batch 1051: loss 0.195533
batch 1052: loss 0.241293
batch 1053: loss 0.318838
batch 1054: loss 0.238642
batch 1055: loss 0.266800
batch 1056: loss 0.096591
batch 1057: loss 0.108956
batch 1058: loss 0.049442
batch 1059: loss 0.089660
batch 1060: loss 0.059911
batch 1061: loss 0.281191
batch 1062: loss 0.123201
batch 1063: loss 0.113689
batch 1064: loss 0.161101
batch 1065: loss 0.378393
batch 1066: loss 0.163909
batch 1067: loss 0.073953
batch 1068: loss 0.136903
batch 1069: loss 0.148149
batch 1070: loss 0.183497
batch 1071: loss 0.236094
batch 1072: loss 0.101146
batch 1073: loss 0.103948
batch 1074: loss 0.275163
batch 1075: loss 0.185820
batch 1076: loss 0.121375
batch 1077: loss 0.115727
batch 1078: loss 0.130624
batch 1079: loss 0.229426
batch 1080: loss 0.143067
batch 1081: loss 0.097142
batch 1082: loss 0.180269
batch 1083: loss 0.208798
batch 1084: loss 0.340739
batch 1085: loss 0.131768
batch 1086: loss 0.125420
batch 1087: loss 0.152654
batch 1088: loss 0.141136
batch 1089: loss 0.100431
batch 1090: loss 0.135799
batch 1091: loss 0.379679
batch 1092: loss 0.078721
batch 1093: loss 0.391473
batch 1094: loss 0.553992
batch 1095: loss 0.139589
batch 1096: loss 0.083935
batch 1097: loss 0.181077
batch 1098: loss 0.142036
batch 1099: loss 0.026884
batch 1100: loss 0.202918
batch 1101: loss 0.113825
batch 1102: loss 0.100030
batch 1103: loss 0.161295
batch 1104: loss 0.096025
batch 1105: loss 0.133122
batch 1106: loss 0.105768
batch 1107: loss 0.199275
batch 1108: loss 0.092459
batch 1109: loss 0.128206
batch 1110: loss 0.271589
batch 1111: loss 0.180427
batch 1112: loss 0.069741
batch 1113: loss 0.122419
batch 1114: loss 0.099530
batch 1115: loss 0.063315
batch 1116: loss 0.298018
batch 1117: loss 0.130231
batch 1118: loss 0.133063
batch 1119: loss 0.135472
batch 1120: loss 0.121924
batch 1121: loss 0.171091
batch 1122: loss 0.279224
batch 1123: loss 0.307852
batch 1124: loss 0.127327
batch 1125: loss 0.093573
batch 1126: loss 0.071929
batch 1127: loss 0.072314
batch 1128: loss 0.297164
batch 1129: loss 0.141945
batch 1130: loss 0.332947
batch 1131: loss 0.064779
batch 1132: loss 0.188216
batch 1133: loss 0.088020
batch 1134: loss 0.089753
batch 1135: loss 0.226469
batch 1136: loss 0.177748
batch 1137: loss 0.070747
batch 1138: loss 0.065446
batch 1139: loss 0.173488
batch 1140: loss 0.385899
batch 1141: loss 0.221569
batch 1142: loss 0.156759
batch 1143: loss 0.112594
batch 1144: loss 0.422016
batch 1145: loss 0.319304
batch 1146: loss 0.182611
batch 1147: loss 0.133673
batch 1148: loss 0.203741
batch 1149: loss 0.126668
batch 1150: loss 0.119662
batch 1151: loss 0.186640
batch 1152: loss 0.139905
batch 1153: loss 0.219523
batch 1154: loss 0.184631
batch 1155: loss 0.204487
batch 1156: loss 0.076739
batch 1157: loss 0.151207
batch 1158: loss 0.135014
batch 1159: loss 0.119042
batch 1160: loss 0.118406
batch 1161: loss 0.351821
batch 1162: loss 0.061940
batch 1163: loss 0.253970
batch 1164: loss 0.167846
batch 1165: loss 0.139168
batch 1166: loss 0.247836
batch 1167: loss 0.057595
batch 1168: loss 0.129493
batch 1169: loss 0.215815
batch 1170: loss 0.223963
batch 1171: loss 0.339548
batch 1172: loss 0.297872
batch 1173: loss 0.327555
batch 1174: loss 0.433832
batch 1175: loss 0.069864
batch 1176: loss 0.094290
batch 1177: loss 0.125915
batch 1178: loss 0.238062
batch 1179: loss 0.148865
batch 1180: loss 0.152390
batch 1181: loss 0.321877
batch 1182: loss 0.159312
batch 1183: loss 0.121863
batch 1184: loss 0.126018
batch 1185: loss 0.298994
batch 1186: loss 0.197636
batch 1187: loss 0.304063
batch 1188: loss 0.212336
batch 1189: loss 0.087733
batch 1190: loss 0.193774
batch 1191: loss 0.048554
batch 1192: loss 0.204662
batch 1193: loss 0.137300
batch 1194: loss 0.127973
batch 1195: loss 0.056515
batch 1196: loss 0.170080
batch 1197: loss 0.178005
batch 1198: loss 0.201939
batch 1199: loss 0.158659
batch 1200: loss 0.057527
batch 1201: loss 0.066631
batch 1202: loss 0.239361
batch 1203: loss 0.295863
batch 1204: loss 0.202713
batch 1205: loss 0.175453
batch 1206: loss 0.151113
batch 1207: loss 0.111702
batch 1208: loss 0.115063
batch 1209: loss 0.405141
batch 1210: loss 0.194995
batch 1211: loss 0.108887
batch 1212: loss 0.110585
batch 1213: loss 0.178238
batch 1214: loss 0.456279
batch 1215: loss 0.215396
batch 1216: loss 0.375919
batch 1217: loss 0.292410
batch 1218: loss 0.233430
batch 1219: loss 0.082915
batch 1220: loss 0.044667
batch 1221: loss 0.205689
batch 1222: loss 0.153146
batch 1223: loss 0.102365
batch 1224: loss 0.178039
batch 1225: loss 0.204808
batch 1226: loss 0.068649
batch 1227: loss 0.122761
batch 1228: loss 0.168883
batch 1229: loss 0.215185
batch 1230: loss 0.059585
batch 1231: loss 0.099353
batch 1232: loss 0.044335
batch 1233: loss 0.235951
batch 1234: loss 0.119594
batch 1235: loss 0.209500
batch 1236: loss 0.259497
batch 1237: loss 0.183863
batch 1238: loss 0.147382
batch 1239: loss 0.106911
batch 1240: loss 0.143807
batch 1241: loss 0.099386
batch 1242: loss 0.070967
batch 1243: loss 0.061872
batch 1244: loss 0.241676
batch 1245: loss 0.119739
batch 1246: loss 0.078217
batch 1247: loss 0.121922
batch 1248: loss 0.160528
batch 1249: loss 0.079236
batch 1250: loss 0.188721
batch 1251: loss 0.198645
batch 1252: loss 0.216188
batch 1253: loss 0.114762
batch 1254: loss 0.314956
batch 1255: loss 0.084592
batch 1256: loss 0.187403
batch 1257: loss 0.129780
batch 1258: loss 0.185557
batch 1259: loss 0.148457
batch 1260: loss 0.072056
batch 1261: loss 0.109900
batch 1262: loss 0.136295
batch 1263: loss 0.101410
batch 1264: loss 0.060827
batch 1265: loss 0.246105
batch 1266: loss 0.103243
batch 1267: loss 0.180316
batch 1268: loss 0.276934
batch 1269: loss 0.241241
batch 1270: loss 0.101124
batch 1271: loss 0.202732
batch 1272: loss 0.207178
batch 1273: loss 0.192073
batch 1274: loss 0.119625
batch 1275: loss 0.090027
batch 1276: loss 0.181500
batch 1277: loss 0.077355
batch 1278: loss 0.099186
batch 1279: loss 0.076298
batch 1280: loss 0.323314
batch 1281: loss 0.222952
batch 1282: loss 0.090059
batch 1283: loss 0.149054
batch 1284: loss 0.222186
batch 1285: loss 0.230546
batch 1286: loss 0.179495
batch 1287: loss 0.126828
batch 1288: loss 0.042617
batch 1289: loss 0.199431
batch 1290: loss 0.034311
batch 1291: loss 0.162598
batch 1292: loss 0.104663
batch 1293: loss 0.103817
batch 1294: loss 0.149288
batch 1295: loss 0.164269
batch 1296: loss 0.191383
batch 1297: loss 0.179974
batch 1298: loss 0.217316
batch 1299: loss 0.226904
batch 1300: loss 0.233357
batch 1301: loss 0.120341
batch 1302: loss 0.246057
batch 1303: loss 0.212662
batch 1304: loss 0.039704
batch 1305: loss 0.087358
batch 1306: loss 0.102482
batch 1307: loss 0.050334
batch 1308: loss 0.109127
batch 1309: loss 0.076433
batch 1310: loss 0.224881
batch 1311: loss 0.155667
batch 1312: loss 0.290537
batch 1313: loss 0.170355
batch 1314: loss 0.078326
batch 1315: loss 0.116402
batch 1316: loss 0.094977
batch 1317: loss 0.080414
batch 1318: loss 0.182958
batch 1319: loss 0.084502
batch 1320: loss 0.144475
batch 1321: loss 0.314457
batch 1322: loss 0.074945
batch 1323: loss 0.129656
batch 1324: loss 0.202856
batch 1325: loss 0.085085
batch 1326: loss 0.159788
batch 1327: loss 0.088743
batch 1328: loss 0.148562
batch 1329: loss 0.061326
batch 1330: loss 0.067049
batch 1331: loss 0.079137
batch 1332: loss 0.149813
batch 1333: loss 0.156012
batch 1334: loss 0.106383
batch 1335: loss 0.195407
batch 1336: loss 0.117331
batch 1337: loss 0.144202
batch 1338: loss 0.035008
batch 1339: loss 0.151514
batch 1340: loss 0.275237
batch 1341: loss 0.108586
batch 1342: loss 0.285805
batch 1343: loss 0.089338
batch 1344: loss 0.090068
batch 1345: loss 0.173817
batch 1346: loss 0.158978
batch 1347: loss 0.072520
batch 1348: loss 0.238436
batch 1349: loss 0.071986
batch 1350: loss 0.133349
batch 1351: loss 0.158019
batch 1352: loss 0.383180
batch 1353: loss 0.181066
batch 1354: loss 0.088203
batch 1355: loss 0.114737
batch 1356: loss 0.060488
batch 1357: loss 0.072827
batch 1358: loss 0.155710
batch 1359: loss 0.329212
batch 1360: loss 0.365677
batch 1361: loss 0.172669
batch 1362: loss 0.208262
batch 1363: loss 0.101943
batch 1364: loss 0.157510
batch 1365: loss 0.083603
batch 1366: loss 0.087421
batch 1367: loss 0.282993
batch 1368: loss 0.131696
batch 1369: loss 0.049725
batch 1370: loss 0.205607
batch 1371: loss 0.139240
batch 1372: loss 0.067435
batch 1373: loss 0.190844
batch 1374: loss 0.151582
batch 1375: loss 0.123935
batch 1376: loss 0.187806
batch 1377: loss 0.154925
batch 1378: loss 0.144847
batch 1379: loss 0.123899
batch 1380: loss 0.058784
batch 1381: loss 0.095483
batch 1382: loss 0.213942
batch 1383: loss 0.044768
batch 1384: loss 0.026207
batch 1385: loss 0.131318
batch 1386: loss 0.113887
batch 1387: loss 0.161639
batch 1388: loss 0.358534
batch 1389: loss 0.106709
batch 1390: loss 0.124312
batch 1391: loss 0.167660
batch 1392: loss 0.161695
batch 1393: loss 0.090013
batch 1394: loss 0.171644
batch 1395: loss 0.206010
batch 1396: loss 0.151351
batch 1397: loss 0.108932
batch 1398: loss 0.093417
batch 1399: loss 0.043502
batch 1400: loss 0.135936
batch 1401: loss 0.027241
batch 1402: loss 0.147960
batch 1403: loss 0.227250
batch 1404: loss 0.060430
batch 1405: loss 0.113846
batch 1406: loss 0.271906
batch 1407: loss 0.223470
batch 1408: loss 0.126127
batch 1409: loss 0.118032
batch 1410: loss 0.153849
batch 1411: loss 0.067678
batch 1412: loss 0.353581
batch 1413: loss 0.134555
batch 1414: loss 0.085473
batch 1415: loss 0.101222
batch 1416: loss 0.265397
batch 1417: loss 0.121898
batch 1418: loss 0.167279
batch 1419: loss 0.161637
batch 1420: loss 0.064142
batch 1421: loss 0.194803
batch 1422: loss 0.244527
batch 1423: loss 0.208915
batch 1424: loss 0.158811
batch 1425: loss 0.161438
batch 1426: loss 0.148854
batch 1427: loss 0.068200
batch 1428: loss 0.140407
batch 1429: loss 0.041441
batch 1430: loss 0.087957
batch 1431: loss 0.263506
batch 1432: loss 0.136997
batch 1433: loss 0.036348
batch 1434: loss 0.171637
batch 1435: loss 0.170839
batch 1436: loss 0.040981
batch 1437: loss 0.141913
batch 1438: loss 0.058457
batch 1439: loss 0.111420
batch 1440: loss 0.072916
batch 1441: loss 0.134955
batch 1442: loss 0.349068
batch 1443: loss 0.403454
batch 1444: loss 0.046174
batch 1445: loss 0.140343
batch 1446: loss 0.361651
batch 1447: loss 0.073325
batch 1448: loss 0.201574
batch 1449: loss 0.282763
batch 1450: loss 0.166776
batch 1451: loss 0.049856
batch 1452: loss 0.442807
batch 1453: loss 0.056928
batch 1454: loss 0.193937
batch 1455: loss 0.186638
batch 1456: loss 0.212456
batch 1457: loss 0.132587
batch 1458: loss 0.061240
batch 1459: loss 0.125790
batch 1460: loss 0.086778
batch 1461: loss 0.223342
batch 1462: loss 0.073102
batch 1463: loss 0.172625
batch 1464: loss 0.049398
batch 1465: loss 0.056480
batch 1466: loss 0.175554
batch 1467: loss 0.096673
batch 1468: loss 0.138041
batch 1469: loss 0.271026
batch 1470: loss 0.184490
batch 1471: loss 0.334884
batch 1472: loss 0.227595
batch 1473: loss 0.195948
batch 1474: loss 0.104903
batch 1475: loss 0.132554
batch 1476: loss 0.074199
batch 1477: loss 0.083951
batch 1478: loss 0.085383
batch 1479: loss 0.110166
batch 1480: loss 0.085300
batch 1481: loss 0.223421
batch 1482: loss 0.054771
batch 1483: loss 0.200337
batch 1484: loss 0.050381
batch 1485: loss 0.074962
batch 1486: loss 0.246032
batch 1487: loss 0.073304
batch 1488: loss 0.113620
batch 1489: loss 0.111328
batch 1490: loss 0.114153
batch 1491: loss 0.176752
batch 1492: loss 0.056558
batch 1493: loss 0.127653
batch 1494: loss 0.204782
batch 1495: loss 0.098405
batch 1496: loss 0.139801
batch 1497: loss 0.171365
batch 1498: loss 0.065936
batch 1499: loss 0.193868
batch 1500: loss 0.067943
batch 1501: loss 0.373573
batch 1502: loss 0.083531
batch 1503: loss 0.169613
batch 1504: loss 0.031984
batch 1505: loss 0.128734
batch 1506: loss 0.067587
batch 1507: loss 0.160154
batch 1508: loss 0.166314
batch 1509: loss 0.126860
batch 1510: loss 0.244981
batch 1511: loss 0.067589
batch 1512: loss 0.177220
batch 1513: loss 0.242172
batch 1514: loss 0.105581
batch 1515: loss 0.098557
batch 1516: loss 0.032413
batch 1517: loss 0.295351
batch 1518: loss 0.332770
batch 1519: loss 0.106635
batch 1520: loss 0.123111
batch 1521: loss 0.223616
batch 1522: loss 0.166255
batch 1523: loss 0.135530
batch 1524: loss 0.169617
batch 1525: loss 0.319859
batch 1526: loss 0.260895
batch 1527: loss 0.117953
batch 1528: loss 0.169725
batch 1529: loss 0.051938
batch 1530: loss 0.142080
batch 1531: loss 0.070138
batch 1532: loss 0.129309
batch 1533: loss 0.065802
batch 1534: loss 0.310964
batch 1535: loss 0.124325
batch 1536: loss 0.281955
batch 1537: loss 0.205220
batch 1538: loss 0.105222
batch 1539: loss 0.061550
batch 1540: loss 0.094480
batch 1541: loss 0.072762
batch 1542: loss 0.227720
batch 1543: loss 0.101869
batch 1544: loss 0.149875
batch 1545: loss 0.045095
batch 1546: loss 0.124846
batch 1547: loss 0.071393
batch 1548: loss 0.263223
batch 1549: loss 0.066497
batch 1550: loss 0.018484
batch 1551: loss 0.162232
batch 1552: loss 0.073881
batch 1553: loss 0.086745
batch 1554: loss 0.250906
batch 1555: loss 0.316796
batch 1556: loss 0.170762
batch 1557: loss 0.070687
batch 1558: loss 0.226285
batch 1559: loss 0.042845
batch 1560: loss 0.272982
batch 1561: loss 0.175275
batch 1562: loss 0.203419
batch 1563: loss 0.046091
batch 1564: loss 0.085855
batch 1565: loss 0.279768
batch 1566: loss 0.267686
batch 1567: loss 0.098631
batch 1568: loss 0.323532
batch 1569: loss 0.316258
batch 1570: loss 0.128388
batch 1571: loss 0.156156
batch 1572: loss 0.067399
batch 1573: loss 0.084954
batch 1574: loss 0.050930
batch 1575: loss 0.052628
batch 1576: loss 0.252350
batch 1577: loss 0.240127
batch 1578: loss 0.116356
batch 1579: loss 0.059999
batch 1580: loss 0.059138
batch 1581: loss 0.075477
batch 1582: loss 0.156270
batch 1583: loss 0.130352
batch 1584: loss 0.166613
batch 1585: loss 0.177924
batch 1586: loss 0.167287
batch 1587: loss 0.078113
batch 1588: loss 0.132710
batch 1589: loss 0.065202
batch 1590: loss 0.150624
batch 1591: loss 0.138538
batch 1592: loss 0.026458
batch 1593: loss 0.135037
batch 1594: loss 0.145239
batch 1595: loss 0.199954
batch 1596: loss 0.106114
batch 1597: loss 0.166718
batch 1598: loss 0.033807
batch 1599: loss 0.135695
batch 1600: loss 0.105491
batch 1601: loss 0.023846
batch 1602: loss 0.122538
batch 1603: loss 0.282346
batch 1604: loss 0.222211
batch 1605: loss 0.149137
batch 1606: loss 0.215130
batch 1607: loss 0.111519
batch 1608: loss 0.142803
batch 1609: loss 0.091796
batch 1610: loss 0.056376
batch 1611: loss 0.076025
batch 1612: loss 0.158699
batch 1613: loss 0.223542
batch 1614: loss 0.299938
batch 1615: loss 0.051778
batch 1616: loss 0.326036
batch 1617: loss 0.112040
batch 1618: loss 0.182789
batch 1619: loss 0.078892
batch 1620: loss 0.060284
batch 1621: loss 0.043222
batch 1622: loss 0.205435
batch 1623: loss 0.199029
batch 1624: loss 0.188738
batch 1625: loss 0.092539
batch 1626: loss 0.130759
batch 1627: loss 0.137500
batch 1628: loss 0.060705
batch 1629: loss 0.089530
batch 1630: loss 0.100268
batch 1631: loss 0.034952
batch 1632: loss 0.134042
batch 1633: loss 0.043620
batch 1634: loss 0.110084
batch 1635: loss 0.083792
batch 1636: loss 0.264239
batch 1637: loss 0.044856
batch 1638: loss 0.223906
batch 1639: loss 0.299948
batch 1640: loss 0.190232
batch 1641: loss 0.135617
batch 1642: loss 0.094353
batch 1643: loss 0.105740
batch 1644: loss 0.088110
batch 1645: loss 0.138510
batch 1646: loss 0.116407
batch 1647: loss 0.101016
batch 1648: loss 0.197392
batch 1649: loss 0.179140
batch 1650: loss 0.133098
batch 1651: loss 0.170886
batch 1652: loss 0.312445
batch 1653: loss 0.090581
batch 1654: loss 0.173874
batch 1655: loss 0.170793
batch 1656: loss 0.071192
batch 1657: loss 0.070332
batch 1658: loss 0.140612
batch 1659: loss 0.188475
batch 1660: loss 0.116071
batch 1661: loss 0.076099
batch 1662: loss 0.120539
batch 1663: loss 0.090774
batch 1664: loss 0.135504
batch 1665: loss 0.183301
batch 1666: loss 0.180372
batch 1667: loss 0.146037
batch 1668: loss 0.188491
batch 1669: loss 0.160445
batch 1670: loss 0.143845
batch 1671: loss 0.087139
batch 1672: loss 0.040511
batch 1673: loss 0.074215
batch 1674: loss 0.263080
batch 1675: loss 0.045759
batch 1676: loss 0.369232
batch 1677: loss 0.090095
batch 1678: loss 0.137385
batch 1679: loss 0.200896
batch 1680: loss 0.084518
batch 1681: loss 0.089530
batch 1682: loss 0.038353
batch 1683: loss 0.066038
batch 1684: loss 0.366885
batch 1685: loss 0.081251
batch 1686: loss 0.229301
batch 1687: loss 0.089203
batch 1688: loss 0.147315
batch 1689: loss 0.111134
batch 1690: loss 0.105883
batch 1691: loss 0.171984
batch 1692: loss 0.041543
batch 1693: loss 0.129988
batch 1694: loss 0.076774
batch 1695: loss 0.103276
batch 1696: loss 0.034778
batch 1697: loss 0.216547
batch 1698: loss 0.096075
batch 1699: loss 0.056325
batch 1700: loss 0.179158
batch 1701: loss 0.034523
batch 1702: loss 0.102419
batch 1703: loss 0.208498
batch 1704: loss 0.218125
batch 1705: loss 0.313097
batch 1706: loss 0.070261
batch 1707: loss 0.096690
batch 1708: loss 0.172761
batch 1709: loss 0.161449
batch 1710: loss 0.112637
batch 1711: loss 0.035839
batch 1712: loss 0.056868
batch 1713: loss 0.399204
batch 1714: loss 0.109838
batch 1715: loss 0.107315
batch 1716: loss 0.159676
batch 1717: loss 0.257750
batch 1718: loss 0.136556
batch 1719: loss 0.134750
batch 1720: loss 0.224041
batch 1721: loss 0.250269
batch 1722: loss 0.218350
batch 1723: loss 0.140447
batch 1724: loss 0.192994
batch 1725: loss 0.087426
batch 1726: loss 0.090143
batch 1727: loss 0.078850
batch 1728: loss 0.142503
batch 1729: loss 0.119079
batch 1730: loss 0.116794
batch 1731: loss 0.062663
batch 1732: loss 0.099542
batch 1733: loss 0.130488
batch 1734: loss 0.048341
batch 1735: loss 0.107689
batch 1736: loss 0.161244
batch 1737: loss 0.178909
batch 1738: loss 0.190889
batch 1739: loss 0.078625
batch 1740: loss 0.118031
batch 1741: loss 0.117682
batch 1742: loss 0.074719
batch 1743: loss 0.101574
batch 1744: loss 0.105531
batch 1745: loss 0.082940
batch 1746: loss 0.097528
batch 1747: loss 0.127166
batch 1748: loss 0.372057
batch 1749: loss 0.037591
batch 1750: loss 0.135776
batch 1751: loss 0.106666
batch 1752: loss 0.062776
batch 1753: loss 0.163271
batch 1754: loss 0.054711
batch 1755: loss 0.181468
batch 1756: loss 0.118719
batch 1757: loss 0.096474
batch 1758: loss 0.177636
batch 1759: loss 0.117505
batch 1760: loss 0.070871
batch 1761: loss 0.070299
batch 1762: loss 0.093885
batch 1763: loss 0.229198
batch 1764: loss 0.164013
batch 1765: loss 0.050751
batch 1766: loss 0.070767
batch 1767: loss 0.158210
batch 1768: loss 0.053186
batch 1769: loss 0.177763
batch 1770: loss 0.091989
batch 1771: loss 0.124035
batch 1772: loss 0.156193
batch 1773: loss 0.152763
batch 1774: loss 0.058325
batch 1775: loss 0.252223
batch 1776: loss 0.165917
batch 1777: loss 0.156356
batch 1778: loss 0.067649
batch 1779: loss 0.056683
batch 1780: loss 0.113961
batch 1781: loss 0.130950
batch 1782: loss 0.074563
batch 1783: loss 0.090217
batch 1784: loss 0.347535
batch 1785: loss 0.217289
batch 1786: loss 0.052428
batch 1787: loss 0.073071
batch 1788: loss 0.045778
batch 1789: loss 0.143138
batch 1790: loss 0.283140
batch 1791: loss 0.165144
batch 1792: loss 0.160652
batch 1793: loss 0.179779
batch 1794: loss 0.209323
batch 1795: loss 0.185784
batch 1796: loss 0.046738
batch 1797: loss 0.084297
batch 1798: loss 0.073651
batch 1799: loss 0.059147
batch 1800: loss 0.108352
batch 1801: loss 0.199218
batch 1802: loss 0.164189
batch 1803: loss 0.052120
batch 1804: loss 0.041858
batch 1805: loss 0.040791
batch 1806: loss 0.264792
batch 1807: loss 0.039177
batch 1808: loss 0.012181
batch 1809: loss 0.095402
batch 1810: loss 0.081484
batch 1811: loss 0.060052
batch 1812: loss 0.042358
batch 1813: loss 0.214642
batch 1814: loss 0.069693
batch 1815: loss 0.106516
batch 1816: loss 0.118471
batch 1817: loss 0.179463
batch 1818: loss 0.122413
batch 1819: loss 0.051865
batch 1820: loss 0.080972
batch 1821: loss 0.075685
batch 1822: loss 0.056151
batch 1823: loss 0.074795
batch 1824: loss 0.229037
batch 1825: loss 0.125459
batch 1826: loss 0.108481
batch 1827: loss 0.105910
batch 1828: loss 0.069914
batch 1829: loss 0.074983
batch 1830: loss 0.116140
batch 1831: loss 0.165648
batch 1832: loss 0.038315
batch 1833: loss 0.043347
batch 1834: loss 0.064977
batch 1835: loss 0.294955
batch 1836: loss 0.110416
batch 1837: loss 0.068603
batch 1838: loss 0.160557
batch 1839: loss 0.051925
batch 1840: loss 0.130004
batch 1841: loss 0.054379
batch 1842: loss 0.077859
batch 1843: loss 0.195668
batch 1844: loss 0.069047
batch 1845: loss 0.158818
batch 1846: loss 0.180149
batch 1847: loss 0.054077
batch 1848: loss 0.085065
batch 1849: loss 0.153359
batch 1850: loss 0.030761
batch 1851: loss 0.258434
batch 1852: loss 0.072958
batch 1853: loss 0.170352
batch 1854: loss 0.087569
batch 1855: loss 0.245388
batch 1856: loss 0.200191
batch 1857: loss 0.219491
batch 1858: loss 0.176265
batch 1859: loss 0.244516
batch 1860: loss 0.075336
batch 1861: loss 0.249817
batch 1862: loss 0.033277
batch 1863: loss 0.106167
batch 1864: loss 0.125661
batch 1865: loss 0.105984
batch 1866: loss 0.116379
batch 1867: loss 0.038542
batch 1868: loss 0.041423
batch 1869: loss 0.123254
batch 1870: loss 0.219028
batch 1871: loss 0.039071
batch 1872: loss 0.116431
batch 1873: loss 0.179716
batch 1874: loss 0.091485
batch 1875: loss 0.142536
batch 1876: loss 0.053697
batch 1877: loss 0.049582
batch 1878: loss 0.039171
batch 1879: loss 0.147749
batch 1880: loss 0.148482
batch 1881: loss 0.035305
batch 1882: loss 0.149237
batch 1883: loss 0.221674
batch 1884: loss 0.124611
batch 1885: loss 0.047674
batch 1886: loss 0.114730
batch 1887: loss 0.031070
batch 1888: loss 0.249503
batch 1889: loss 0.029535
batch 1890: loss 0.089287
batch 1891: loss 0.161713
batch 1892: loss 0.176594
batch 1893: loss 0.062994
batch 1894: loss 0.045783
batch 1895: loss 0.170782
batch 1896: loss 0.152454
batch 1897: loss 0.159894
batch 1898: loss 0.064036
batch 1899: loss 0.098752
batch 1900: loss 0.079641
batch 1901: loss 0.089821
batch 1902: loss 0.098600
batch 1903: loss 0.077301
batch 1904: loss 0.045340
batch 1905: loss 0.069823
batch 1906: loss 0.117374
batch 1907: loss 0.397197
batch 1908: loss 0.184773
batch 1909: loss 0.128028
batch 1910: loss 0.044520
batch 1911: loss 0.131747
batch 1912: loss 0.055526
batch 1913: loss 0.097785
batch 1914: loss 0.141876
batch 1915: loss 0.094185
batch 1916: loss 0.188243
batch 1917: loss 0.239114
batch 1918: loss 0.087452
batch 1919: loss 0.192090
batch 1920: loss 0.055119
batch 1921: loss 0.075957
batch 1922: loss 0.070447
batch 1923: loss 0.088847
batch 1924: loss 0.232021
batch 1925: loss 0.057556
batch 1926: loss 0.038216
batch 1927: loss 0.160622
batch 1928: loss 0.060114
batch 1929: loss 0.142777
batch 1930: loss 0.103879
batch 1931: loss 0.120142
batch 1932: loss 0.056373
batch 1933: loss 0.168933
batch 1934: loss 0.082298
batch 1935: loss 0.302873
batch 1936: loss 0.050983
batch 1937: loss 0.044958
batch 1938: loss 0.105662
batch 1939: loss 0.068246
batch 1940: loss 0.183631
batch 1941: loss 0.093929
batch 1942: loss 0.078421
batch 1943: loss 0.053653
batch 1944: loss 0.034445
batch 1945: loss 0.105185
batch 1946: loss 0.088628
batch 1947: loss 0.063155
batch 1948: loss 0.055042
batch 1949: loss 0.085763
batch 1950: loss 0.088806
batch 1951: loss 0.107227
batch 1952: loss 0.213745
batch 1953: loss 0.012995
batch 1954: loss 0.250901
batch 1955: loss 0.081365
batch 1956: loss 0.055996
batch 1957: loss 0.101625
batch 1958: loss 0.099686
batch 1959: loss 0.159460
batch 1960: loss 0.068753
batch 1961: loss 0.111464
batch 1962: loss 0.182887
batch 1963: loss 0.129073
batch 1964: loss 0.095786
batch 1965: loss 0.226592
batch 1966: loss 0.107665
batch 1967: loss 0.140821
batch 1968: loss 0.135071
batch 1969: loss 0.108046
batch 1970: loss 0.072988
batch 1971: loss 0.076133
batch 1972: loss 0.409847
batch 1973: loss 0.104600
batch 1974: loss 0.172850
batch 1975: loss 0.156614
batch 1976: loss 0.120276
batch 1977: loss 0.294092
batch 1978: loss 0.040400
batch 1979: loss 0.184762
batch 1980: loss 0.159166
batch 1981: loss 0.125464
batch 1982: loss 0.110157
batch 1983: loss 0.116699
batch 1984: loss 0.179050
batch 1985: loss 0.111528
batch 1986: loss 0.242980
batch 1987: loss 0.101456
batch 1988: loss 0.124083
batch 1989: loss 0.070688
batch 1990: loss 0.068309
batch 1991: loss 0.201139
batch 1992: loss 0.172714
batch 1993: loss 0.260794
batch 1994: loss 0.213429
batch 1995: loss 0.034231
batch 1996: loss 0.103945
batch 1997: loss 0.043852
batch 1998: loss 0.162212
batch 1999: loss 0.118345
batch 2000: loss 0.099801
batch 2001: loss 0.117550
batch 2002: loss 0.121649
batch 2003: loss 0.076137
batch 2004: loss 0.053568
batch 2005: loss 0.106507
batch 2006: loss 0.136887
batch 2007: loss 0.135367
batch 2008: loss 0.080878
batch 2009: loss 0.055990
batch 2010: loss 0.045424
batch 2011: loss 0.088845
batch 2012: loss 0.043518
batch 2013: loss 0.181097
batch 2014: loss 0.112270
batch 2015: loss 0.089554
batch 2016: loss 0.126148
batch 2017: loss 0.213931
batch 2018: loss 0.131840
batch 2019: loss 0.132186
batch 2020: loss 0.127939
batch 2021: loss 0.115083
batch 2022: loss 0.077384
batch 2023: loss 0.047261
batch 2024: loss 0.117580
batch 2025: loss 0.057511
batch 2026: loss 0.100288
batch 2027: loss 0.050494
batch 2028: loss 0.140605
batch 2029: loss 0.085217
batch 2030: loss 0.143036
batch 2031: loss 0.167178
batch 2032: loss 0.055348
batch 2033: loss 0.057698
batch 2034: loss 0.086289
batch 2035: loss 0.065926
batch 2036: loss 0.087779
batch 2037: loss 0.095253
batch 2038: loss 0.046297
batch 2039: loss 0.075541
batch 2040: loss 0.136054
batch 2041: loss 0.052063
batch 2042: loss 0.104310
batch 2043: loss 0.040114
batch 2044: loss 0.105298
batch 2045: loss 0.069492
batch 2046: loss 0.091104
batch 2047: loss 0.086986
batch 2048: loss 0.139647
batch 2049: loss 0.168638
batch 2050: loss 0.029907
batch 2051: loss 0.223844
batch 2052: loss 0.054940
batch 2053: loss 0.071488
batch 2054: loss 0.019379
batch 2055: loss 0.223432
batch 2056: loss 0.188931
batch 2057: loss 0.107153
batch 2058: loss 0.237211
batch 2059: loss 0.040990
batch 2060: loss 0.037086
batch 2061: loss 0.031749
batch 2062: loss 0.189404
batch 2063: loss 0.054488
batch 2064: loss 0.231051
batch 2065: loss 0.035488
batch 2066: loss 0.106608
batch 2067: loss 0.068480
batch 2068: loss 0.098740
batch 2069: loss 0.021819
batch 2070: loss 0.062418
batch 2071: loss 0.109029
batch 2072: loss 0.039535
batch 2073: loss 0.097327
batch 2074: loss 0.085248
batch 2075: loss 0.109630
batch 2076: loss 0.266461
batch 2077: loss 0.077677
batch 2078: loss 0.167701
batch 2079: loss 0.162781
batch 2080: loss 0.054092
batch 2081: loss 0.163317
batch 2082: loss 0.215065
batch 2083: loss 0.028323
batch 2084: loss 0.108646
batch 2085: loss 0.124446
batch 2086: loss 0.102016
batch 2087: loss 0.039120
batch 2088: loss 0.037539
batch 2089: loss 0.186389
batch 2090: loss 0.083818
batch 2091: loss 0.162352
batch 2092: loss 0.171544
batch 2093: loss 0.116858
batch 2094: loss 0.063018
batch 2095: loss 0.077614
batch 2096: loss 0.197045
batch 2097: loss 0.204948
batch 2098: loss 0.190535
batch 2099: loss 0.108149
batch 2100: loss 0.040935
batch 2101: loss 0.088851
batch 2102: loss 0.066368
batch 2103: loss 0.156774
batch 2104: loss 0.160701
batch 2105: loss 0.216518
batch 2106: loss 0.044270
batch 2107: loss 0.112050
batch 2108: loss 0.067445
batch 2109: loss 0.144916
batch 2110: loss 0.323275
batch 2111: loss 0.232285
batch 2112: loss 0.120042
batch 2113: loss 0.068352
batch 2114: loss 0.022879
batch 2115: loss 0.083588
batch 2116: loss 0.049247
batch 2117: loss 0.069721
batch 2118: loss 0.278008
batch 2119: loss 0.207941
batch 2120: loss 0.113306
batch 2121: loss 0.093082
batch 2122: loss 0.052260
batch 2123: loss 0.129949
batch 2124: loss 0.094349
batch 2125: loss 0.101061
batch 2126: loss 0.022710
batch 2127: loss 0.054315
batch 2128: loss 0.099191
batch 2129: loss 0.195863
batch 2130: loss 0.106258
batch 2131: loss 0.084452
batch 2132: loss 0.214441
batch 2133: loss 0.124368
batch 2134: loss 0.076658
batch 2135: loss 0.127517
batch 2136: loss 0.123186
batch 2137: loss 0.028855
batch 2138: loss 0.094900
batch 2139: loss 0.091339
batch 2140: loss 0.117207
batch 2141: loss 0.154731
batch 2142: loss 0.316819
batch 2143: loss 0.127391
batch 2144: loss 0.040678
batch 2145: loss 0.057417
batch 2146: loss 0.028939
batch 2147: loss 0.057256
batch 2148: loss 0.082537
batch 2149: loss 0.270890
batch 2150: loss 0.171724
batch 2151: loss 0.118603
batch 2152: loss 0.046658
batch 2153: loss 0.047429
batch 2154: loss 0.128118
batch 2155: loss 0.174142
batch 2156: loss 0.169153
batch 2157: loss 0.211276
batch 2158: loss 0.054728
batch 2159: loss 0.071654
batch 2160: loss 0.195510
batch 2161: loss 0.099759
batch 2162: loss 0.107017
batch 2163: loss 0.046577
batch 2164: loss 0.048809
batch 2165: loss 0.169562
batch 2166: loss 0.140188
batch 2167: loss 0.177018
batch 2168: loss 0.094916
batch 2169: loss 0.078423
batch 2170: loss 0.046259
batch 2171: loss 0.222049
batch 2172: loss 0.075821
batch 2173: loss 0.031597
batch 2174: loss 0.045104
batch 2175: loss 0.117715
batch 2176: loss 0.097754
batch 2177: loss 0.105593
batch 2178: loss 0.170776
batch 2179: loss 0.047802
batch 2180: loss 0.086726
batch 2181: loss 0.064146
batch 2182: loss 0.053728
batch 2183: loss 0.082792
batch 2184: loss 0.060306
batch 2185: loss 0.164374
batch 2186: loss 0.084848
batch 2187: loss 0.162745
batch 2188: loss 0.047202
batch 2189: loss 0.321620
batch 2190: loss 0.044670
batch 2191: loss 0.054446
batch 2192: loss 0.074428
batch 2193: loss 0.125714
batch 2194: loss 0.105559
batch 2195: loss 0.168688
batch 2196: loss 0.096728
batch 2197: loss 0.175856
batch 2198: loss 0.077481
batch 2199: loss 0.178769
batch 2200: loss 0.050626
batch 2201: loss 0.151853
batch 2202: loss 0.112574
batch 2203: loss 0.112074
batch 2204: loss 0.067253
batch 2205: loss 0.093848
batch 2206: loss 0.190393
batch 2207: loss 0.122270
batch 2208: loss 0.133001
batch 2209: loss 0.117507
batch 2210: loss 0.098383
batch 2211: loss 0.050633
batch 2212: loss 0.094727
batch 2213: loss 0.199272
batch 2214: loss 0.371252
batch 2215: loss 0.044117
batch 2216: loss 0.148335
batch 2217: loss 0.104065
batch 2218: loss 0.025950
batch 2219: loss 0.025230
batch 2220: loss 0.082230
batch 2221: loss 0.072990
batch 2222: loss 0.133862
batch 2223: loss 0.071249
batch 2224: loss 0.095412
batch 2225: loss 0.090388
batch 2226: loss 0.052083
batch 2227: loss 0.070574
batch 2228: loss 0.119940
batch 2229: loss 0.331446
batch 2230: loss 0.048530
batch 2231: loss 0.111818
batch 2232: loss 0.120275
batch 2233: loss 0.070555
batch 2234: loss 0.075260
batch 2235: loss 0.070783
batch 2236: loss 0.135814
batch 2237: loss 0.080797
batch 2238: loss 0.230642
batch 2239: loss 0.117067
batch 2240: loss 0.033978
batch 2241: loss 0.137043
batch 2242: loss 0.160756
batch 2243: loss 0.067254
batch 2244: loss 0.137866
batch 2245: loss 0.013117
batch 2246: loss 0.091011
batch 2247: loss 0.114157
batch 2248: loss 0.145093
batch 2249: loss 0.011853
batch 2250: loss 0.044016
batch 2251: loss 0.203984
batch 2252: loss 0.040443
batch 2253: loss 0.267859
batch 2254: loss 0.045825
batch 2255: loss 0.263607
batch 2256: loss 0.032824
batch 2257: loss 0.035476
batch 2258: loss 0.111427
batch 2259: loss 0.069930
batch 2260: loss 0.078893
batch 2261: loss 0.042369
batch 2262: loss 0.171617
batch 2263: loss 0.162972
batch 2264: loss 0.096747
batch 2265: loss 0.034975
batch 2266: loss 0.115731
batch 2267: loss 0.106630
batch 2268: loss 0.181339
batch 2269: loss 0.137724
batch 2270: loss 0.236440
batch 2271: loss 0.133825
batch 2272: loss 0.048614
batch 2273: loss 0.065759
batch 2274: loss 0.220944
batch 2275: loss 0.233031
batch 2276: loss 0.134165
batch 2277: loss 0.031560
batch 2278: loss 0.087928
batch 2279: loss 0.090313
batch 2280: loss 0.045827
batch 2281: loss 0.242329
batch 2282: loss 0.120253
batch 2283: loss 0.024921
batch 2284: loss 0.019673
batch 2285: loss 0.077881
batch 2286: loss 0.049019
batch 2287: loss 0.308374
batch 2288: loss 0.067729
batch 2289: loss 0.143257
batch 2290: loss 0.208173
batch 2291: loss 0.134978
batch 2292: loss 0.073019
batch 2293: loss 0.158777
batch 2294: loss 0.128241
batch 2295: loss 0.120979
batch 2296: loss 0.192083
batch 2297: loss 0.024797
batch 2298: loss 0.206165
batch 2299: loss 0.081230
batch 2300: loss 0.035767
batch 2301: loss 0.372301
batch 2302: loss 0.083951
batch 2303: loss 0.159182
batch 2304: loss 0.102079
batch 2305: loss 0.132990
batch 2306: loss 0.259630
batch 2307: loss 0.097127
batch 2308: loss 0.061028
batch 2309: loss 0.117226
batch 2310: loss 0.081015
batch 2311: loss 0.105767
batch 2312: loss 0.122520
batch 2313: loss 0.088505
batch 2314: loss 0.262486
batch 2315: loss 0.028969
batch 2316: loss 0.068813
batch 2317: loss 0.057839
batch 2318: loss 0.056701
batch 2319: loss 0.101041
batch 2320: loss 0.134646
batch 2321: loss 0.052265
batch 2322: loss 0.082732
batch 2323: loss 0.104522
batch 2324: loss 0.045507
batch 2325: loss 0.061100
batch 2326: loss 0.164034
batch 2327: loss 0.077477
batch 2328: loss 0.165566
batch 2329: loss 0.056151
batch 2330: loss 0.036146
batch 2331: loss 0.085672
batch 2332: loss 0.049412
batch 2333: loss 0.114119
batch 2334: loss 0.166560
batch 2335: loss 0.200484
batch 2336: loss 0.065813
batch 2337: loss 0.100295
batch 2338: loss 0.045945
batch 2339: loss 0.119898
batch 2340: loss 0.087072
batch 2341: loss 0.049834
batch 2342: loss 0.133757
batch 2343: loss 0.024311
batch 2344: loss 0.138366
batch 2345: loss 0.138137
batch 2346: loss 0.085360
batch 2347: loss 0.168303
batch 2348: loss 0.319910
batch 2349: loss 0.115932
batch 2350: loss 0.285810
batch 2351: loss 0.035245
batch 2352: loss 0.140166
batch 2353: loss 0.039795
batch 2354: loss 0.133811
batch 2355: loss 0.045648
batch 2356: loss 0.118512
batch 2357: loss 0.022973
batch 2358: loss 0.147371
batch 2359: loss 0.089748
batch 2360: loss 0.187103
batch 2361: loss 0.069024
batch 2362: loss 0.265330
batch 2363: loss 0.065203
batch 2364: loss 0.072959
batch 2365: loss 0.088629
batch 2366: loss 0.065608
batch 2367: loss 0.118344
batch 2368: loss 0.132412
batch 2369: loss 0.067511
batch 2370: loss 0.021516
batch 2371: loss 0.078161
batch 2372: loss 0.206378
batch 2373: loss 0.062835
batch 2374: loss 0.057236
batch 2375: loss 0.116917
batch 2376: loss 0.031234
batch 2377: loss 0.127626
batch 2378: loss 0.267391
batch 2379: loss 0.205021
batch 2380: loss 0.078147
batch 2381: loss 0.200300
batch 2382: loss 0.049659
batch 2383: loss 0.025734
batch 2384: loss 0.210905
batch 2385: loss 0.104394
batch 2386: loss 0.396229
batch 2387: loss 0.128366
batch 2388: loss 0.087249
batch 2389: loss 0.042653
batch 2390: loss 0.169094
batch 2391: loss 0.031525
batch 2392: loss 0.108546
batch 2393: loss 0.097850
batch 2394: loss 0.147849
batch 2395: loss 0.296469
batch 2396: loss 0.243278
batch 2397: loss 0.235298
batch 2398: loss 0.073908
batch 2399: loss 0.054330
batch 2400: loss 0.035316
batch 2401: loss 0.099515
batch 2402: loss 0.121339
batch 2403: loss 0.063011
batch 2404: loss 0.144236
batch 2405: loss 0.053653
batch 2406: loss 0.180597
batch 2407: loss 0.074333
batch 2408: loss 0.188613
batch 2409: loss 0.216947
batch 2410: loss 0.037465
batch 2411: loss 0.106635
batch 2412: loss 0.114755
batch 2413: loss 0.162277
batch 2414: loss 0.175186
batch 2415: loss 0.177908
batch 2416: loss 0.110744
batch 2417: loss 0.144465
batch 2418: loss 0.091488
batch 2419: loss 0.174325
batch 2420: loss 0.102947
batch 2421: loss 0.335942
batch 2422: loss 0.060872
batch 2423: loss 0.123211
batch 2424: loss 0.033299
batch 2425: loss 0.049931
batch 2426: loss 0.039300
batch 2427: loss 0.108649
batch 2428: loss 0.130832
batch 2429: loss 0.102620
batch 2430: loss 0.096865
batch 2431: loss 0.073403
batch 2432: loss 0.048842
batch 2433: loss 0.064237
batch 2434: loss 0.120006
batch 2435: loss 0.056485
batch 2436: loss 0.046867
batch 2437: loss 0.106654
batch 2438: loss 0.156932
batch 2439: loss 0.110637
batch 2440: loss 0.067990
batch 2441: loss 0.033373
batch 2442: loss 0.070552
batch 2443: loss 0.152081
batch 2444: loss 0.169224
batch 2445: loss 0.077853
batch 2446: loss 0.102406
batch 2447: loss 0.044763
batch 2448: loss 0.081227
batch 2449: loss 0.047784
batch 2450: loss 0.058886
batch 2451: loss 0.077766
batch 2452: loss 0.058236
batch 2453: loss 0.073230
batch 2454: loss 0.150986
batch 2455: loss 0.057122
batch 2456: loss 0.117883
batch 2457: loss 0.124631
batch 2458: loss 0.304661
batch 2459: loss 0.261390
batch 2460: loss 0.101651
batch 2461: loss 0.041638
batch 2462: loss 0.132002
batch 2463: loss 0.045885
batch 2464: loss 0.065377
batch 2465: loss 0.064607
batch 2466: loss 0.127290
batch 2467: loss 0.131247
batch 2468: loss 0.146634
batch 2469: loss 0.076913
batch 2470: loss 0.148554
batch 2471: loss 0.122767
batch 2472: loss 0.054948
batch 2473: loss 0.222312
batch 2474: loss 0.067683
batch 2475: loss 0.029403
batch 2476: loss 0.161208
batch 2477: loss 0.061154
batch 2478: loss 0.097040
batch 2479: loss 0.092694
batch 2480: loss 0.064942
batch 2481: loss 0.235496
batch 2482: loss 0.060862
batch 2483: loss 0.109259
batch 2484: loss 0.117387
batch 2485: loss 0.065532
batch 2486: loss 0.069975
batch 2487: loss 0.095729
batch 2488: loss 0.077348
batch 2489: loss 0.148780
batch 2490: loss 0.188306
batch 2491: loss 0.114841
batch 2492: loss 0.073572
batch 2493: loss 0.125296
batch 2494: loss 0.030146
batch 2495: loss 0.093276
batch 2496: loss 0.112917
batch 2497: loss 0.190006
batch 2498: loss 0.045583
batch 2499: loss 0.038891
batch 2500: loss 0.096989
batch 2501: loss 0.037331
batch 2502: loss 0.201616
batch 2503: loss 0.183452
batch 2504: loss 0.146712
batch 2505: loss 0.134766
batch 2506: loss 0.060836
batch 2507: loss 0.186835
batch 2508: loss 0.029473
batch 2509: loss 0.340676
batch 2510: loss 0.159562
batch 2511: loss 0.024262
batch 2512: loss 0.059782
batch 2513: loss 0.098132
batch 2514: loss 0.047379
batch 2515: loss 0.025717
batch 2516: loss 0.066144
batch 2517: loss 0.135343
batch 2518: loss 0.057650
batch 2519: loss 0.134322
batch 2520: loss 0.063340
batch 2521: loss 0.176935
batch 2522: loss 0.200986
batch 2523: loss 0.104974
batch 2524: loss 0.104376
batch 2525: loss 0.110122
batch 2526: loss 0.053304
batch 2527: loss 0.059460
batch 2528: loss 0.063182
batch 2529: loss 0.190996
batch 2530: loss 0.052869
batch 2531: loss 0.327181
batch 2532: loss 0.090784
batch 2533: loss 0.142102
batch 2534: loss 0.074161
batch 2535: loss 0.130836
batch 2536: loss 0.145145
batch 2537: loss 0.052228
batch 2538: loss 0.070141
batch 2539: loss 0.059600
batch 2540: loss 0.410482
batch 2541: loss 0.103141
batch 2542: loss 0.107028
batch 2543: loss 0.019363
batch 2544: loss 0.073347
batch 2545: loss 0.169215
batch 2546: loss 0.100402
batch 2547: loss 0.171854
batch 2548: loss 0.047173
batch 2549: loss 0.095795
batch 2550: loss 0.109635
batch 2551: loss 0.104924
batch 2552: loss 0.073703
batch 2553: loss 0.146292
batch 2554: loss 0.028191
batch 2555: loss 0.088509
batch 2556: loss 0.051441
batch 2557: loss 0.136806
batch 2558: loss 0.051592
batch 2559: loss 0.205888
batch 2560: loss 0.031155
batch 2561: loss 0.016299
batch 2562: loss 0.101633
batch 2563: loss 0.104342
batch 2564: loss 0.124204
batch 2565: loss 0.115037
batch 2566: loss 0.121996
batch 2567: loss 0.114064
batch 2568: loss 0.094038
batch 2569: loss 0.072685
batch 2570: loss 0.041008
batch 2571: loss 0.038755
batch 2572: loss 0.027709
batch 2573: loss 0.077376
batch 2574: loss 0.038834
batch 2575: loss 0.110488
batch 2576: loss 0.085437
batch 2577: loss 0.124497
batch 2578: loss 0.086632
batch 2579: loss 0.041960
batch 2580: loss 0.034672
batch 2581: loss 0.112180
batch 2582: loss 0.035530
batch 2583: loss 0.081249
batch 2584: loss 0.030768
batch 2585: loss 0.042566
batch 2586: loss 0.034475
batch 2587: loss 0.043240
batch 2588: loss 0.094416
batch 2589: loss 0.154302
batch 2590: loss 0.136871
batch 2591: loss 0.035198
batch 2592: loss 0.065246
batch 2593: loss 0.291198
batch 2594: loss 0.204802
batch 2595: loss 0.090625
batch 2596: loss 0.092345
batch 2597: loss 0.171040
batch 2598: loss 0.083133
batch 2599: loss 0.137311
batch 2600: loss 0.128040
batch 2601: loss 0.113079
batch 2602: loss 0.186724
batch 2603: loss 0.099878
batch 2604: loss 0.088291
batch 2605: loss 0.077896
batch 2606: loss 0.022994
batch 2607: loss 0.081213
batch 2608: loss 0.010062
batch 2609: loss 0.158614
batch 2610: loss 0.041458
batch 2611: loss 0.119908
batch 2612: loss 0.052834
batch 2613: loss 0.042188
batch 2614: loss 0.095937
batch 2615: loss 0.067414
batch 2616: loss 0.067680
batch 2617: loss 0.078403
batch 2618: loss 0.265817
batch 2619: loss 0.055515
batch 2620: loss 0.114203
batch 2621: loss 0.105551
batch 2622: loss 0.112139
batch 2623: loss 0.041461
batch 2624: loss 0.067521
batch 2625: loss 0.192606
batch 2626: loss 0.238889
batch 2627: loss 0.020628
batch 2628: loss 0.020481
batch 2629: loss 0.034400
batch 2630: loss 0.053323
batch 2631: loss 0.139760
batch 2632: loss 0.051428
batch 2633: loss 0.330742
batch 2634: loss 0.221004
batch 2635: loss 0.117882
batch 2636: loss 0.100632
batch 2637: loss 0.061541
batch 2638: loss 0.097311
batch 2639: loss 0.038172
batch 2640: loss 0.118863
batch 2641: loss 0.030213
batch 2642: loss 0.203043
batch 2643: loss 0.054331
batch 2644: loss 0.053619
batch 2645: loss 0.059641
batch 2646: loss 0.058335
batch 2647: loss 0.053327
batch 2648: loss 0.058502
batch 2649: loss 0.122648
batch 2650: loss 0.035477
batch 2651: loss 0.109605
batch 2652: loss 0.142805
batch 2653: loss 0.071935
batch 2654: loss 0.149172
batch 2655: loss 0.120208
batch 2656: loss 0.062261
batch 2657: loss 0.045480
batch 2658: loss 0.161215
batch 2659: loss 0.018196
batch 2660: loss 0.224535
batch 2661: loss 0.049440
batch 2662: loss 0.136129
batch 2663: loss 0.281404
batch 2664: loss 0.055708
batch 2665: loss 0.060052
batch 2666: loss 0.015204
batch 2667: loss 0.040329
batch 2668: loss 0.088053
batch 2669: loss 0.127262
batch 2670: loss 0.153834
batch 2671: loss 0.065424
batch 2672: loss 0.056376
batch 2673: loss 0.157774
batch 2674: loss 0.160465
batch 2675: loss 0.125867
batch 2676: loss 0.127164
batch 2677: loss 0.063175
batch 2678: loss 0.060743
batch 2679: loss 0.164843
batch 2680: loss 0.057850
batch 2681: loss 0.054966
batch 2682: loss 0.032494
batch 2683: loss 0.072478
batch 2684: loss 0.130502
batch 2685: loss 0.218493
batch 2686: loss 0.014327
batch 2687: loss 0.073516
batch 2688: loss 0.097198
batch 2689: loss 0.165445
batch 2690: loss 0.061148
batch 2691: loss 0.009922
batch 2692: loss 0.063239
batch 2693: loss 0.231569
batch 2694: loss 0.196506
batch 2695: loss 0.051624
batch 2696: loss 0.109353
batch 2697: loss 0.123137
batch 2698: loss 0.141707
batch 2699: loss 0.059271
batch 2700: loss 0.106449
batch 2701: loss 0.123462
batch 2702: loss 0.190912
batch 2703: loss 0.035056
batch 2704: loss 0.091108
batch 2705: loss 0.069200
batch 2706: loss 0.066386
batch 2707: loss 0.109529
batch 2708: loss 0.040073
batch 2709: loss 0.026390
batch 2710: loss 0.261490
batch 2711: loss 0.132179
batch 2712: loss 0.017715
batch 2713: loss 0.068546
batch 2714: loss 0.022451
batch 2715: loss 0.229155
batch 2716: loss 0.166324
batch 2717: loss 0.041806
batch 2718: loss 0.017503
batch 2719: loss 0.222007
batch 2720: loss 0.087725
batch 2721: loss 0.082828
batch 2722: loss 0.074638
batch 2723: loss 0.206848
batch 2724: loss 0.162949
batch 2725: loss 0.153386
batch 2726: loss 0.117089
batch 2727: loss 0.044066
batch 2728: loss 0.048277
batch 2729: loss 0.070288
batch 2730: loss 0.069885
batch 2731: loss 0.050774
batch 2732: loss 0.151488
batch 2733: loss 0.100890
batch 2734: loss 0.314152
batch 2735: loss 0.151567
batch 2736: loss 0.058626
batch 2737: loss 0.156589
batch 2738: loss 0.056171
batch 2739: loss 0.034370
batch 2740: loss 0.025779
batch 2741: loss 0.147952
batch 2742: loss 0.189554
batch 2743: loss 0.098857
batch 2744: loss 0.106843
batch 2745: loss 0.029075
batch 2746: loss 0.255149
batch 2747: loss 0.066728
batch 2748: loss 0.114790
batch 2749: loss 0.226938
batch 2750: loss 0.034445
batch 2751: loss 0.083574
batch 2752: loss 0.124418
batch 2753: loss 0.202262
batch 2754: loss 0.054189
batch 2755: loss 0.069401
batch 2756: loss 0.045437
batch 2757: loss 0.054367
batch 2758: loss 0.069197
batch 2759: loss 0.049403
batch 2760: loss 0.091467
batch 2761: loss 0.147495
batch 2762: loss 0.053641
batch 2763: loss 0.126574
batch 2764: loss 0.093131
batch 2765: loss 0.198343
batch 2766: loss 0.140443
batch 2767: loss 0.185746
batch 2768: loss 0.043800
batch 2769: loss 0.113297
batch 2770: loss 0.045543
batch 2771: loss 0.237832
batch 2772: loss 0.051720
batch 2773: loss 0.035765
batch 2774: loss 0.040603
batch 2775: loss 0.031047
batch 2776: loss 0.091546
batch 2777: loss 0.073693
batch 2778: loss 0.147553
batch 2779: loss 0.112706
batch 2780: loss 0.110769
batch 2781: loss 0.029223
batch 2782: loss 0.059538
batch 2783: loss 0.112961
batch 2784: loss 0.065100
batch 2785: loss 0.046241
batch 2786: loss 0.130105
batch 2787: loss 0.040576
batch 2788: loss 0.086965
batch 2789: loss 0.034207
batch 2790: loss 0.035452
batch 2791: loss 0.060949
batch 2792: loss 0.114487
batch 2793: loss 0.053699
batch 2794: loss 0.183460
batch 2795: loss 0.135753
batch 2796: loss 0.103838
batch 2797: loss 0.201988
batch 2798: loss 0.105683
batch 2799: loss 0.135567
batch 2800: loss 0.094224
batch 2801: loss 0.032137
batch 2802: loss 0.140710
batch 2803: loss 0.025757
batch 2804: loss 0.048811
batch 2805: loss 0.175533
batch 2806: loss 0.041633
batch 2807: loss 0.077199
batch 2808: loss 0.138269
batch 2809: loss 0.118726
batch 2810: loss 0.093719
batch 2811: loss 0.221910
batch 2812: loss 0.033974
batch 2813: loss 0.036453
batch 2814: loss 0.048721
batch 2815: loss 0.246333
batch 2816: loss 0.046525
batch 2817: loss 0.088407
batch 2818: loss 0.148720
batch 2819: loss 0.229667
batch 2820: loss 0.094067
batch 2821: loss 0.060017
batch 2822: loss 0.023441
batch 2823: loss 0.040260
batch 2824: loss 0.120533
batch 2825: loss 0.081407
batch 2826: loss 0.042574
batch 2827: loss 0.117059
batch 2828: loss 0.031964
batch 2829: loss 0.219405
batch 2830: loss 0.061379
batch 2831: loss 0.158788
batch 2832: loss 0.199451
batch 2833: loss 0.121607
batch 2834: loss 0.030610
batch 2835: loss 0.193763
batch 2836: loss 0.053679
batch 2837: loss 0.077362
batch 2838: loss 0.023715
batch 2839: loss 0.066217
batch 2840: loss 0.077085
batch 2841: loss 0.111193
batch 2842: loss 0.034469
batch 2843: loss 0.128675
batch 2844: loss 0.014871
batch 2845: loss 0.105816
batch 2846: loss 0.072831
batch 2847: loss 0.049402
batch 2848: loss 0.067355
batch 2849: loss 0.127958
batch 2850: loss 0.022007
batch 2851: loss 0.092668
batch 2852: loss 0.063599
batch 2853: loss 0.015110
batch 2854: loss 0.036652
batch 2855: loss 0.056961
batch 2856: loss 0.101932
batch 2857: loss 0.150158
batch 2858: loss 0.066397
batch 2859: loss 0.033692
batch 2860: loss 0.044363
batch 2861: loss 0.143562
batch 2862: loss 0.164431
batch 2863: loss 0.016209
batch 2864: loss 0.076408
batch 2865: loss 0.174671
batch 2866: loss 0.147806
batch 2867: loss 0.048588
batch 2868: loss 0.140864
batch 2869: loss 0.117751
batch 2870: loss 0.082783
batch 2871: loss 0.048790
batch 2872: loss 0.016817
batch 2873: loss 0.077975
batch 2874: loss 0.127769
batch 2875: loss 0.026229
batch 2876: loss 0.029969
batch 2877: loss 0.052575
batch 2878: loss 0.018094
batch 2879: loss 0.100180
batch 2880: loss 0.104185
batch 2881: loss 0.240619
batch 2882: loss 0.080203
batch 2883: loss 0.060011
batch 2884: loss 0.160973
batch 2885: loss 0.171373
batch 2886: loss 0.047513
batch 2887: loss 0.133097
batch 2888: loss 0.067771
batch 2889: loss 0.146652
batch 2890: loss 0.160037
batch 2891: loss 0.214736
batch 2892: loss 0.223683
batch 2893: loss 0.072658
batch 2894: loss 0.114359
batch 2895: loss 0.118364
batch 2896: loss 0.126207
batch 2897: loss 0.069194
batch 2898: loss 0.076250
batch 2899: loss 0.094734
batch 2900: loss 0.225986
batch 2901: loss 0.087582
batch 2902: loss 0.029486
batch 2903: loss 0.120747
batch 2904: loss 0.171919
batch 2905: loss 0.014408
batch 2906: loss 0.036226
batch 2907: loss 0.037396
batch 2908: loss 0.069799
batch 2909: loss 0.139819
batch 2910: loss 0.146117
batch 2911: loss 0.134537
batch 2912: loss 0.073379
batch 2913: loss 0.017941
batch 2914: loss 0.108830
batch 2915: loss 0.120168
batch 2916: loss 0.057294
batch 2917: loss 0.086308
batch 2918: loss 0.104172
batch 2919: loss 0.098884
batch 2920: loss 0.131789
batch 2921: loss 0.132534
batch 2922: loss 0.127908
batch 2923: loss 0.071703
batch 2924: loss 0.071085
batch 2925: loss 0.027933
batch 2926: loss 0.071104
batch 2927: loss 0.024645
batch 2928: loss 0.035569
batch 2929: loss 0.051919
batch 2930: loss 0.025827
batch 2931: loss 0.032096
batch 2932: loss 0.078134
batch 2933: loss 0.304205
batch 2934: loss 0.131509
batch 2935: loss 0.037503
batch 2936: loss 0.027318
batch 2937: loss 0.137880
batch 2938: loss 0.028879
batch 2939: loss 0.086373
batch 2940: loss 0.024296
batch 2941: loss 0.149536
batch 2942: loss 0.069811
batch 2943: loss 0.108039
batch 2944: loss 0.101495
batch 2945: loss 0.034692
batch 2946: loss 0.057642
batch 2947: loss 0.141645
batch 2948: loss 0.227917
batch 2949: loss 0.198968
batch 2950: loss 0.045659
batch 2951: loss 0.050291
batch 2952: loss 0.287447
batch 2953: loss 0.138536
batch 2954: loss 0.100019
batch 2955: loss 0.066797
batch 2956: loss 0.040532
batch 2957: loss 0.093007
batch 2958: loss 0.127129
batch 2959: loss 0.180827
batch 2960: loss 0.096839
batch 2961: loss 0.010205
batch 2962: loss 0.028896
batch 2963: loss 0.175320
batch 2964: loss 0.082548
batch 2965: loss 0.108984
batch 2966: loss 0.072219
batch 2967: loss 0.101981
batch 2968: loss 0.018508
batch 2969: loss 0.201158
batch 2970: loss 0.109368
batch 2971: loss 0.040149
batch 2972: loss 0.058591
batch 2973: loss 0.089099
batch 2974: loss 0.080765
batch 2975: loss 0.152291
batch 2976: loss 0.061699
batch 2977: loss 0.038602
batch 2978: loss 0.067932
batch 2979: loss 0.113907
batch 2980: loss 0.040154
batch 2981: loss 0.033548
batch 2982: loss 0.066104
batch 2983: loss 0.060751
batch 2984: loss 0.049267
batch 2985: loss 0.034609
batch 2986: loss 0.021304
batch 2987: loss 0.021631
batch 2988: loss 0.018803
batch 2989: loss 0.090298
batch 2990: loss 0.123393
batch 2991: loss 0.147768
batch 2992: loss 0.122571
batch 2993: loss 0.031240
batch 2994: loss 0.071979
batch 2995: loss 0.054207
batch 2996: loss 0.032726
batch 2997: loss 0.010670
batch 2998: loss 0.024553
batch 2999: loss 0.172647
batch 3000: loss 0.016162
batch 3001: loss 0.094054
batch 3002: loss 0.050852
batch 3003: loss 0.160226
batch 3004: loss 0.126904
batch 3005: loss 0.169238
batch 3006: loss 0.060816
batch 3007: loss 0.084097
batch 3008: loss 0.197921
batch 3009: loss 0.054555
batch 3010: loss 0.072899
batch 3011: loss 0.051477
batch 3012: loss 0.142266
batch 3013: loss 0.035635
batch 3014: loss 0.140865
batch 3015: loss 0.041906
batch 3016: loss 0.035038
batch 3017: loss 0.177504
batch 3018: loss 0.202994
batch 3019: loss 0.044963
batch 3020: loss 0.048123
batch 3021: loss 0.112737
batch 3022: loss 0.020455
batch 3023: loss 0.064041
batch 3024: loss 0.125332
batch 3025: loss 0.036608
batch 3026: loss 0.039888
batch 3027: loss 0.081957
batch 3028: loss 0.019873
batch 3029: loss 0.159622
batch 3030: loss 0.106787
batch 3031: loss 0.062571
batch 3032: loss 0.053696
batch 3033: loss 0.043005
batch 3034: loss 0.077571
batch 3035: loss 0.067641
batch 3036: loss 0.030307
batch 3037: loss 0.046041
batch 3038: loss 0.132116
batch 3039: loss 0.019853
batch 3040: loss 0.041464
batch 3041: loss 0.224154
batch 3042: loss 0.058418
batch 3043: loss 0.109427
batch 3044: loss 0.059359
batch 3045: loss 0.036182
batch 3046: loss 0.077949
batch 3047: loss 0.135785
batch 3048: loss 0.023465
batch 3049: loss 0.049200
batch 3050: loss 0.089911
batch 3051: loss 0.042653
batch 3052: loss 0.073625
batch 3053: loss 0.165107
batch 3054: loss 0.045451
batch 3055: loss 0.042677
batch 3056: loss 0.047621
batch 3057: loss 0.084458
batch 3058: loss 0.008700
batch 3059: loss 0.045664
batch 3060: loss 0.035346
batch 3061: loss 0.073541
batch 3062: loss 0.016843
batch 3063: loss 0.233342
batch 3064: loss 0.068986
batch 3065: loss 0.013362
batch 3066: loss 0.175059
batch 3067: loss 0.053394
batch 3068: loss 0.064270
batch 3069: loss 0.089015
batch 3070: loss 0.014639
batch 3071: loss 0.118316
batch 3072: loss 0.123039
batch 3073: loss 0.041319
batch 3074: loss 0.042567
batch 3075: loss 0.036776
batch 3076: loss 0.088550
batch 3077: loss 0.042273
batch 3078: loss 0.076253
batch 3079: loss 0.034210
batch 3080: loss 0.145425
batch 3081: loss 0.042392
batch 3082: loss 0.167557
batch 3083: loss 0.041957
batch 3084: loss 0.076271
batch 3085: loss 0.058199
batch 3086: loss 0.026478
batch 3087: loss 0.017120
batch 3088: loss 0.049630
batch 3089: loss 0.128512
batch 3090: loss 0.093828
batch 3091: loss 0.118385
batch 3092: loss 0.060140
batch 3093: loss 0.046071
batch 3094: loss 0.054762
batch 3095: loss 0.133036
batch 3096: loss 0.049396
batch 3097: loss 0.073196
batch 3098: loss 0.062884
batch 3099: loss 0.184181
batch 3100: loss 0.085176
batch 3101: loss 0.113781
batch 3102: loss 0.059160
batch 3103: loss 0.252820
batch 3104: loss 0.052991
batch 3105: loss 0.162628
batch 3106: loss 0.030153
batch 3107: loss 0.011267
batch 3108: loss 0.064862
batch 3109: loss 0.071609
batch 3110: loss 0.013007
batch 3111: loss 0.131425
batch 3112: loss 0.042067
batch 3113: loss 0.274472
batch 3114: loss 0.026204
batch 3115: loss 0.123130
batch 3116: loss 0.088673
batch 3117: loss 0.116034
batch 3118: loss 0.047998
batch 3119: loss 0.110048
batch 3120: loss 0.111168
batch 3121: loss 0.063610
batch 3122: loss 0.033242
batch 3123: loss 0.075100
batch 3124: loss 0.089099
batch 3125: loss 0.052805
batch 3126: loss 0.068871
batch 3127: loss 0.030379
batch 3128: loss 0.052955
batch 3129: loss 0.067916
batch 3130: loss 0.034722
batch 3131: loss 0.092810
batch 3132: loss 0.105122
batch 3133: loss 0.036819
batch 3134: loss 0.105110
batch 3135: loss 0.027324
batch 3136: loss 0.092217
batch 3137: loss 0.177884
batch 3138: loss 0.027181
batch 3139: loss 0.208275
batch 3140: loss 0.063032
batch 3141: loss 0.218680
batch 3142: loss 0.199485
batch 3143: loss 0.050511
batch 3144: loss 0.117661
batch 3145: loss 0.155156
batch 3146: loss 0.042918
batch 3147: loss 0.078106
batch 3148: loss 0.018503
batch 3149: loss 0.027181
batch 3150: loss 0.060958
batch 3151: loss 0.015746
batch 3152: loss 0.079343
batch 3153: loss 0.062721
batch 3154: loss 0.022427
batch 3155: loss 0.064720
batch 3156: loss 0.066936
batch 3157: loss 0.072067
batch 3158: loss 0.089947
batch 3159: loss 0.094338
batch 3160: loss 0.219249
batch 3161: loss 0.148604
batch 3162: loss 0.036357
batch 3163: loss 0.055262
batch 3164: loss 0.139171
batch 3165: loss 0.054798
batch 3166: loss 0.022787
batch 3167: loss 0.021684
batch 3168: loss 0.061234
batch 3169: loss 0.052468
batch 3170: loss 0.027093
batch 3171: loss 0.120282
batch 3172: loss 0.060609
batch 3173: loss 0.041195
batch 3174: loss 0.132234
batch 3175: loss 0.170634
batch 3176: loss 0.020329
batch 3177: loss 0.042300
batch 3178: loss 0.084389
batch 3179: loss 0.064275
batch 3180: loss 0.081518
batch 3181: loss 0.098802
batch 3182: loss 0.255707
batch 3183: loss 0.088179
batch 3184: loss 0.051717
batch 3185: loss 0.049906
batch 3186: loss 0.063391
batch 3187: loss 0.016028
batch 3188: loss 0.047303
batch 3189: loss 0.200987
batch 3190: loss 0.272412
batch 3191: loss 0.158576
batch 3192: loss 0.118790
batch 3193: loss 0.048423
batch 3194: loss 0.024103
batch 3195: loss 0.052175
batch 3196: loss 0.100330
batch 3197: loss 0.026925
batch 3198: loss 0.133947
batch 3199: loss 0.120849
batch 3200: loss 0.042929
batch 3201: loss 0.043758
batch 3202: loss 0.031292
batch 3203: loss 0.091013
batch 3204: loss 0.186015
batch 3205: loss 0.104934
batch 3206: loss 0.058089
batch 3207: loss 0.079310
batch 3208: loss 0.096896
batch 3209: loss 0.052359
batch 3210: loss 0.056829
batch 3211: loss 0.196980
batch 3212: loss 0.021778
batch 3213: loss 0.077032
batch 3214: loss 0.014343
batch 3215: loss 0.046921
batch 3216: loss 0.023875
batch 3217: loss 0.008695
batch 3218: loss 0.025473
batch 3219: loss 0.046102
batch 3220: loss 0.020507
batch 3221: loss 0.254781
batch 3222: loss 0.089769
batch 3223: loss 0.038528
batch 3224: loss 0.201474
batch 3225: loss 0.054082
batch 3226: loss 0.157739
batch 3227: loss 0.038076
batch 3228: loss 0.148559
batch 3229: loss 0.063494
batch 3230: loss 0.084657
batch 3231: loss 0.046188
batch 3232: loss 0.049497
batch 3233: loss 0.024607
batch 3234: loss 0.026146
batch 3235: loss 0.078845
batch 3236: loss 0.065707
batch 3237: loss 0.035892
batch 3238: loss 0.156410
batch 3239: loss 0.135309
batch 3240: loss 0.058482
batch 3241: loss 0.066217
batch 3242: loss 0.084654
batch 3243: loss 0.155717
batch 3244: loss 0.075748
batch 3245: loss 0.126834
batch 3246: loss 0.056946
batch 3247: loss 0.121869
batch 3248: loss 0.106531
batch 3249: loss 0.104069
batch 3250: loss 0.023605
batch 3251: loss 0.045167
batch 3252: loss 0.173588
batch 3253: loss 0.021286
batch 3254: loss 0.038854
batch 3255: loss 0.041912
batch 3256: loss 0.069858
batch 3257: loss 0.041463
batch 3258: loss 0.046608
batch 3259: loss 0.018010
batch 3260: loss 0.071883
batch 3261: loss 0.195806
batch 3262: loss 0.062334
batch 3263: loss 0.101246
batch 3264: loss 0.132293
batch 3265: loss 0.135344
batch 3266: loss 0.111892
batch 3267: loss 0.029102
batch 3268: loss 0.019910
batch 3269: loss 0.216288
batch 3270: loss 0.020685
batch 3271: loss 0.182845
batch 3272: loss 0.069528
batch 3273: loss 0.022999
batch 3274: loss 0.023696
batch 3275: loss 0.214639
batch 3276: loss 0.116951
batch 3277: loss 0.191142
batch 3278: loss 0.191913
batch 3279: loss 0.104663
batch 3280: loss 0.059542
batch 3281: loss 0.090233
batch 3282: loss 0.124980
batch 3283: loss 0.121909
batch 3284: loss 0.194358
batch 3285: loss 0.066071
batch 3286: loss 0.135988
batch 3287: loss 0.016971
batch 3288: loss 0.010091
batch 3289: loss 0.062276
batch 3290: loss 0.028592
batch 3291: loss 0.048167
batch 3292: loss 0.065406
batch 3293: loss 0.043364
batch 3294: loss 0.048738
batch 3295: loss 0.061108
batch 3296: loss 0.172177
batch 3297: loss 0.060781
batch 3298: loss 0.257636
batch 3299: loss 0.114497
batch 3300: loss 0.132669
batch 3301: loss 0.040140
batch 3302: loss 0.042503
batch 3303: loss 0.008979
batch 3304: loss 0.032271
batch 3305: loss 0.182318
batch 3306: loss 0.090444
batch 3307: loss 0.076494
batch 3308: loss 0.155983
batch 3309: loss 0.122372
batch 3310: loss 0.158974
batch 3311: loss 0.035308
batch 3312: loss 0.047203
batch 3313: loss 0.159616
batch 3314: loss 0.090613
batch 3315: loss 0.077015
batch 3316: loss 0.034314
batch 3317: loss 0.069029
batch 3318: loss 0.028625
batch 3319: loss 0.059710
batch 3320: loss 0.029678
batch 3321: loss 0.064543
batch 3322: loss 0.059141
batch 3323: loss 0.073866
batch 3324: loss 0.080895
batch 3325: loss 0.083603
batch 3326: loss 0.073075
batch 3327: loss 0.050972
batch 3328: loss 0.259670
batch 3329: loss 0.028056
batch 3330: loss 0.029354
batch 3331: loss 0.144302
batch 3332: loss 0.045380
batch 3333: loss 0.036232
batch 3334: loss 0.097870
batch 3335: loss 0.053138
batch 3336: loss 0.108772
batch 3337: loss 0.095967
batch 3338: loss 0.195734
batch 3339: loss 0.178373
batch 3340: loss 0.052158
batch 3341: loss 0.069854
batch 3342: loss 0.049561
batch 3343: loss 0.016180
batch 3344: loss 0.049316
batch 3345: loss 0.098863
batch 3346: loss 0.072421
batch 3347: loss 0.080588
batch 3348: loss 0.019947
batch 3349: loss 0.025300
batch 3350: loss 0.041366
batch 3351: loss 0.212370
batch 3352: loss 0.179548
batch 3353: loss 0.094389
batch 3354: loss 0.192425
batch 3355: loss 0.160173
batch 3356: loss 0.034010
batch 3357: loss 0.122845
batch 3358: loss 0.049153
batch 3359: loss 0.126327
batch 3360: loss 0.009727
batch 3361: loss 0.023561
batch 3362: loss 0.076271
batch 3363: loss 0.017475
batch 3364: loss 0.135514
batch 3365: loss 0.067674
batch 3366: loss 0.022927
batch 3367: loss 0.039637
batch 3368: loss 0.103678
batch 3369: loss 0.132143
batch 3370: loss 0.065126
batch 3371: loss 0.101989
batch 3372: loss 0.034707
batch 3373: loss 0.071045
batch 3374: loss 0.078118
batch 3375: loss 0.013556
batch 3376: loss 0.095852
batch 3377: loss 0.015387
batch 3378: loss 0.134044
batch 3379: loss 0.026116
batch 3380: loss 0.024939
batch 3381: loss 0.017225
batch 3382: loss 0.064240
batch 3383: loss 0.232438
batch 3384: loss 0.039557
batch 3385: loss 0.021858
batch 3386: loss 0.074469
batch 3387: loss 0.026312
batch 3388: loss 0.119216
batch 3389: loss 0.107057
batch 3390: loss 0.014083
batch 3391: loss 0.019024
batch 3392: loss 0.251078
batch 3393: loss 0.008355
batch 3394: loss 0.184119
batch 3395: loss 0.115170
batch 3396: loss 0.082333
batch 3397: loss 0.113000
batch 3398: loss 0.139303
batch 3399: loss 0.194184
batch 3400: loss 0.084802
batch 3401: loss 0.182492
batch 3402: loss 0.153221
batch 3403: loss 0.167210
batch 3404: loss 0.126744
batch 3405: loss 0.055653
batch 3406: loss 0.036710
batch 3407: loss 0.091294
batch 3408: loss 0.081764
batch 3409: loss 0.183760
batch 3410: loss 0.019310
batch 3411: loss 0.022952
batch 3412: loss 0.022466
batch 3413: loss 0.069284
batch 3414: loss 0.063209
batch 3415: loss 0.237051
batch 3416: loss 0.047917
batch 3417: loss 0.044946
batch 3418: loss 0.056190
batch 3419: loss 0.113780
batch 3420: loss 0.097001
batch 3421: loss 0.048318
batch 3422: loss 0.071907
batch 3423: loss 0.034713
batch 3424: loss 0.088933
batch 3425: loss 0.169611
batch 3426: loss 0.035414
batch 3427: loss 0.045204
batch 3428: loss 0.019068
batch 3429: loss 0.016449
batch 3430: loss 0.120069
batch 3431: loss 0.027161
batch 3432: loss 0.109154
batch 3433: loss 0.124476
batch 3434: loss 0.281493
batch 3435: loss 0.048781
batch 3436: loss 0.103384
batch 3437: loss 0.192719
batch 3438: loss 0.075531
batch 3439: loss 0.035738
batch 3440: loss 0.146846
batch 3441: loss 0.120652
batch 3442: loss 0.195040
batch 3443: loss 0.021982
batch 3444: loss 0.098390
batch 3445: loss 0.148201
batch 3446: loss 0.121476
batch 3447: loss 0.158515
batch 3448: loss 0.091949
batch 3449: loss 0.039842
batch 3450: loss 0.200253
batch 3451: loss 0.099511
batch 3452: loss 0.029318
batch 3453: loss 0.090771
batch 3454: loss 0.237766
batch 3455: loss 0.073147
batch 3456: loss 0.033224
batch 3457: loss 0.141807
batch 3458: loss 0.055900
batch 3459: loss 0.103876
batch 3460: loss 0.142997
batch 3461: loss 0.081141
batch 3462: loss 0.201085
batch 3463: loss 0.206525
batch 3464: loss 0.056422
batch 3465: loss 0.083155
batch 3466: loss 0.164938
batch 3467: loss 0.069068
batch 3468: loss 0.047836
batch 3469: loss 0.077531
batch 3470: loss 0.054873
batch 3471: loss 0.193195
batch 3472: loss 0.017711
batch 3473: loss 0.092858
batch 3474: loss 0.084633
batch 3475: loss 0.122767
batch 3476: loss 0.200599
batch 3477: loss 0.037120
batch 3478: loss 0.031879
batch 3479: loss 0.128076
batch 3480: loss 0.032407
batch 3481: loss 0.055305
batch 3482: loss 0.079066
batch 3483: loss 0.180599
batch 3484: loss 0.080631
batch 3485: loss 0.056342
batch 3486: loss 0.144444
batch 3487: loss 0.049738
batch 3488: loss 0.086109
batch 3489: loss 0.037311
batch 3490: loss 0.119130
batch 3491: loss 0.153305
batch 3492: loss 0.145259
batch 3493: loss 0.016717
batch 3494: loss 0.082472
batch 3495: loss 0.161581
batch 3496: loss 0.044009
batch 3497: loss 0.025144
batch 3498: loss 0.048358
batch 3499: loss 0.029275
batch 3500: loss 0.048358
batch 3501: loss 0.051142
batch 3502: loss 0.071461
batch 3503: loss 0.023508
batch 3504: loss 0.189850
batch 3505: loss 0.066643
batch 3506: loss 0.103992
batch 3507: loss 0.066446
batch 3508: loss 0.283058
batch 3509: loss 0.078081
batch 3510: loss 0.034566
batch 3511: loss 0.058186
batch 3512: loss 0.183896
batch 3513: loss 0.077557
batch 3514: loss 0.109903
batch 3515: loss 0.108135
batch 3516: loss 0.015999
batch 3517: loss 0.033098
batch 3518: loss 0.036819
batch 3519: loss 0.095139
batch 3520: loss 0.131633
batch 3521: loss 0.010553
batch 3522: loss 0.131866
batch 3523: loss 0.145421
batch 3524: loss 0.055233
batch 3525: loss 0.144425
batch 3526: loss 0.113122
batch 3527: loss 0.196668
batch 3528: loss 0.085435
batch 3529: loss 0.114491
batch 3530: loss 0.031560
batch 3531: loss 0.033445
batch 3532: loss 0.067673
batch 3533: loss 0.071394
batch 3534: loss 0.034131
batch 3535: loss 0.045909
batch 3536: loss 0.168238
batch 3537: loss 0.018725
batch 3538: loss 0.186275
batch 3539: loss 0.187012
batch 3540: loss 0.026744
batch 3541: loss 0.010031
batch 3542: loss 0.180491
batch 3543: loss 0.063098
batch 3544: loss 0.251621
batch 3545: loss 0.055422
batch 3546: loss 0.097997
batch 3547: loss 0.092187
batch 3548: loss 0.032802
batch 3549: loss 0.098702
batch 3550: loss 0.047430
batch 3551: loss 0.062799
batch 3552: loss 0.057412
batch 3553: loss 0.056566
batch 3554: loss 0.043177
batch 3555: loss 0.033105
batch 3556: loss 0.200998
batch 3557: loss 0.149770
batch 3558: loss 0.060750
batch 3559: loss 0.158443
batch 3560: loss 0.016642
batch 3561: loss 0.034679
batch 3562: loss 0.224738
batch 3563: loss 0.049488
batch 3564: loss 0.043503
batch 3565: loss 0.016763
batch 3566: loss 0.043537
batch 3567: loss 0.026693
batch 3568: loss 0.085768
batch 3569: loss 0.132269
batch 3570: loss 0.214066
batch 3571: loss 0.039194
batch 3572: loss 0.078755
batch 3573: loss 0.060788
batch 3574: loss 0.052459
batch 3575: loss 0.113728
batch 3576: loss 0.131991
batch 3577: loss 0.129502
batch 3578: loss 0.063211
batch 3579: loss 0.032442
batch 3580: loss 0.014267
batch 3581: loss 0.013332
batch 3582: loss 0.069659
batch 3583: loss 0.032774
batch 3584: loss 0.061474
batch 3585: loss 0.036999
batch 3586: loss 0.043291
batch 3587: loss 0.060758
batch 3588: loss 0.107970
batch 3589: loss 0.013378
batch 3590: loss 0.035633
batch 3591: loss 0.049024
batch 3592: loss 0.250083
batch 3593: loss 0.156941
batch 3594: loss 0.247165
batch 3595: loss 0.039757
batch 3596: loss 0.299551
batch 3597: loss 0.227633
batch 3598: loss 0.102178
batch 3599: loss 0.040544
batch 3600: loss 0.033012
batch 3601: loss 0.070142
batch 3602: loss 0.053532
batch 3603: loss 0.056480
batch 3604: loss 0.032481
batch 3605: loss 0.054660
batch 3606: loss 0.076942
batch 3607: loss 0.074108
batch 3608: loss 0.065328
batch 3609: loss 0.058044
batch 3610: loss 0.083769
batch 3611: loss 0.076121
batch 3612: loss 0.188364
batch 3613: loss 0.039273
batch 3614: loss 0.028833
batch 3615: loss 0.065790
batch 3616: loss 0.134769
batch 3617: loss 0.112542
batch 3618: loss 0.066866
batch 3619: loss 0.015732
batch 3620: loss 0.092364
batch 3621: loss 0.017026
batch 3622: loss 0.018236
batch 3623: loss 0.106736
batch 3624: loss 0.139937
batch 3625: loss 0.044240
batch 3626: loss 0.026460
batch 3627: loss 0.024805
batch 3628: loss 0.107343
batch 3629: loss 0.021346
batch 3630: loss 0.039589
batch 3631: loss 0.017557
batch 3632: loss 0.052593
batch 3633: loss 0.022290
batch 3634: loss 0.087310
batch 3635: loss 0.055296
batch 3636: loss 0.038513
batch 3637: loss 0.015594
batch 3638: loss 0.117865
batch 3639: loss 0.068039
batch 3640: loss 0.035969
batch 3641: loss 0.025177
batch 3642: loss 0.043916
batch 3643: loss 0.080175
batch 3644: loss 0.118525
batch 3645: loss 0.099121
batch 3646: loss 0.041454
batch 3647: loss 0.086432
batch 3648: loss 0.104241
batch 3649: loss 0.094710
batch 3650: loss 0.100071
batch 3651: loss 0.030274
batch 3652: loss 0.058616
batch 3653: loss 0.059503
batch 3654: loss 0.153517
batch 3655: loss 0.056617
batch 3656: loss 0.062895
batch 3657: loss 0.169437
batch 3658: loss 0.145941
batch 3659: loss 0.036861
batch 3660: loss 0.168873
batch 3661: loss 0.176380
batch 3662: loss 0.071006
batch 3663: loss 0.022692
batch 3664: loss 0.086791
batch 3665: loss 0.058497
batch 3666: loss 0.016474
batch 3667: loss 0.072316
batch 3668: loss 0.037830
batch 3669: loss 0.087887
batch 3670: loss 0.057143
batch 3671: loss 0.041221
batch 3672: loss 0.158496
batch 3673: loss 0.133228
batch 3674: loss 0.017608
batch 3675: loss 0.032679
batch 3676: loss 0.039006
batch 3677: loss 0.015280
batch 3678: loss 0.041864
batch 3679: loss 0.073312
batch 3680: loss 0.109319
batch 3681: loss 0.028197
batch 3682: loss 0.099594
batch 3683: loss 0.013919
batch 3684: loss 0.073064
batch 3685: loss 0.047669
batch 3686: loss 0.037515
batch 3687: loss 0.048307
batch 3688: loss 0.028034
batch 3689: loss 0.025031
batch 3690: loss 0.019115
batch 3691: loss 0.015701
batch 3692: loss 0.031698
batch 3693: loss 0.039488
batch 3694: loss 0.036359
batch 3695: loss 0.056773
batch 3696: loss 0.079049
batch 3697: loss 0.104650
batch 3698: loss 0.014753
batch 3699: loss 0.004488
batch 3700: loss 0.060892
batch 3701: loss 0.032059
batch 3702: loss 0.044741
batch 3703: loss 0.050605
batch 3704: loss 0.042644
batch 3705: loss 0.067575
batch 3706: loss 0.133208
batch 3707: loss 0.129810
batch 3708: loss 0.055719
batch 3709: loss 0.099972
batch 3710: loss 0.020411
batch 3711: loss 0.025230
batch 3712: loss 0.032557
batch 3713: loss 0.193336
batch 3714: loss 0.034531
batch 3715: loss 0.022045
batch 3716: loss 0.063424
batch 3717: loss 0.088888
batch 3718: loss 0.239848
batch 3719: loss 0.072175
batch 3720: loss 0.102680
batch 3721: loss 0.037626
batch 3722: loss 0.083646
batch 3723: loss 0.022967
batch 3724: loss 0.044112
batch 3725: loss 0.442587
batch 3726: loss 0.164225
batch 3727: loss 0.115515
batch 3728: loss 0.050101
batch 3729: loss 0.112223
batch 3730: loss 0.073576
batch 3731: loss 0.097908
batch 3732: loss 0.115102
batch 3733: loss 0.028155
batch 3734: loss 0.035561
batch 3735: loss 0.094800
batch 3736: loss 0.029041
batch 3737: loss 0.200682
batch 3738: loss 0.085091
batch 3739: loss 0.100567
batch 3740: loss 0.059829
batch 3741: loss 0.132616
batch 3742: loss 0.133785
batch 3743: loss 0.007812
batch 3744: loss 0.073601
batch 3745: loss 0.111710
batch 3746: loss 0.066905
batch 3747: loss 0.034427
batch 3748: loss 0.022554
batch 3749: loss 0.063855
batch 3750: loss 0.050146
batch 3751: loss 0.099086
batch 3752: loss 0.033884
batch 3753: loss 0.039453
batch 3754: loss 0.061725
batch 3755: loss 0.068948
batch 3756: loss 0.016616
batch 3757: loss 0.062654
batch 3758: loss 0.018078
batch 3759: loss 0.061073
batch 3760: loss 0.055996
batch 3761: loss 0.023092
batch 3762: loss 0.035831
batch 3763: loss 0.033686
batch 3764: loss 0.120032
batch 3765: loss 0.125890
batch 3766: loss 0.036956
batch 3767: loss 0.063958
batch 3768: loss 0.177221
batch 3769: loss 0.052395
batch 3770: loss 0.028789
batch 3771: loss 0.036930
batch 3772: loss 0.099067
batch 3773: loss 0.203606
batch 3774: loss 0.096664
batch 3775: loss 0.038905
batch 3776: loss 0.095390
batch 3777: loss 0.045465
batch 3778: loss 0.031935
batch 3779: loss 0.056117
batch 3780: loss 0.097239
batch 3781: loss 0.050190
batch 3782: loss 0.139106
batch 3783: loss 0.087736
batch 3784: loss 0.058375
batch 3785: loss 0.043586
batch 3786: loss 0.065183
batch 3787: loss 0.084331
batch 3788: loss 0.045191
batch 3789: loss 0.039591
batch 3790: loss 0.130626
batch 3791: loss 0.073615
batch 3792: loss 0.232690
batch 3793: loss 0.082146
batch 3794: loss 0.060869
batch 3795: loss 0.089446
batch 3796: loss 0.038654
batch 3797: loss 0.263912
batch 3798: loss 0.126201
batch 3799: loss 0.048698
batch 3800: loss 0.023861
batch 3801: loss 0.104328
batch 3802: loss 0.048551
batch 3803: loss 0.096467
batch 3804: loss 0.028295
batch 3805: loss 0.118229
batch 3806: loss 0.034454
batch 3807: loss 0.045606
batch 3808: loss 0.090843
batch 3809: loss 0.022815
batch 3810: loss 0.098546
batch 3811: loss 0.223701
batch 3812: loss 0.041131
batch 3813: loss 0.066936
batch 3814: loss 0.030754
batch 3815: loss 0.117744
batch 3816: loss 0.026638
batch 3817: loss 0.035825
batch 3818: loss 0.061398
batch 3819: loss 0.084128
batch 3820: loss 0.060328
batch 3821: loss 0.015553
batch 3822: loss 0.031687
batch 3823: loss 0.117905
batch 3824: loss 0.032522
batch 3825: loss 0.025695
batch 3826: loss 0.054360
batch 3827: loss 0.054804
batch 3828: loss 0.108450
batch 3829: loss 0.133967
batch 3830: loss 0.031336
batch 3831: loss 0.016750
batch 3832: loss 0.046737
batch 3833: loss 0.047111
batch 3834: loss 0.059076
batch 3835: loss 0.018647
batch 3836: loss 0.163024
batch 3837: loss 0.082595
batch 3838: loss 0.125525
batch 3839: loss 0.057641
batch 3840: loss 0.154439
batch 3841: loss 0.012756
batch 3842: loss 0.064767
batch 3843: loss 0.013256
batch 3844: loss 0.048813
batch 3845: loss 0.075208
batch 3846: loss 0.062405
batch 3847: loss 0.043332
batch 3848: loss 0.038185
batch 3849: loss 0.083498
batch 3850: loss 0.250468
batch 3851: loss 0.111343
batch 3852: loss 0.131680
batch 3853: loss 0.015853
batch 3854: loss 0.034401
batch 3855: loss 0.079763
batch 3856: loss 0.062515
batch 3857: loss 0.016449
batch 3858: loss 0.052742
batch 3859: loss 0.108266
batch 3860: loss 0.102539
batch 3861: loss 0.078697
batch 3862: loss 0.131316
batch 3863: loss 0.112478
batch 3864: loss 0.079659
batch 3865: loss 0.080872
batch 3866: loss 0.010819
batch 3867: loss 0.028492
batch 3868: loss 0.032985
batch 3869: loss 0.043389
batch 3870: loss 0.077653
batch 3871: loss 0.074920
batch 3872: loss 0.063141
batch 3873: loss 0.130628
batch 3874: loss 0.129113
batch 3875: loss 0.013510
batch 3876: loss 0.028198
batch 3877: loss 0.028375
batch 3878: loss 0.193375
batch 3879: loss 0.288241
batch 3880: loss 0.112753
batch 3881: loss 0.025391
batch 3882: loss 0.033296
batch 3883: loss 0.083863
batch 3884: loss 0.054361
batch 3885: loss 0.026203
batch 3886: loss 0.024399
batch 3887: loss 0.039314
batch 3888: loss 0.119826
batch 3889: loss 0.033480
batch 3890: loss 0.057795
batch 3891: loss 0.037860
batch 3892: loss 0.050205
batch 3893: loss 0.036694
batch 3894: loss 0.076026
batch 3895: loss 0.033926
batch 3896: loss 0.094258
batch 3897: loss 0.351842
batch 3898: loss 0.018569
batch 3899: loss 0.090539
batch 3900: loss 0.059572
batch 3901: loss 0.031402
batch 3902: loss 0.091676
batch 3903: loss 0.080421
batch 3904: loss 0.060037
batch 3905: loss 0.042643
batch 3906: loss 0.071234
batch 3907: loss 0.026966
batch 3908: loss 0.100806
batch 3909: loss 0.028551
batch 3910: loss 0.016212
batch 3911: loss 0.149068
batch 3912: loss 0.287697
batch 3913: loss 0.021705
batch 3914: loss 0.068270
batch 3915: loss 0.121027
batch 3916: loss 0.026349
batch 3917: loss 0.213738
batch 3918: loss 0.092184
batch 3919: loss 0.025430
batch 3920: loss 0.057901
batch 3921: loss 0.030061
batch 3922: loss 0.052916
batch 3923: loss 0.020877
batch 3924: loss 0.063215
batch 3925: loss 0.055653
batch 3926: loss 0.042590
batch 3927: loss 0.044100
batch 3928: loss 0.160007
batch 3929: loss 0.088986
batch 3930: loss 0.011925
batch 3931: loss 0.046432
batch 3932: loss 0.111305
batch 3933: loss 0.035642
batch 3934: loss 0.071927
batch 3935: loss 0.077100
batch 3936: loss 0.065505
batch 3937: loss 0.019960
batch 3938: loss 0.083013
batch 3939: loss 0.151002
batch 3940: loss 0.081325
batch 3941: loss 0.067025
batch 3942: loss 0.010614
batch 3943: loss 0.107649
batch 3944: loss 0.049255
batch 3945: loss 0.016715
batch 3946: loss 0.031382
batch 3947: loss 0.019847
batch 3948: loss 0.149461
batch 3949: loss 0.092014
batch 3950: loss 0.018382
batch 3951: loss 0.051366
batch 3952: loss 0.073470
batch 3953: loss 0.178826
batch 3954: loss 0.123364
batch 3955: loss 0.061792
batch 3956: loss 0.034720
batch 3957: loss 0.025862
batch 3958: loss 0.015723
batch 3959: loss 0.026100
batch 3960: loss 0.063927
batch 3961: loss 0.022783
batch 3962: loss 0.017627
batch 3963: loss 0.017300
batch 3964: loss 0.034102
batch 3965: loss 0.098722
batch 3966: loss 0.028329
batch 3967: loss 0.017111
batch 3968: loss 0.164212
batch 3969: loss 0.108116
batch 3970: loss 0.010743
batch 3971: loss 0.042277
batch 3972: loss 0.009545
batch 3973: loss 0.060492
batch 3974: loss 0.017138
batch 3975: loss 0.044878
batch 3976: loss 0.198532
batch 3977: loss 0.045665
batch 3978: loss 0.089341
batch 3979: loss 0.165601
batch 3980: loss 0.084999
batch 3981: loss 0.028329
batch 3982: loss 0.079247
batch 3983: loss 0.047072
batch 3984: loss 0.021180
batch 3985: loss 0.092807
batch 3986: loss 0.039425
batch 3987: loss 0.032426
batch 3988: loss 0.039770
batch 3989: loss 0.049432
batch 3990: loss 0.045745
batch 3991: loss 0.105015
batch 3992: loss 0.036190
batch 3993: loss 0.051569
batch 3994: loss 0.030486
batch 3995: loss 0.031147
batch 3996: loss 0.051694
batch 3997: loss 0.078052
batch 3998: loss 0.018037
batch 3999: loss 0.063878
batch 4000: loss 0.107369
batch 4001: loss 0.017058
batch 4002: loss 0.065315
batch 4003: loss 0.133365
batch 4004: loss 0.076809
batch 4005: loss 0.075350
batch 4006: loss 0.021314
batch 4007: loss 0.120227
batch 4008: loss 0.089818
batch 4009: loss 0.027170
batch 4010: loss 0.110535
batch 4011: loss 0.104919
batch 4012: loss 0.086921
batch 4013: loss 0.063221
batch 4014: loss 0.240878
batch 4015: loss 0.034082
batch 4016: loss 0.068278
batch 4017: loss 0.019798
batch 4018: loss 0.061941
batch 4019: loss 0.107186
batch 4020: loss 0.067445
batch 4021: loss 0.024847
batch 4022: loss 0.062375
batch 4023: loss 0.040711
batch 4024: loss 0.088344
batch 4025: loss 0.043186
batch 4026: loss 0.136265
batch 4027: loss 0.009737
batch 4028: loss 0.046232
batch 4029: loss 0.068902
batch 4030: loss 0.240814
batch 4031: loss 0.094619
batch 4032: loss 0.021039
batch 4033: loss 0.029039
batch 4034: loss 0.160625
batch 4035: loss 0.083235
batch 4036: loss 0.144590
batch 4037: loss 0.024796
batch 4038: loss 0.043132
batch 4039: loss 0.039560
batch 4040: loss 0.024882
batch 4041: loss 0.047459
batch 4042: loss 0.165861
batch 4043: loss 0.018224
batch 4044: loss 0.045064
batch 4045: loss 0.282988
batch 4046: loss 0.029581
batch 4047: loss 0.033211
batch 4048: loss 0.085410
batch 4049: loss 0.018318
batch 4050: loss 0.009715
batch 4051: loss 0.015749
batch 4052: loss 0.074448
batch 4053: loss 0.027714
batch 4054: loss 0.151940
batch 4055: loss 0.078842
batch 4056: loss 0.048110
batch 4057: loss 0.013550
batch 4058: loss 0.071166
batch 4059: loss 0.025840
batch 4060: loss 0.010592
batch 4061: loss 0.062287
batch 4062: loss 0.031426
batch 4063: loss 0.095684
batch 4064: loss 0.148020
batch 4065: loss 0.055006
batch 4066: loss 0.096351
batch 4067: loss 0.104462
batch 4068: loss 0.033827
batch 4069: loss 0.025762
batch 4070: loss 0.030143
batch 4071: loss 0.050860
batch 4072: loss 0.035617
batch 4073: loss 0.023989
batch 4074: loss 0.146956
batch 4075: loss 0.068437
batch 4076: loss 0.124822
batch 4077: loss 0.069257
batch 4078: loss 0.083250
batch 4079: loss 0.014233
batch 4080: loss 0.152549
batch 4081: loss 0.077747
batch 4082: loss 0.064525
batch 4083: loss 0.055905
batch 4084: loss 0.116295
batch 4085: loss 0.023283
batch 4086: loss 0.013147
batch 4087: loss 0.016212
batch 4088: loss 0.040418
batch 4089: loss 0.040283
batch 4090: loss 0.115233
batch 4091: loss 0.023286
batch 4092: loss 0.034804
batch 4093: loss 0.011996
batch 4094: loss 0.021648
batch 4095: loss 0.040073
batch 4096: loss 0.040808
batch 4097: loss 0.043609
batch 4098: loss 0.025492
batch 4099: loss 0.029313
batch 4100: loss 0.012574
batch 4101: loss 0.038563
batch 4102: loss 0.036337
batch 4103: loss 0.062084
batch 4104: loss 0.103595
batch 4105: loss 0.011554
batch 4106: loss 0.110331
batch 4107: loss 0.202486
batch 4108: loss 0.156625
batch 4109: loss 0.080408
batch 4110: loss 0.080215
batch 4111: loss 0.035383
batch 4112: loss 0.018022
batch 4113: loss 0.023502
batch 4114: loss 0.243215
batch 4115: loss 0.051569
batch 4116: loss 0.005764
batch 4117: loss 0.037011
batch 4118: loss 0.016080
batch 4119: loss 0.044110
batch 4120: loss 0.037052
batch 4121: loss 0.027043
batch 4122: loss 0.051199
batch 4123: loss 0.056908
batch 4124: loss 0.058967
batch 4125: loss 0.168250
batch 4126: loss 0.043312
batch 4127: loss 0.096739
batch 4128: loss 0.081905
batch 4129: loss 0.069751
batch 4130: loss 0.019592
batch 4131: loss 0.011159
batch 4132: loss 0.056865
batch 4133: loss 0.067476
batch 4134: loss 0.016793
batch 4135: loss 0.073278
batch 4136: loss 0.260357
batch 4137: loss 0.036116
batch 4138: loss 0.040832
batch 4139: loss 0.069383
batch 4140: loss 0.039017
batch 4141: loss 0.134644
batch 4142: loss 0.218052
batch 4143: loss 0.052090
batch 4144: loss 0.033593
batch 4145: loss 0.031694
batch 4146: loss 0.012201
batch 4147: loss 0.142814
batch 4148: loss 0.038096
batch 4149: loss 0.169943
batch 4150: loss 0.202975
batch 4151: loss 0.103145
batch 4152: loss 0.033626
batch 4153: loss 0.022346
batch 4154: loss 0.062512
batch 4155: loss 0.020932
batch 4156: loss 0.057397
batch 4157: loss 0.028983
batch 4158: loss 0.078143
batch 4159: loss 0.009809
batch 4160: loss 0.059932
batch 4161: loss 0.018184
batch 4162: loss 0.027542
batch 4163: loss 0.024069
batch 4164: loss 0.032677
batch 4165: loss 0.031462
batch 4166: loss 0.030570
batch 4167: loss 0.044136
batch 4168: loss 0.009922
batch 4169: loss 0.100120
batch 4170: loss 0.024348
batch 4171: loss 0.049505
batch 4172: loss 0.116061
batch 4173: loss 0.070022
batch 4174: loss 0.073901
batch 4175: loss 0.094977
batch 4176: loss 0.043858
batch 4177: loss 0.027354
batch 4178: loss 0.093270
batch 4179: loss 0.027614
batch 4180: loss 0.110144
batch 4181: loss 0.078291
batch 4182: loss 0.116596
batch 4183: loss 0.094379
batch 4184: loss 0.079861
batch 4185: loss 0.061025
batch 4186: loss 0.052278
batch 4187: loss 0.068470
batch 4188: loss 0.036438
batch 4189: loss 0.121193
batch 4190: loss 0.066411
batch 4191: loss 0.037054
batch 4192: loss 0.033539
batch 4193: loss 0.041911
batch 4194: loss 0.012619
batch 4195: loss 0.121263
batch 4196: loss 0.028694
batch 4197: loss 0.034576
batch 4198: loss 0.031181
batch 4199: loss 0.032672
batch 4200: loss 0.063967
batch 4201: loss 0.055629
batch 4202: loss 0.054001
batch 4203: loss 0.019090
batch 4204: loss 0.016794
batch 4205: loss 0.101938
batch 4206: loss 0.020136
batch 4207: loss 0.010868
batch 4208: loss 0.072612
batch 4209: loss 0.021893
batch 4210: loss 0.024759
batch 4211: loss 0.104780
batch 4212: loss 0.027515
batch 4213: loss 0.038699
batch 4214: loss 0.127623
batch 4215: loss 0.022926
batch 4216: loss 0.027775
batch 4217: loss 0.009050
batch 4218: loss 0.165356
batch 4219: loss 0.056571
batch 4220: loss 0.049263
batch 4221: loss 0.020511
batch 4222: loss 0.057492
batch 4223: loss 0.069880
batch 4224: loss 0.084668
batch 4225: loss 0.114218
batch 4226: loss 0.026923
batch 4227: loss 0.025806
batch 4228: loss 0.056442
batch 4229: loss 0.054397
batch 4230: loss 0.198230
batch 4231: loss 0.030126
batch 4232: loss 0.049388
batch 4233: loss 0.036379
batch 4234: loss 0.070147
batch 4235: loss 0.034129
batch 4236: loss 0.041817
batch 4237: loss 0.054895
batch 4238: loss 0.029602
batch 4239: loss 0.088970
batch 4240: loss 0.035132
batch 4241: loss 0.035825
batch 4242: loss 0.111460
batch 4243: loss 0.039041
batch 4244: loss 0.170241
batch 4245: loss 0.098304
batch 4246: loss 0.038031
batch 4247: loss 0.093403
batch 4248: loss 0.067439
batch 4249: loss 0.024526
batch 4250: loss 0.014258
batch 4251: loss 0.013984
batch 4252: loss 0.097118
batch 4253: loss 0.125053
batch 4254: loss 0.052694
batch 4255: loss 0.082522
batch 4256: loss 0.025319
batch 4257: loss 0.017925
batch 4258: loss 0.033297
batch 4259: loss 0.020978
batch 4260: loss 0.210877
batch 4261: loss 0.051886
batch 4262: loss 0.101297
batch 4263: loss 0.024880
batch 4264: loss 0.045190
batch 4265: loss 0.066787
batch 4266: loss 0.030417
batch 4267: loss 0.145596
batch 4268: loss 0.082445
batch 4269: loss 0.090376
batch 4270: loss 0.056749
batch 4271: loss 0.038192
batch 4272: loss 0.078953
batch 4273: loss 0.027205
batch 4274: loss 0.028332
batch 4275: loss 0.049005
batch 4276: loss 0.081437
batch 4277: loss 0.038279
batch 4278: loss 0.114312
batch 4279: loss 0.045355
batch 4280: loss 0.280410
batch 4281: loss 0.029365
batch 4282: loss 0.171193
batch 4283: loss 0.012605
batch 4284: loss 0.093855
batch 4285: loss 0.023516
batch 4286: loss 0.264199
batch 4287: loss 0.201072
batch 4288: loss 0.109581
batch 4289: loss 0.012515
batch 4290: loss 0.058061
batch 4291: loss 0.106864
batch 4292: loss 0.113133
batch 4293: loss 0.045321
batch 4294: loss 0.121048
batch 4295: loss 0.012077
batch 4296: loss 0.050099
batch 4297: loss 0.074575
batch 4298: loss 0.033153
batch 4299: loss 0.088799
batch 4300: loss 0.098647
batch 4301: loss 0.191936
batch 4302: loss 0.039692
batch 4303: loss 0.077334
batch 4304: loss 0.105449
batch 4305: loss 0.136374
batch 4306: loss 0.186118
batch 4307: loss 0.098241
batch 4308: loss 0.026751
batch 4309: loss 0.022714
batch 4310: loss 0.208058
batch 4311: loss 0.045072
batch 4312: loss 0.042191
batch 4313: loss 0.120545
batch 4314: loss 0.061863
batch 4315: loss 0.080573
batch 4316: loss 0.203217
batch 4317: loss 0.152934
batch 4318: loss 0.032757
batch 4319: loss 0.028440
batch 4320: loss 0.010329
batch 4321: loss 0.022681
batch 4322: loss 0.024831
batch 4323: loss 0.187937
batch 4324: loss 0.071306
batch 4325: loss 0.058630
batch 4326: loss 0.018656
batch 4327: loss 0.064613
batch 4328: loss 0.021099
batch 4329: loss 0.015910
batch 4330: loss 0.196749
batch 4331: loss 0.050550
batch 4332: loss 0.011712
batch 4333: loss 0.029287
batch 4334: loss 0.112335
batch 4335: loss 0.111313
batch 4336: loss 0.063762
batch 4337: loss 0.107429
batch 4338: loss 0.089416
batch 4339: loss 0.016413
batch 4340: loss 0.104138
batch 4341: loss 0.152222
batch 4342: loss 0.026453
batch 4343: loss 0.047773
batch 4344: loss 0.133173
batch 4345: loss 0.035334
batch 4346: loss 0.094140
batch 4347: loss 0.085653
batch 4348: loss 0.064478
batch 4349: loss 0.019753
batch 4350: loss 0.055455
batch 4351: loss 0.093907
batch 4352: loss 0.105303
batch 4353: loss 0.029370
batch 4354: loss 0.025895
batch 4355: loss 0.031982
batch 4356: loss 0.043547
batch 4357: loss 0.020289
batch 4358: loss 0.059443
batch 4359: loss 0.119490
batch 4360: loss 0.121171
batch 4361: loss 0.115523
batch 4362: loss 0.090860
batch 4363: loss 0.089882
batch 4364: loss 0.007122
batch 4365: loss 0.099395
batch 4366: loss 0.018151
batch 4367: loss 0.019194
batch 4368: loss 0.093040
batch 4369: loss 0.086788
batch 4370: loss 0.016913
batch 4371: loss 0.104821
batch 4372: loss 0.032723
batch 4373: loss 0.011644
batch 4374: loss 0.063435
batch 4375: loss 0.178748
batch 4376: loss 0.095031
batch 4377: loss 0.094182
batch 4378: loss 0.060030
batch 4379: loss 0.084501
batch 4380: loss 0.101152
batch 4381: loss 0.064565
batch 4382: loss 0.021469
batch 4383: loss 0.138676
batch 4384: loss 0.028108
batch 4385: loss 0.239589
batch 4386: loss 0.038974
batch 4387: loss 0.026198
batch 4388: loss 0.044969
batch 4389: loss 0.089225
batch 4390: loss 0.042497
batch 4391: loss 0.234375
batch 4392: loss 0.094983
batch 4393: loss 0.071042
batch 4394: loss 0.144799
batch 4395: loss 0.041227
batch 4396: loss 0.122418
batch 4397: loss 0.196605
batch 4398: loss 0.071087
batch 4399: loss 0.042146
batch 4400: loss 0.097154
batch 4401: loss 0.049896
batch 4402: loss 0.079467
batch 4403: loss 0.200671
batch 4404: loss 0.055155
batch 4405: loss 0.084627
batch 4406: loss 0.037813
batch 4407: loss 0.037378
batch 4408: loss 0.104879
batch 4409: loss 0.359503
batch 4410: loss 0.042335
batch 4411: loss 0.050792
batch 4412: loss 0.054467
batch 4413: loss 0.108014
batch 4414: loss 0.025853
batch 4415: loss 0.186901
batch 4416: loss 0.062161
batch 4417: loss 0.085667
batch 4418: loss 0.073591
batch 4419: loss 0.026807
batch 4420: loss 0.029952
batch 4421: loss 0.044306
batch 4422: loss 0.155360
batch 4423: loss 0.026815
batch 4424: loss 0.091095
batch 4425: loss 0.032507
batch 4426: loss 0.016069
batch 4427: loss 0.071888
batch 4428: loss 0.011043
batch 4429: loss 0.028137
batch 4430: loss 0.124240
batch 4431: loss 0.055434
batch 4432: loss 0.034492
batch 4433: loss 0.092140
batch 4434: loss 0.074054
batch 4435: loss 0.100669
batch 4436: loss 0.014944
batch 4437: loss 0.046123
batch 4438: loss 0.084201
batch 4439: loss 0.071019
batch 4440: loss 0.066251
batch 4441: loss 0.040368
batch 4442: loss 0.021354
batch 4443: loss 0.137964
batch 4444: loss 0.273096
batch 4445: loss 0.058630
batch 4446: loss 0.166808
batch 4447: loss 0.049412
batch 4448: loss 0.008893
batch 4449: loss 0.078823
batch 4450: loss 0.160548
batch 4451: loss 0.013031
batch 4452: loss 0.037854
batch 4453: loss 0.044730
batch 4454: loss 0.083948
batch 4455: loss 0.011996
batch 4456: loss 0.015293
batch 4457: loss 0.081876
batch 4458: loss 0.044806
batch 4459: loss 0.045197
batch 4460: loss 0.162683
batch 4461: loss 0.100964
batch 4462: loss 0.079187
batch 4463: loss 0.060997
batch 4464: loss 0.006516
batch 4465: loss 0.041508
batch 4466: loss 0.020316
batch 4467: loss 0.052016
batch 4468: loss 0.014887
batch 4469: loss 0.026400
batch 4470: loss 0.080016
batch 4471: loss 0.032920
batch 4472: loss 0.060706
batch 4473: loss 0.034913
batch 4474: loss 0.007165
batch 4475: loss 0.019091
batch 4476: loss 0.065910
batch 4477: loss 0.021663
batch 4478: loss 0.055570
batch 4479: loss 0.049860
batch 4480: loss 0.086456
batch 4481: loss 0.010507
batch 4482: loss 0.042307
batch 4483: loss 0.066337
batch 4484: loss 0.010053
batch 4485: loss 0.075453
batch 4486: loss 0.053280
batch 4487: loss 0.029741
batch 4488: loss 0.098131
batch 4489: loss 0.095543
batch 4490: loss 0.104449
batch 4491: loss 0.030720
batch 4492: loss 0.077790
batch 4493: loss 0.151310
batch 4494: loss 0.042771
batch 4495: loss 0.163987
batch 4496: loss 0.099394
batch 4497: loss 0.018110
batch 4498: loss 0.046340
batch 4499: loss 0.027575
batch 4500: loss 0.032341
batch 4501: loss 0.010788
batch 4502: loss 0.172819
batch 4503: loss 0.036104
batch 4504: loss 0.065781
batch 4505: loss 0.047860
batch 4506: loss 0.013137
batch 4507: loss 0.108438
batch 4508: loss 0.020371
batch 4509: loss 0.031772
batch 4510: loss 0.025071
batch 4511: loss 0.036624
batch 4512: loss 0.024859
batch 4513: loss 0.065112
batch 4514: loss 0.053658
batch 4515: loss 0.049479
batch 4516: loss 0.111419
batch 4517: loss 0.029739
batch 4518: loss 0.155259
batch 4519: loss 0.064110
batch 4520: loss 0.023245
batch 4521: loss 0.051879
batch 4522: loss 0.015465
batch 4523: loss 0.032972
batch 4524: loss 0.091544
batch 4525: loss 0.096006
batch 4526: loss 0.049457
batch 4527: loss 0.054215
batch 4528: loss 0.070639
batch 4529: loss 0.023142
batch 4530: loss 0.076602
batch 4531: loss 0.085155
batch 4532: loss 0.028787
batch 4533: loss 0.036044
batch 4534: loss 0.055263
batch 4535: loss 0.032175
batch 4536: loss 0.023407
batch 4537: loss 0.008450
batch 4538: loss 0.015778
batch 4539: loss 0.032573
batch 4540: loss 0.010861
batch 4541: loss 0.130010
batch 4542: loss 0.012192
batch 4543: loss 0.019954
batch 4544: loss 0.013477
batch 4545: loss 0.088395
batch 4546: loss 0.038227
batch 4547: loss 0.031660
batch 4548: loss 0.168496
batch 4549: loss 0.036639
batch 4550: loss 0.102891
batch 4551: loss 0.099097
batch 4552: loss 0.094111
batch 4553: loss 0.040132
batch 4554: loss 0.028471
batch 4555: loss 0.028467
batch 4556: loss 0.069227
batch 4557: loss 0.067762
batch 4558: loss 0.008052
batch 4559: loss 0.026709
batch 4560: loss 0.081124
batch 4561: loss 0.047191
batch 4562: loss 0.012784
batch 4563: loss 0.106652
batch 4564: loss 0.023883
batch 4565: loss 0.050714
batch 4566: loss 0.108784
batch 4567: loss 0.066303
batch 4568: loss 0.090321
batch 4569: loss 0.069097
batch 4570: loss 0.060434
batch 4571: loss 0.045543
batch 4572: loss 0.039802
batch 4573: loss 0.223406
batch 4574: loss 0.067940
batch 4575: loss 0.096096
batch 4576: loss 0.030764
batch 4577: loss 0.024669
batch 4578: loss 0.101815
batch 4579: loss 0.140765
batch 4580: loss 0.014577
batch 4581: loss 0.114143
batch 4582: loss 0.055311
batch 4583: loss 0.032176
batch 4584: loss 0.030481
batch 4585: loss 0.072150
batch 4586: loss 0.052974
batch 4587: loss 0.090104
batch 4588: loss 0.013065
batch 4589: loss 0.026878
batch 4590: loss 0.052569
batch 4591: loss 0.024143
batch 4592: loss 0.126002
batch 4593: loss 0.030466
batch 4594: loss 0.016073
batch 4595: loss 0.054686
batch 4596: loss 0.066967
batch 4597: loss 0.056272
batch 4598: loss 0.066965
batch 4599: loss 0.037827
batch 4600: loss 0.046917
batch 4601: loss 0.007089
batch 4602: loss 0.021880
batch 4603: loss 0.057453
batch 4604: loss 0.099508
batch 4605: loss 0.013380
batch 4606: loss 0.016564
batch 4607: loss 0.025500
batch 4608: loss 0.034941
batch 4609: loss 0.110723
batch 4610: loss 0.023440
batch 4611: loss 0.031124
batch 4612: loss 0.044305
batch 4613: loss 0.168771
batch 4614: loss 0.070487
batch 4615: loss 0.012196
batch 4616: loss 0.018992
batch 4617: loss 0.155992
batch 4618: loss 0.039360
batch 4619: loss 0.018045
batch 4620: loss 0.057273
batch 4621: loss 0.055915
batch 4622: loss 0.137737
batch 4623: loss 0.083681
batch 4624: loss 0.094664
batch 4625: loss 0.059923
batch 4626: loss 0.051896
batch 4627: loss 0.052322
batch 4628: loss 0.063411
batch 4629: loss 0.169422
batch 4630: loss 0.051723
batch 4631: loss 0.036875
batch 4632: loss 0.022083
batch 4633: loss 0.121328
batch 4634: loss 0.039044
batch 4635: loss 0.011390
batch 4636: loss 0.104353
batch 4637: loss 0.011759
batch 4638: loss 0.085270
batch 4639: loss 0.056657
batch 4640: loss 0.015071
batch 4641: loss 0.158927
batch 4642: loss 0.052256
batch 4643: loss 0.070012
batch 4644: loss 0.044866
batch 4645: loss 0.050564
batch 4646: loss 0.062543
batch 4647: loss 0.049503
batch 4648: loss 0.112320
batch 4649: loss 0.072179
batch 4650: loss 0.080378
batch 4651: loss 0.088997
batch 4652: loss 0.030903
batch 4653: loss 0.116215
batch 4654: loss 0.023699
batch 4655: loss 0.014023
batch 4656: loss 0.015343
batch 4657: loss 0.123055
batch 4658: loss 0.230287
batch 4659: loss 0.079738
batch 4660: loss 0.034877
batch 4661: loss 0.029492
batch 4662: loss 0.038927
batch 4663: loss 0.056998
batch 4664: loss 0.032622
batch 4665: loss 0.046884
batch 4666: loss 0.047500
batch 4667: loss 0.031908
batch 4668: loss 0.012713
batch 4669: loss 0.069062
batch 4670: loss 0.201234
batch 4671: loss 0.079138
batch 4672: loss 0.106432
batch 4673: loss 0.053451
batch 4674: loss 0.156960
batch 4675: loss 0.118392
batch 4676: loss 0.089006
batch 4677: loss 0.074952
batch 4678: loss 0.080031
batch 4679: loss 0.036832
batch 4680: loss 0.088770
batch 4681: loss 0.088663
batch 4682: loss 0.186573
batch 4683: loss 0.010726
batch 4684: loss 0.012908
batch 4685: loss 0.021980
batch 4686: loss 0.085684
batch 4687: loss 0.019400
batch 4688: loss 0.020273
batch 4689: loss 0.019536
batch 4690: loss 0.046320
batch 4691: loss 0.037101
batch 4692: loss 0.036982
batch 4693: loss 0.062887
batch 4694: loss 0.179082
batch 4695: loss 0.016728
batch 4696: loss 0.036290
batch 4697: loss 0.079870
batch 4698: loss 0.033611
batch 4699: loss 0.040801
batch 4700: loss 0.056086
batch 4701: loss 0.009776
batch 4702: loss 0.050277
batch 4703: loss 0.042263
batch 4704: loss 0.103989
batch 4705: loss 0.047666
batch 4706: loss 0.056409
batch 4707: loss 0.078563
batch 4708: loss 0.092661
batch 4709: loss 0.029944
batch 4710: loss 0.019457
batch 4711: loss 0.146076
batch 4712: loss 0.026677
batch 4713: loss 0.093151
batch 4714: loss 0.019231
batch 4715: loss 0.041439
batch 4716: loss 0.080136
batch 4717: loss 0.010337
batch 4718: loss 0.094987
batch 4719: loss 0.106420
batch 4720: loss 0.034649
batch 4721: loss 0.052458
batch 4722: loss 0.015950
batch 4723: loss 0.020024
batch 4724: loss 0.063007
batch 4725: loss 0.035840
batch 4726: loss 0.025345
batch 4727: loss 0.079833
batch 4728: loss 0.036193
batch 4729: loss 0.085705
batch 4730: loss 0.098688
batch 4731: loss 0.009309
batch 4732: loss 0.052039
batch 4733: loss 0.101911
batch 4734: loss 0.012058
batch 4735: loss 0.010120
batch 4736: loss 0.021645
batch 4737: loss 0.091934
batch 4738: loss 0.038469
batch 4739: loss 0.104023
batch 4740: loss 0.095530
batch 4741: loss 0.125661
batch 4742: loss 0.039686
batch 4743: loss 0.059877
batch 4744: loss 0.046033
batch 4745: loss 0.041591
batch 4746: loss 0.064125
batch 4747: loss 0.019644
batch 4748: loss 0.036442
batch 4749: loss 0.149722
batch 4750: loss 0.055788
batch 4751: loss 0.168237
batch 4752: loss 0.006602
batch 4753: loss 0.008671
batch 4754: loss 0.158841
batch 4755: loss 0.119690
batch 4756: loss 0.261532
batch 4757: loss 0.189246
batch 4758: loss 0.036806
batch 4759: loss 0.044932
batch 4760: loss 0.122907
batch 4761: loss 0.056965
batch 4762: loss 0.015173
batch 4763: loss 0.035458
batch 4764: loss 0.079013
batch 4765: loss 0.097796
batch 4766: loss 0.255749
batch 4767: loss 0.097251
batch 4768: loss 0.137249
batch 4769: loss 0.035827
batch 4770: loss 0.008004
batch 4771: loss 0.096414
batch 4772: loss 0.029982
batch 4773: loss 0.013900
batch 4774: loss 0.052228
batch 4775: loss 0.036472
batch 4776: loss 0.033692
batch 4777: loss 0.053079
batch 4778: loss 0.007781
batch 4779: loss 0.022241
batch 4780: loss 0.035858
batch 4781: loss 0.162971
batch 4782: loss 0.087733
batch 4783: loss 0.112193
batch 4784: loss 0.038159
batch 4785: loss 0.015901
batch 4786: loss 0.026310
batch 4787: loss 0.074286
batch 4788: loss 0.036621
batch 4789: loss 0.119199
batch 4790: loss 0.117911
batch 4791: loss 0.171902
batch 4792: loss 0.014947
batch 4793: loss 0.060078
batch 4794: loss 0.020346
batch 4795: loss 0.036720
batch 4796: loss 0.052850
batch 4797: loss 0.041447
batch 4798: loss 0.032260
batch 4799: loss 0.093821
batch 4800: loss 0.128712
batch 4801: loss 0.013567
batch 4802: loss 0.049046
batch 4803: loss 0.070926
batch 4804: loss 0.016453
batch 4805: loss 0.083013
batch 4806: loss 0.075990
batch 4807: loss 0.016627
batch 4808: loss 0.043088
batch 4809: loss 0.014281
batch 4810: loss 0.105461
batch 4811: loss 0.031006
batch 4812: loss 0.056301
batch 4813: loss 0.044069
batch 4814: loss 0.062976
batch 4815: loss 0.053726
batch 4816: loss 0.187430
batch 4817: loss 0.205005
batch 4818: loss 0.017967
batch 4819: loss 0.037856
batch 4820: loss 0.028505
batch 4821: loss 0.038099
batch 4822: loss 0.084085
batch 4823: loss 0.118319
batch 4824: loss 0.046616
batch 4825: loss 0.049863
batch 4826: loss 0.009117
batch 4827: loss 0.022149
batch 4828: loss 0.016912
batch 4829: loss 0.064445
batch 4830: loss 0.068601
batch 4831: loss 0.064295
batch 4832: loss 0.065443
batch 4833: loss 0.053534
batch 4834: loss 0.066952
batch 4835: loss 0.140454
batch 4836: loss 0.129848
batch 4837: loss 0.028837
batch 4838: loss 0.028006
batch 4839: loss 0.112528
batch 4840: loss 0.144156
batch 4841: loss 0.050641
batch 4842: loss 0.008825
batch 4843: loss 0.041035
batch 4844: loss 0.109681
batch 4845: loss 0.067921
batch 4846: loss 0.040927
batch 4847: loss 0.071044
batch 4848: loss 0.031649
batch 4849: loss 0.064040
batch 4850: loss 0.061621
batch 4851: loss 0.074186
batch 4852: loss 0.019951
batch 4853: loss 0.076593
batch 4854: loss 0.026399
batch 4855: loss 0.009817
batch 4856: loss 0.081607
batch 4857: loss 0.015654
batch 4858: loss 0.065499
batch 4859: loss 0.026631
batch 4860: loss 0.092057
batch 4861: loss 0.050768
batch 4862: loss 0.017087
batch 4863: loss 0.017769
batch 4864: loss 0.084876
batch 4865: loss 0.131372
batch 4866: loss 0.084049
batch 4867: loss 0.036028
batch 4868: loss 0.025037
batch 4869: loss 0.091521
batch 4870: loss 0.019386
batch 4871: loss 0.162100
batch 4872: loss 0.114902
batch 4873: loss 0.051623
batch 4874: loss 0.050571
batch 4875: loss 0.023215
batch 4876: loss 0.021685
batch 4877: loss 0.034365
batch 4878: loss 0.091946
batch 4879: loss 0.021178
batch 4880: loss 0.076962
batch 4881: loss 0.007043
batch 4882: loss 0.067199
batch 4883: loss 0.085488
batch 4884: loss 0.082923
batch 4885: loss 0.021203
batch 4886: loss 0.068113
batch 4887: loss 0.040221
batch 4888: loss 0.020480
batch 4889: loss 0.087354
batch 4890: loss 0.094778
batch 4891: loss 0.034214
batch 4892: loss 0.019393
batch 4893: loss 0.026096
batch 4894: loss 0.118116
batch 4895: loss 0.035144
batch 4896: loss 0.046021
batch 4897: loss 0.047447
batch 4898: loss 0.034535
batch 4899: loss 0.072640
batch 4900: loss 0.099463
batch 4901: loss 0.129775
batch 4902: loss 0.085363
batch 4903: loss 0.021579
batch 4904: loss 0.069015
batch 4905: loss 0.025396
batch 4906: loss 0.115769
batch 4907: loss 0.009730
batch 4908: loss 0.030019
batch 4909: loss 0.115774
batch 4910: loss 0.014535
batch 4911: loss 0.048896
batch 4912: loss 0.018935
batch 4913: loss 0.111427
batch 4914: loss 0.056371
batch 4915: loss 0.132346
batch 4916: loss 0.039594
batch 4917: loss 0.051369
batch 4918: loss 0.086309
batch 4919: loss 0.057338
batch 4920: loss 0.031405
batch 4921: loss 0.248657
batch 4922: loss 0.183186
batch 4923: loss 0.041257
batch 4924: loss 0.009377
batch 4925: loss 0.173270
batch 4926: loss 0.076990
batch 4927: loss 0.112737
batch 4928: loss 0.059203
batch 4929: loss 0.025857
batch 4930: loss 0.049073
batch 4931: loss 0.019522
batch 4932: loss 0.026895
batch 4933: loss 0.022746
batch 4934: loss 0.033874
batch 4935: loss 0.111467
batch 4936: loss 0.064248
batch 4937: loss 0.004498
batch 4938: loss 0.074024
batch 4939: loss 0.020515
batch 4940: loss 0.036666
batch 4941: loss 0.036736
batch 4942: loss 0.129989
batch 4943: loss 0.211215
batch 4944: loss 0.066839
batch 4945: loss 0.070109
batch 4946: loss 0.015832
batch 4947: loss 0.104254
batch 4948: loss 0.017904
batch 4949: loss 0.022330
batch 4950: loss 0.031925
batch 4951: loss 0.037710
batch 4952: loss 0.029300
batch 4953: loss 0.030525
batch 4954: loss 0.047843
batch 4955: loss 0.018322
batch 4956: loss 0.009292
batch 4957: loss 0.019295
batch 4958: loss 0.178928
batch 4959: loss 0.135022
batch 4960: loss 0.122249
batch 4961: loss 0.037201
batch 4962: loss 0.089647
batch 4963: loss 0.022553
batch 4964: loss 0.095265
batch 4965: loss 0.199133
batch 4966: loss 0.134087
batch 4967: loss 0.122174
batch 4968: loss 0.097500
batch 4969: loss 0.099123
batch 4970: loss 0.104263
batch 4971: loss 0.008501
batch 4972: loss 0.009429
batch 4973: loss 0.052673
batch 4974: loss 0.025128
batch 4975: loss 0.073957
batch 4976: loss 0.071531
batch 4977: loss 0.073035
batch 4978: loss 0.112723
batch 4979: loss 0.025720
batch 4980: loss 0.131755
batch 4981: loss 0.020989
batch 4982: loss 0.024024
batch 4983: loss 0.073854
batch 4984: loss 0.175505
batch 4985: loss 0.038962
batch 4986: loss 0.030334
batch 4987: loss 0.073001
batch 4988: loss 0.105511
batch 4989: loss 0.102569
batch 4990: loss 0.068215
batch 4991: loss 0.008326
batch 4992: loss 0.023286
batch 4993: loss 0.135186
batch 4994: loss 0.056828
batch 4995: loss 0.042246
batch 4996: loss 0.055637
batch 4997: loss 0.178904
batch 4998: loss 0.066551
batch 4999: loss 0.053441
batch 5000: loss 0.028850
batch 5001: loss 0.088131
batch 5002: loss 0.040721
batch 5003: loss 0.085497
batch 5004: loss 0.009506
batch 5005: loss 0.009876
batch 5006: loss 0.011460
batch 5007: loss 0.031110
batch 5008: loss 0.058142
batch 5009: loss 0.026400
batch 5010: loss 0.010011
batch 5011: loss 0.092928
batch 5012: loss 0.138878
batch 5013: loss 0.138896
batch 5014: loss 0.086096
batch 5015: loss 0.028122
batch 5016: loss 0.034773
batch 5017: loss 0.036653
batch 5018: loss 0.033446
batch 5019: loss 0.027146
batch 5020: loss 0.126757
batch 5021: loss 0.012749
batch 5022: loss 0.054300
batch 5023: loss 0.036602
batch 5024: loss 0.013189
batch 5025: loss 0.017937
batch 5026: loss 0.044180
batch 5027: loss 0.071706
batch 5028: loss 0.032391
batch 5029: loss 0.081595
batch 5030: loss 0.028434
batch 5031: loss 0.014479
batch 5032: loss 0.006957
batch 5033: loss 0.017652
batch 5034: loss 0.007032
batch 5035: loss 0.166440
batch 5036: loss 0.097836
batch 5037: loss 0.024540
batch 5038: loss 0.188763
batch 5039: loss 0.032752
batch 5040: loss 0.019572
batch 5041: loss 0.052951
batch 5042: loss 0.052240
batch 5043: loss 0.018665
batch 5044: loss 0.032368
batch 5045: loss 0.077310
batch 5046: loss 0.028545
batch 5047: loss 0.089569
batch 5048: loss 0.085755
batch 5049: loss 0.010440
batch 5050: loss 0.004824
batch 5051: loss 0.251458
batch 5052: loss 0.173320
batch 5053: loss 0.121966
batch 5054: loss 0.081305
batch 5055: loss 0.073943
batch 5056: loss 0.025946
batch 5057: loss 0.042060
batch 5058: loss 0.041102
batch 5059: loss 0.063516
batch 5060: loss 0.016129
batch 5061: loss 0.296195
batch 5062: loss 0.013997
batch 5063: loss 0.011040
batch 5064: loss 0.118559
batch 5065: loss 0.030121
batch 5066: loss 0.019794
batch 5067: loss 0.086454
batch 5068: loss 0.017165
batch 5069: loss 0.066462
batch 5070: loss 0.049082
batch 5071: loss 0.011881
batch 5072: loss 0.180335
batch 5073: loss 0.029341
batch 5074: loss 0.216979
batch 5075: loss 0.045997
batch 5076: loss 0.026935
batch 5077: loss 0.073494
batch 5078: loss 0.118646
batch 5079: loss 0.025956
batch 5080: loss 0.057727
batch 5081: loss 0.070503
batch 5082: loss 0.093224
batch 5083: loss 0.070749
batch 5084: loss 0.019833
batch 5085: loss 0.058947
batch 5086: loss 0.040708
batch 5087: loss 0.020880
batch 5088: loss 0.063114
batch 5089: loss 0.020471
batch 5090: loss 0.026924
batch 5091: loss 0.080842
batch 5092: loss 0.022111
batch 5093: loss 0.034976
batch 5094: loss 0.172243
batch 5095: loss 0.023813
batch 5096: loss 0.075317
batch 5097: loss 0.011822
batch 5098: loss 0.045877
batch 5099: loss 0.069413
batch 5100: loss 0.025283
batch 5101: loss 0.098571
batch 5102: loss 0.066243
batch 5103: loss 0.039222
batch 5104: loss 0.117883
batch 5105: loss 0.055316
batch 5106: loss 0.084937
batch 5107: loss 0.009594
batch 5108: loss 0.142995
batch 5109: loss 0.041185
batch 5110: loss 0.005035
batch 5111: loss 0.011483
batch 5112: loss 0.068976
batch 5113: loss 0.066691
batch 5114: loss 0.039711
batch 5115: loss 0.014225
batch 5116: loss 0.133180
batch 5117: loss 0.021184
batch 5118: loss 0.045953
batch 5119: loss 0.004237
batch 5120: loss 0.103790
batch 5121: loss 0.044221
batch 5122: loss 0.010225
batch 5123: loss 0.153761
batch 5124: loss 0.076567
batch 5125: loss 0.102062
batch 5126: loss 0.138778
batch 5127: loss 0.041004
batch 5128: loss 0.041107
batch 5129: loss 0.045074
batch 5130: loss 0.035944
batch 5131: loss 0.238629
batch 5132: loss 0.016975
batch 5133: loss 0.049462
batch 5134: loss 0.014096
batch 5135: loss 0.156871
batch 5136: loss 0.072294
batch 5137: loss 0.103469
batch 5138: loss 0.074299
batch 5139: loss 0.029697
batch 5140: loss 0.031770
batch 5141: loss 0.068154
batch 5142: loss 0.163474
batch 5143: loss 0.037851
batch 5144: loss 0.094527
batch 5145: loss 0.111045
batch 5146: loss 0.039679
batch 5147: loss 0.029374
batch 5148: loss 0.025441
batch 5149: loss 0.049318
batch 5150: loss 0.084789
batch 5151: loss 0.041355
batch 5152: loss 0.034036
batch 5153: loss 0.038907
batch 5154: loss 0.040578
batch 5155: loss 0.069858
batch 5156: loss 0.068751
batch 5157: loss 0.148221
batch 5158: loss 0.065327
batch 5159: loss 0.133723
batch 5160: loss 0.013196
batch 5161: loss 0.101538
batch 5162: loss 0.058671
batch 5163: loss 0.038952
batch 5164: loss 0.028130
batch 5165: loss 0.058064
batch 5166: loss 0.010085
batch 5167: loss 0.021379
batch 5168: loss 0.016377
batch 5169: loss 0.008404
batch 5170: loss 0.084116
batch 5171: loss 0.025994
batch 5172: loss 0.017264
batch 5173: loss 0.014514
batch 5174: loss 0.041260
batch 5175: loss 0.047394
batch 5176: loss 0.047622
batch 5177: loss 0.041940
batch 5178: loss 0.015520
batch 5179: loss 0.023892
batch 5180: loss 0.018186
batch 5181: loss 0.134075
batch 5182: loss 0.048091
batch 5183: loss 0.047962
batch 5184: loss 0.020889
batch 5185: loss 0.048969
batch 5186: loss 0.062193
batch 5187: loss 0.019590
batch 5188: loss 0.042603
batch 5189: loss 0.030934
batch 5190: loss 0.020449
batch 5191: loss 0.066746
batch 5192: loss 0.084068
batch 5193: loss 0.043223
batch 5194: loss 0.121985
batch 5195: loss 0.016259
batch 5196: loss 0.027487
batch 5197: loss 0.053082
batch 5198: loss 0.011938
batch 5199: loss 0.028845
batch 5200: loss 0.023638
batch 5201: loss 0.125653
batch 5202: loss 0.155187
batch 5203: loss 0.040461
batch 5204: loss 0.009429
batch 5205: loss 0.056667
batch 5206: loss 0.014658
batch 5207: loss 0.009943
batch 5208: loss 0.005042
batch 5209: loss 0.051747
batch 5210: loss 0.044722
batch 5211: loss 0.071055
batch 5212: loss 0.074056
batch 5213: loss 0.109041
batch 5214: loss 0.044122
batch 5215: loss 0.014322
batch 5216: loss 0.029651
batch 5217: loss 0.027404
batch 5218: loss 0.079532
batch 5219: loss 0.007698
batch 5220: loss 0.187837
batch 5221: loss 0.034397
batch 5222: loss 0.100966
batch 5223: loss 0.043697
batch 5224: loss 0.015990
batch 5225: loss 0.066768
batch 5226: loss 0.067146
batch 5227: loss 0.035725
batch 5228: loss 0.016225
batch 5229: loss 0.013803
batch 5230: loss 0.111041
batch 5231: loss 0.006096
batch 5232: loss 0.058905
batch 5233: loss 0.060596
batch 5234: loss 0.023276
batch 5235: loss 0.085979
batch 5236: loss 0.027131
batch 5237: loss 0.080005
batch 5238: loss 0.221334
batch 5239: loss 0.104414
batch 5240: loss 0.012608
batch 5241: loss 0.052190
batch 5242: loss 0.021938
batch 5243: loss 0.050845
batch 5244: loss 0.117278
batch 5245: loss 0.038269
batch 5246: loss 0.125928
batch 5247: loss 0.048462
batch 5248: loss 0.042149
batch 5249: loss 0.021579
batch 5250: loss 0.021766
batch 5251: loss 0.075002
batch 5252: loss 0.022815
batch 5253: loss 0.015873
batch 5254: loss 0.031299
batch 5255: loss 0.060922
batch 5256: loss 0.012103
batch 5257: loss 0.079146
batch 5258: loss 0.067185
batch 5259: loss 0.045768
batch 5260: loss 0.029637
batch 5261: loss 0.185773
batch 5262: loss 0.113803
batch 5263: loss 0.043896
batch 5264: loss 0.017143
batch 5265: loss 0.087825
batch 5266: loss 0.025921
batch 5267: loss 0.014981
batch 5268: loss 0.044332
batch 5269: loss 0.043275
batch 5270: loss 0.196432
batch 5271: loss 0.136639
batch 5272: loss 0.031565
batch 5273: loss 0.028806
batch 5274: loss 0.019694
batch 5275: loss 0.063021
batch 5276: loss 0.199911
batch 5277: loss 0.047468
batch 5278: loss 0.037735
batch 5279: loss 0.030374
batch 5280: loss 0.044902
batch 5281: loss 0.115441
batch 5282: loss 0.263368
batch 5283: loss 0.014849
batch 5284: loss 0.027878
batch 5285: loss 0.033405
batch 5286: loss 0.031633
batch 5287: loss 0.038310
batch 5288: loss 0.131500
batch 5289: loss 0.096931
batch 5290: loss 0.047118
batch 5291: loss 0.028651
batch 5292: loss 0.014588
batch 5293: loss 0.189578
batch 5294: loss 0.029751
batch 5295: loss 0.041506
batch 5296: loss 0.123325
batch 5297: loss 0.017476
batch 5298: loss 0.009465
batch 5299: loss 0.056575
batch 5300: loss 0.028832
batch 5301: loss 0.031757
batch 5302: loss 0.113808
batch 5303: loss 0.021797
batch 5304: loss 0.007932
batch 5305: loss 0.012684
batch 5306: loss 0.018981
batch 5307: loss 0.037556
batch 5308: loss 0.039677
batch 5309: loss 0.050398
batch 5310: loss 0.098582
batch 5311: loss 0.031698
batch 5312: loss 0.011967
batch 5313: loss 0.034085
batch 5314: loss 0.034769
batch 5315: loss 0.094262
batch 5316: loss 0.017703
batch 5317: loss 0.010949
batch 5318: loss 0.011917
batch 5319: loss 0.044551
batch 5320: loss 0.043121
batch 5321: loss 0.010501
batch 5322: loss 0.044159
batch 5323: loss 0.033110
batch 5324: loss 0.008135
batch 5325: loss 0.226942
batch 5326: loss 0.012742
batch 5327: loss 0.011417
batch 5328: loss 0.094163
batch 5329: loss 0.035434
batch 5330: loss 0.099491
batch 5331: loss 0.019964
batch 5332: loss 0.071496
batch 5333: loss 0.016375
batch 5334: loss 0.091471
batch 5335: loss 0.012270
batch 5336: loss 0.028635
batch 5337: loss 0.042597
batch 5338: loss 0.015870
batch 5339: loss 0.050971
batch 5340: loss 0.114931
batch 5341: loss 0.049966
batch 5342: loss 0.026458
batch 5343: loss 0.049191
batch 5344: loss 0.018867
batch 5345: loss 0.012995
batch 5346: loss 0.055489
batch 5347: loss 0.013709
batch 5348: loss 0.025409
batch 5349: loss 0.017114
batch 5350: loss 0.024518
batch 5351: loss 0.016778
batch 5352: loss 0.016821
batch 5353: loss 0.104263
batch 5354: loss 0.149062
batch 5355: loss 0.054057
batch 5356: loss 0.047562
batch 5357: loss 0.134043
batch 5358: loss 0.014384
batch 5359: loss 0.069940
batch 5360: loss 0.030339
batch 5361: loss 0.036374
batch 5362: loss 0.029280
batch 5363: loss 0.005946
batch 5364: loss 0.060558
batch 5365: loss 0.090080
batch 5366: loss 0.006535
batch 5367: loss 0.063761
batch 5368: loss 0.047283
batch 5369: loss 0.065209
batch 5370: loss 0.046113
batch 5371: loss 0.022774
batch 5372: loss 0.018150
batch 5373: loss 0.021401
batch 5374: loss 0.041338
batch 5375: loss 0.032243
batch 5376: loss 0.105306
batch 5377: loss 0.035627
batch 5378: loss 0.047912
batch 5379: loss 0.024647
batch 5380: loss 0.023630
batch 5381: loss 0.050649
batch 5382: loss 0.118397
batch 5383: loss 0.060716
batch 5384: loss 0.011577
batch 5385: loss 0.058204
batch 5386: loss 0.047690
batch 5387: loss 0.041513
batch 5388: loss 0.031384
batch 5389: loss 0.010281
batch 5390: loss 0.040012
batch 5391: loss 0.011467
batch 5392: loss 0.057782
batch 5393: loss 0.113466
batch 5394: loss 0.010212
batch 5395: loss 0.035058
batch 5396: loss 0.098784
batch 5397: loss 0.047868
batch 5398: loss 0.045673
batch 5399: loss 0.049021
batch 5400: loss 0.059740
batch 5401: loss 0.039984
batch 5402: loss 0.036000
batch 5403: loss 0.046787
batch 5404: loss 0.022980
batch 5405: loss 0.047528
batch 5406: loss 0.034952
batch 5407: loss 0.046270
batch 5408: loss 0.022087
batch 5409: loss 0.020762
batch 5410: loss 0.031906
batch 5411: loss 0.005755
batch 5412: loss 0.021576
batch 5413: loss 0.058837
batch 5414: loss 0.027836
batch 5415: loss 0.033715
batch 5416: loss 0.013729
batch 5417: loss 0.159253
batch 5418: loss 0.029742
batch 5419: loss 0.013221
batch 5420: loss 0.016345
batch 5421: loss 0.009831
batch 5422: loss 0.036451
batch 5423: loss 0.056323
batch 5424: loss 0.048562
batch 5425: loss 0.089491
batch 5426: loss 0.124031
batch 5427: loss 0.008275
batch 5428: loss 0.071261
batch 5429: loss 0.062116
batch 5430: loss 0.020111
batch 5431: loss 0.042261
batch 5432: loss 0.060780
batch 5433: loss 0.025681
batch 5434: loss 0.105348
batch 5435: loss 0.058634
batch 5436: loss 0.011927
batch 5437: loss 0.030048
batch 5438: loss 0.061002
batch 5439: loss 0.023110
batch 5440: loss 0.018622
batch 5441: loss 0.028063
batch 5442: loss 0.030642
batch 5443: loss 0.059055
batch 5444: loss 0.028386
batch 5445: loss 0.081186
batch 5446: loss 0.090848
batch 5447: loss 0.085245
batch 5448: loss 0.036731
batch 5449: loss 0.068969
batch 5450: loss 0.016909
batch 5451: loss 0.130783
batch 5452: loss 0.053821
batch 5453: loss 0.012836
batch 5454: loss 0.048596
batch 5455: loss 0.016801
batch 5456: loss 0.020469
batch 5457: loss 0.006711
batch 5458: loss 0.010587
batch 5459: loss 0.019333
batch 5460: loss 0.178319
batch 5461: loss 0.078078
batch 5462: loss 0.041693
batch 5463: loss 0.158783
batch 5464: loss 0.085376
batch 5465: loss 0.008839
batch 5466: loss 0.021944
batch 5467: loss 0.050234
batch 5468: loss 0.067978
batch 5469: loss 0.142236
batch 5470: loss 0.011384
batch 5471: loss 0.045686
batch 5472: loss 0.015635
batch 5473: loss 0.116318
batch 5474: loss 0.054032
batch 5475: loss 0.080744
batch 5476: loss 0.036811
batch 5477: loss 0.030946
batch 5478: loss 0.072183
batch 5479: loss 0.032425
batch 5480: loss 0.021616
batch 5481: loss 0.039092
batch 5482: loss 0.091669
batch 5483: loss 0.018965
batch 5484: loss 0.133598
batch 5485: loss 0.134469
batch 5486: loss 0.084380
batch 5487: loss 0.007067
batch 5488: loss 0.021859
batch 5489: loss 0.021816
batch 5490: loss 0.018905
batch 5491: loss 0.043204
batch 5492: loss 0.047717
batch 5493: loss 0.048740
batch 5494: loss 0.069465
batch 5495: loss 0.009737
batch 5496: loss 0.073883
batch 5497: loss 0.063999
batch 5498: loss 0.007109
batch 5499: loss 0.092888
batch 5500: loss 0.008262
batch 5501: loss 0.003872
batch 5502: loss 0.018548
batch 5503: loss 0.076378
batch 5504: loss 0.073810
batch 5505: loss 0.038688
batch 5506: loss 0.033700
batch 5507: loss 0.125476
batch 5508: loss 0.050596
batch 5509: loss 0.095771
batch 5510: loss 0.128830
batch 5511: loss 0.058562
batch 5512: loss 0.021338
batch 5513: loss 0.049048
batch 5514: loss 0.022056
batch 5515: loss 0.017519
batch 5516: loss 0.062283
batch 5517: loss 0.009962
batch 5518: loss 0.033396
batch 5519: loss 0.050326
batch 5520: loss 0.099038
batch 5521: loss 0.036746
batch 5522: loss 0.030480
batch 5523: loss 0.068563
batch 5524: loss 0.028735
batch 5525: loss 0.149290
batch 5526: loss 0.041628
batch 5527: loss 0.114755
batch 5528: loss 0.047158
batch 5529: loss 0.068015
batch 5530: loss 0.023177
batch 5531: loss 0.050686
batch 5532: loss 0.042581
batch 5533: loss 0.132746
batch 5534: loss 0.016320
batch 5535: loss 0.021432
batch 5536: loss 0.012736
batch 5537: loss 0.097171
batch 5538: loss 0.090396
batch 5539: loss 0.131856
batch 5540: loss 0.065298
batch 5541: loss 0.025324
batch 5542: loss 0.021691
batch 5543: loss 0.017477
batch 5544: loss 0.036063
batch 5545: loss 0.078253
batch 5546: loss 0.016440
batch 5547: loss 0.073619
batch 5548: loss 0.017778
batch 5549: loss 0.018028
batch 5550: loss 0.010777
batch 5551: loss 0.061480
batch 5552: loss 0.042252
batch 5553: loss 0.014163
batch 5554: loss 0.289101
batch 5555: loss 0.042543
batch 5556: loss 0.032538
batch 5557: loss 0.042285
batch 5558: loss 0.033102
batch 5559: loss 0.112102
batch 5560: loss 0.009775
batch 5561: loss 0.024630
batch 5562: loss 0.006485
batch 5563: loss 0.145416
batch 5564: loss 0.088937
batch 5565: loss 0.068903
batch 5566: loss 0.035230
batch 5567: loss 0.222641
batch 5568: loss 0.025545
batch 5569: loss 0.013517
batch 5570: loss 0.026259
batch 5571: loss 0.044684
batch 5572: loss 0.127517
batch 5573: loss 0.055221
batch 5574: loss 0.070744
batch 5575: loss 0.121658
batch 5576: loss 0.135889
batch 5577: loss 0.016416
batch 5578: loss 0.031202
batch 5579: loss 0.016097
batch 5580: loss 0.166100
batch 5581: loss 0.015914
batch 5582: loss 0.072538
batch 5583: loss 0.085634
batch 5584: loss 0.055336
batch 5585: loss 0.080997
batch 5586: loss 0.064136
batch 5587: loss 0.031893
batch 5588: loss 0.038358
batch 5589: loss 0.010157
batch 5590: loss 0.061361
batch 5591: loss 0.025177
batch 5592: loss 0.021897
batch 5593: loss 0.032213
batch 5594: loss 0.081480
batch 5595: loss 0.191158
batch 5596: loss 0.095689
batch 5597: loss 0.012946
batch 5598: loss 0.060195
batch 5599: loss 0.082038
batch 5600: loss 0.017194
batch 5601: loss 0.022762
batch 5602: loss 0.008987
batch 5603: loss 0.006971
batch 5604: loss 0.035762
batch 5605: loss 0.117454
batch 5606: loss 0.024319
batch 5607: loss 0.103382
batch 5608: loss 0.009653
batch 5609: loss 0.047799
batch 5610: loss 0.024652
batch 5611: loss 0.058850
batch 5612: loss 0.068445
batch 5613: loss 0.050544
batch 5614: loss 0.042579
batch 5615: loss 0.128287
batch 5616: loss 0.044630
batch 5617: loss 0.047136
batch 5618: loss 0.127633
batch 5619: loss 0.032039
batch 5620: loss 0.228841
batch 5621: loss 0.040782
batch 5622: loss 0.072660
batch 5623: loss 0.050331
batch 5624: loss 0.102315
batch 5625: loss 0.011928
batch 5626: loss 0.038430
batch 5627: loss 0.035746
batch 5628: loss 0.064970
batch 5629: loss 0.040175
batch 5630: loss 0.155042
batch 5631: loss 0.062531
batch 5632: loss 0.017721
batch 5633: loss 0.041254
batch 5634: loss 0.058866
batch 5635: loss 0.015560
batch 5636: loss 0.038743
batch 5637: loss 0.027683
batch 5638: loss 0.013523
batch 5639: loss 0.026396
batch 5640: loss 0.007339
batch 5641: loss 0.011638
batch 5642: loss 0.097889
batch 5643: loss 0.155634
batch 5644: loss 0.040471
batch 5645: loss 0.035392
batch 5646: loss 0.130170
batch 5647: loss 0.041929
batch 5648: loss 0.009079
batch 5649: loss 0.048280
batch 5650: loss 0.024288
batch 5651: loss 0.015509
batch 5652: loss 0.029018
batch 5653: loss 0.018779
batch 5654: loss 0.045191
batch 5655: loss 0.055012
batch 5656: loss 0.065206
batch 5657: loss 0.032879
batch 5658: loss 0.020553
batch 5659: loss 0.006807
batch 5660: loss 0.025637
batch 5661: loss 0.007869
batch 5662: loss 0.057162
batch 5663: loss 0.105517
batch 5664: loss 0.013553
batch 5665: loss 0.036666
batch 5666: loss 0.024671
batch 5667: loss 0.151983
batch 5668: loss 0.061997
batch 5669: loss 0.021984
batch 5670: loss 0.082796
batch 5671: loss 0.068134
batch 5672: loss 0.007532
batch 5673: loss 0.014194
batch 5674: loss 0.047751
batch 5675: loss 0.015430
batch 5676: loss 0.050169
batch 5677: loss 0.015618
batch 5678: loss 0.027916
batch 5679: loss 0.061821
batch 5680: loss 0.366698
batch 5681: loss 0.025614
batch 5682: loss 0.012070
batch 5683: loss 0.042222
batch 5684: loss 0.035186
batch 5685: loss 0.020575
batch 5686: loss 0.011986
batch 5687: loss 0.023524
batch 5688: loss 0.024434
batch 5689: loss 0.050086
batch 5690: loss 0.033140
batch 5691: loss 0.136872
batch 5692: loss 0.058871
batch 5693: loss 0.043681
batch 5694: loss 0.024123
batch 5695: loss 0.065432
batch 5696: loss 0.047144
batch 5697: loss 0.132143
batch 5698: loss 0.034263
batch 5699: loss 0.070547
batch 5700: loss 0.022289
batch 5701: loss 0.027041
batch 5702: loss 0.049774
batch 5703: loss 0.015165
batch 5704: loss 0.041095
batch 5705: loss 0.042420
batch 5706: loss 0.025288
batch 5707: loss 0.083823
batch 5708: loss 0.149506
batch 5709: loss 0.022332
batch 5710: loss 0.078117
batch 5711: loss 0.028635
batch 5712: loss 0.016131
batch 5713: loss 0.016988
batch 5714: loss 0.058594
batch 5715: loss 0.049941
batch 5716: loss 0.058673
batch 5717: loss 0.006045
batch 5718: loss 0.006084
batch 5719: loss 0.069791
batch 5720: loss 0.014360
batch 5721: loss 0.008886
batch 5722: loss 0.008067
batch 5723: loss 0.154745
batch 5724: loss 0.077300
batch 5725: loss 0.042662
batch 5726: loss 0.023662
batch 5727: loss 0.003589
batch 5728: loss 0.079309
batch 5729: loss 0.003501
batch 5730: loss 0.015382
batch 5731: loss 0.064861
batch 5732: loss 0.041572
batch 5733: loss 0.103591
batch 5734: loss 0.015152
batch 5735: loss 0.060318
batch 5736: loss 0.120438
batch 5737: loss 0.026421
batch 5738: loss 0.151007
batch 5739: loss 0.091101
batch 5740: loss 0.063436
batch 5741: loss 0.026835
batch 5742: loss 0.006698
batch 5743: loss 0.057030
batch 5744: loss 0.141537
batch 5745: loss 0.022894
batch 5746: loss 0.072800
batch 5747: loss 0.023535
batch 5748: loss 0.043831
batch 5749: loss 0.051107
batch 5750: loss 0.062881
batch 5751: loss 0.040806
batch 5752: loss 0.122049
batch 5753: loss 0.026684
batch 5754: loss 0.071633
batch 5755: loss 0.093752
batch 5756: loss 0.034386
batch 5757: loss 0.065737
batch 5758: loss 0.010755
batch 5759: loss 0.067810
batch 5760: loss 0.056465
batch 5761: loss 0.009939
batch 5762: loss 0.096591
batch 5763: loss 0.036973
batch 5764: loss 0.025380
batch 5765: loss 0.007160
batch 5766: loss 0.036381
batch 5767: loss 0.023027
batch 5768: loss 0.052809
batch 5769: loss 0.054649
batch 5770: loss 0.021423
batch 5771: loss 0.044901
batch 5772: loss 0.034439
batch 5773: loss 0.011092
batch 5774: loss 0.039149
batch 5775: loss 0.028322
batch 5776: loss 0.021590
batch 5777: loss 0.084340
batch 5778: loss 0.019431
batch 5779: loss 0.088469
batch 5780: loss 0.087616
batch 5781: loss 0.125727
batch 5782: loss 0.172571
batch 5783: loss 0.016598
batch 5784: loss 0.052399
batch 5785: loss 0.043908
batch 5786: loss 0.033501
batch 5787: loss 0.088589
batch 5788: loss 0.012024
batch 5789: loss 0.025669
batch 5790: loss 0.008051
batch 5791: loss 0.044581
batch 5792: loss 0.010305
batch 5793: loss 0.070647
batch 5794: loss 0.012524
batch 5795: loss 0.045576
batch 5796: loss 0.014061
batch 5797: loss 0.024105
batch 5798: loss 0.055043
batch 5799: loss 0.047484
batch 5800: loss 0.020454
batch 5801: loss 0.025495
batch 5802: loss 0.058372
batch 5803: loss 0.040768
batch 5804: loss 0.105227
batch 5805: loss 0.050387
batch 5806: loss 0.069810
batch 5807: loss 0.020016
batch 5808: loss 0.214380
batch 5809: loss 0.022578
batch 5810: loss 0.006505
batch 5811: loss 0.184085
batch 5812: loss 0.159036
batch 5813: loss 0.014327
batch 5814: loss 0.043697
batch 5815: loss 0.081793
batch 5816: loss 0.007090
batch 5817: loss 0.027643
batch 5818: loss 0.110865
batch 5819: loss 0.015673
batch 5820: loss 0.116397
batch 5821: loss 0.005229
batch 5822: loss 0.113874
batch 5823: loss 0.084433
batch 5824: loss 0.121834
batch 5825: loss 0.015066
batch 5826: loss 0.009601
batch 5827: loss 0.030337
batch 5828: loss 0.051750
batch 5829: loss 0.097990
batch 5830: loss 0.009115
batch 5831: loss 0.022647
batch 5832: loss 0.117070
batch 5833: loss 0.039509
batch 5834: loss 0.056335
batch 5835: loss 0.036227
batch 5836: loss 0.086372
batch 5837: loss 0.071478
batch 5838: loss 0.022099
batch 5839: loss 0.006681
batch 5840: loss 0.020492
batch 5841: loss 0.102006
batch 5842: loss 0.046540
batch 5843: loss 0.170319
batch 5844: loss 0.044385
batch 5845: loss 0.029887
batch 5846: loss 0.021446
batch 5847: loss 0.063949
batch 5848: loss 0.119589
batch 5849: loss 0.052909
batch 5850: loss 0.011821
batch 5851: loss 0.049450
batch 5852: loss 0.046914
batch 5853: loss 0.018364
batch 5854: loss 0.045158
batch 5855: loss 0.026421
batch 5856: loss 0.097313
batch 5857: loss 0.103442
batch 5858: loss 0.045115
batch 5859: loss 0.140003
batch 5860: loss 0.006781
batch 5861: loss 0.106588
batch 5862: loss 0.031728
batch 5863: loss 0.026505
batch 5864: loss 0.046442
batch 5865: loss 0.419527
batch 5866: loss 0.044328
batch 5867: loss 0.007951
batch 5868: loss 0.008843
batch 5869: loss 0.083347
batch 5870: loss 0.027698
batch 5871: loss 0.082519
batch 5872: loss 0.059761
batch 5873: loss 0.073613
batch 5874: loss 0.095204
batch 5875: loss 0.032202
batch 5876: loss 0.065316
batch 5877: loss 0.077422
batch 5878: loss 0.012743
batch 5879: loss 0.015157
batch 5880: loss 0.006283
batch 5881: loss 0.017501
batch 5882: loss 0.021098
batch 5883: loss 0.053772
batch 5884: loss 0.103134
batch 5885: loss 0.095139
batch 5886: loss 0.114110
batch 5887: loss 0.023619
batch 5888: loss 0.007666
batch 5889: loss 0.035593
batch 5890: loss 0.017376
batch 5891: loss 0.018199
batch 5892: loss 0.004334
batch 5893: loss 0.010680
batch 5894: loss 0.087260
batch 5895: loss 0.032905
batch 5896: loss 0.025602
batch 5897: loss 0.044130
batch 5898: loss 0.019488
batch 5899: loss 0.019539
batch 5900: loss 0.012399
batch 5901: loss 0.066325
batch 5902: loss 0.004783
batch 5903: loss 0.014306
batch 5904: loss 0.055025
batch 5905: loss 0.033073
batch 5906: loss 0.093589
batch 5907: loss 0.030501
batch 5908: loss 0.011838
batch 5909: loss 0.023106
batch 5910: loss 0.004261
batch 5911: loss 0.032819
batch 5912: loss 0.054443
batch 5913: loss 0.028543
batch 5914: loss 0.045290
batch 5915: loss 0.041801
batch 5916: loss 0.013849
batch 5917: loss 0.024994
batch 5918: loss 0.035086
batch 5919: loss 0.019547
batch 5920: loss 0.049572
batch 5921: loss 0.071710
batch 5922: loss 0.047604
batch 5923: loss 0.016937
batch 5924: loss 0.023682
batch 5925: loss 0.033678
batch 5926: loss 0.008400
batch 5927: loss 0.041706
batch 5928: loss 0.008495
batch 5929: loss 0.015540
batch 5930: loss 0.040528
batch 5931: loss 0.079341
batch 5932: loss 0.020230
batch 5933: loss 0.096160
batch 5934: loss 0.030741
batch 5935: loss 0.007231
batch 5936: loss 0.014083
batch 5937: loss 0.016426
batch 5938: loss 0.059377
batch 5939: loss 0.020526
batch 5940: loss 0.037242
batch 5941: loss 0.062530
batch 5942: loss 0.133277
batch 5943: loss 0.015095
batch 5944: loss 0.012379
batch 5945: loss 0.035021
batch 5946: loss 0.038277
batch 5947: loss 0.012293
batch 5948: loss 0.032851
batch 5949: loss 0.046117
batch 5950: loss 0.005167
batch 5951: loss 0.139379
batch 5952: loss 0.047683
batch 5953: loss 0.027055
batch 5954: loss 0.033997
batch 5955: loss 0.006297
batch 5956: loss 0.033070
batch 5957: loss 0.023040
batch 5958: loss 0.140323
batch 5959: loss 0.032970
batch 5960: loss 0.026770
batch 5961: loss 0.008854
batch 5962: loss 0.095583
batch 5963: loss 0.074820
batch 5964: loss 0.121828
batch 5965: loss 0.151749
batch 5966: loss 0.033811
batch 5967: loss 0.118380
batch 5968: loss 0.070386
batch 5969: loss 0.095430
batch 5970: loss 0.022959
batch 5971: loss 0.013712
batch 5972: loss 0.056306
batch 5973: loss 0.009865
batch 5974: loss 0.013615
batch 5975: loss 0.119400
batch 5976: loss 0.031232
batch 5977: loss 0.150520
batch 5978: loss 0.006416
batch 5979: loss 0.081510
batch 5980: loss 0.170064
batch 5981: loss 0.098980
batch 5982: loss 0.041630
batch 5983: loss 0.003886
batch 5984: loss 0.025790
batch 5985: loss 0.049279
batch 5986: loss 0.077624
batch 5987: loss 0.011060
batch 5988: loss 0.096650
batch 5989: loss 0.108936
batch 5990: loss 0.013117
batch 5991: loss 0.050317
batch 5992: loss 0.085866
batch 5993: loss 0.017701
batch 5994: loss 0.010873
batch 5995: loss 0.062688
batch 5996: loss 0.046317
batch 5997: loss 0.059086
batch 5998: loss 0.017112
batch 5999: loss 0.144052
</code></pre> <h1 id="模型的评估：-tf-keras-metrics">模型的评估： tf.keras.metrics</h1> <p>最后，我们使用测试集评估模型的性能。这里，我们使用 tf.keras.metrics 中的 SparseCategoricalAccuracy 评估器来评估模型在测试集上的性能，该评估器能够对模型预测的结果与真实结果进行比较，并输出预测正确的样本数占总样本数的比例。我们迭代测试数据集，每次通过 update_state() 方法向评估器输入两个参数： y_pred 和 y_true ，即模型预测出的结果和真实结果。评估器具有内部变量来保存当前评估指标相关的参数数值（例如当前已传入的累计样本数和当前预测正确的样本数）。迭代结束后，我们使用 result() 方法输出最终的评估指标值（预测正确的样本数占总样本数的比例）。</p> <p>在以下代码中，我们实例化了一个 tf.keras.metrics.SparseCategoricalAccuracy 评估器，并使用 For 循环迭代分批次传入了测试集数据的预测结果与真实结果，并输出训练后的模型在测试数据集上的准确率。</p> <div class="language-python extra-class"><pre class="language-python"><code>sparse_categorical_accuracy <span class="token operator">=</span> tf<span class="token punctuation">.</span>keras<span class="token punctuation">.</span>metrics<span class="token punctuation">.</span>SparseCategoricalAccuracy<span class="token punctuation">(</span><span class="token punctuation">)</span>
num_batches <span class="token operator">=</span> <span class="token builtin">int</span><span class="token punctuation">(</span>data_loader<span class="token punctuation">.</span>num_test_data <span class="token operator">//</span> batch_size<span class="token punctuation">)</span>
<span class="token keyword">for</span> batch_index <span class="token keyword">in</span> <span class="token builtin">range</span><span class="token punctuation">(</span>num_batches<span class="token punctuation">)</span><span class="token punctuation">:</span>
  start_index<span class="token punctuation">,</span> end_index <span class="token operator">=</span> batch_index <span class="token operator">*</span> batch_size<span class="token punctuation">,</span> <span class="token punctuation">(</span>batch_index <span class="token operator">+</span> <span class="token number">1</span><span class="token punctuation">)</span> <span class="token operator">*</span> batch_size
  y_pred <span class="token operator">=</span> model<span class="token punctuation">.</span>predict<span class="token punctuation">(</span>data_loader<span class="token punctuation">.</span>test_data<span class="token punctuation">[</span>start_index<span class="token punctuation">:</span> end_index<span class="token punctuation">]</span><span class="token punctuation">)</span>
  sparse_categorical_accuracy<span class="token punctuation">.</span>update_state<span class="token punctuation">(</span>y_true<span class="token operator">=</span>data_loader<span class="token punctuation">.</span>test_label<span class="token punctuation">[</span>start_index<span class="token punctuation">:</span>end_index<span class="token punctuation">]</span><span class="token punctuation">,</span> y_pred<span class="token operator">=</span>y_pred<span class="token punctuation">)</span>
<span class="token keyword">print</span><span class="token punctuation">(</span><span class="token string">&quot;test accuracy: %f&quot;</span> <span class="token operator">%</span> sparse_categorical_accuracy<span class="token punctuation">.</span>result<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><pre><code>test accuracy: 0.973400
</code></pre> <div class="language-python extra-class"><pre class="language-python"><code>
</code></pre></div></div> <div class="page-edit"><!----> <!----></div> <div class="page-nav"><p class="inner"><span class="prev">
        ←
        <a href="/articles/datascience/Deeplearning.html" class="prev">
          Deep Learning
        </a></span> <span class="next"><a href="/articles/datascience/NLP.html">
          NLP
        </a>
        →
      </span></p></div> </div> <!----></div></div>
    <script src="/assets/js/app.265cfc62.js" defer></script><script src="/assets/js/20.25d4abb7.js" defer></script>
  </body>
</html>
